{"0": {
    "doc": "CASFileCache",
    "title": "File Metadata",
    "content": "Due to limitations of posix filesystem presentations, input FileNodes with ‘executable’ specifications require a unique inode from their non-executable counterparts. The CFC considers a CAS entry keyed with its path and executability uniquely, and their files are named as such: &lt;hash&gt; for the non-executable content, &lt;hash&gt;_exec for executable content. Each is charged with its own size to the storage pool, can be used for executable-idempotent requests on equal footing, and can be expired independently. Using any digest-based interface method without the option for an executable specification will result in executable key impartiality. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/CASFileCache/#file-metadata",
    "relUrl": "/docs/architecture/CASFileCache/#file-metadata"
  },"1": {
    "doc": "CASFileCache",
    "title": "Operations",
    "content": "Read(digest, offset, limit): The CFC will locate the indexed file for a digest with executable impartiality, attempt to open it, seek to the offset requested, and present a limited stream of the content’s bytes. A missing digest will result in predefined behavior per the CAS Interface, as well as an invalidation of its process index entry, if it exists. An access for the entry is inserted into the Access Queue. A read is (currently) impermeable to expiration, and can result in overcharge of filesystem content in such a case. Write(id): The CFC provides a Write-interface object that can be used to insert content into the CAS. Service and Operation lifetime implementations use this to inject content as requested, and leverage its restartable and asynchronous completion capacities. Writes may be rejected out of hand due to configurable CAS entry size limitations. Under the hood, there are several constraints present per-write: . | A Write is uniquely identified by its digest and a uuid. | A Write will charge the storage pool size with its full content size upon opening a stream via getOutput(). | A Write’s OutputStream is a mutually exclusive transaction, with a premature close() restoring the stream’s capacity for representation. | A close() upon completion of a Write’s OutputStream will initiate a validation for it. | Upon successful validation (size and content are compared to the expected digest), a commit is effected, making it available as a CAS entry with a recorded access placing it at the head of the RC-LRU. | Within a CFC, concurrent Writes to a single digest will compete for completion, with the first one to commit cancelling all other Writes’ OutputStreams to the same digest. | Upon a commit, a shared future for all Writes to a single digest is completed, allowing premature indication of committed content receipt. | . FindMissingBlobs(digests): The process index is queried for each requested digest with executable impartiality, to filter for only those entries that do not exist in the index. No file interactions take place during this request, and no invalidation of content occurs upon this request. Accesses for all existing entries enter the Access Queue. Batch Read/Write: These requests are sequenced for each entry, resulting in only latency reduction, corresponding to each individual operation. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/CASFileCache/#operations",
    "relUrl": "/docs/architecture/CASFileCache/#operations"
  },"2": {
    "doc": "CASFileCache",
    "title": "Structures",
    "content": "Entry Index . This is a Map of CFC Entry Keys ({digest, executability}) to accounting entry metadata. Reference counts, directories containing the entry, and file metadata ttl are metadata fields for the entry, as well as support fields for the RC-LRU. RC-LRU . The Reference Counted Least Recently Used list is a composite structure with the Process Index. Entries are either in or out of the RC-LRU, with only those entries in the RC-LRU considered for expiration. All entries with reference counts &gt; 0 are out of the RC-LRU. Manipulation of the RC-LRU is synchronized for the entire CFC, with the actors being the transactive put[Directory]() and decrementReferences() methods, as well as the Access Recorder. Directory Index . Directories can be maintained by the CFC to provide several important heuristic optimizations, both in terms of storage and IO complexity. For a given directory entry, a key of its digest is used to refer to all of its flattened content digests. This is complemented by a filesystem directory named for the digest with a &lt;hash&gt;_dir name, where it will contain a) real directories underneath it named for each level of DirectoryNode in its REAPI specification, and b) files with names and executability from a FileNode for each Directory’s files, the inodes for which are all CAS entry files (or empty files, in that special size case). The CFC also maintains a reverse index for each CAS entry to all of the directories it exists under. Upon expiration of an entry, each directory it is contained in is also expired, with both Directory Index and filesystem directories being deleted. This is the only growth bound check on any directory, being the only path to expiration. Access Queue . A simple queue of records of digest set accesses, meant to indicate the entries more desired than not represented. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/CASFileCache/#structures",
    "relUrl": "/docs/architecture/CASFileCache/#structures"
  },"3": {
    "doc": "CASFileCache",
    "title": "Agents",
    "content": "Startup Scanner . Upon start(), a CFC scans the contents of its root to populate its indices. It first locates and injects all entry inodes, filtering for valid entry names and filesystem permissions, populating the Entry Index. Invalid names are deleted. It then scans directories, reproducing a Directory tree definition for each toplevel dir for validation, and populates the Directory Index. Invalid directories are deleted. A callback for the set of valid entry digests discovered is invoked. This procedure is (currently) blocking for valid entries, and operates asynchronously during invalid content removal. Access Recorder . The Access Queue is depleted by the Access Recorder, active after start() completion, to perform a simple reordering for accessed entries into the front of the RC-LRU. An entry that is out of the RC-LRU remains out after being processed by the recorder, and an entry that is in is moved to the head. A single thread runs the Access Recorder, and it is terminated when stop() is called on the CFC. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/CASFileCache/#agents",
    "relUrl": "/docs/architecture/CASFileCache/#agents"
  },"4": {
    "doc": "CASFileCache",
    "title": "Behaviors",
    "content": "Reference/Dereference . Entries can have their reference counts incremented, with 0-&gt;1 moving them out of the RC-LRU, or decremented, with 1-&gt;0 moving them in to the RC-LRU, being placed at the head. Directory {,de}references operate in batch over their contained entries, with directory digests functioning as extreme space-saving shorthands for digest sets. Asynchronous Filesystem Monitor . Entry and Directory metadata content Time To Live checks provide convergent consistency in the event of asynchronous filesystem manipulation - in the event of an unexpected availability or permission change to the filesystem, the CFC eventually discovers and presents corrected state. Expiration . As a result of any intended insertion into the CFC, the total content size of the CAS is ‘charged’ by adding the size of the new content. A test is performed against this size. If the size exceeds a configurable maximum content size, expiration occurs and blocks the insertion until complete. To expire content, the CFC removes the tail of the RC-LRU, attempts a delegate write if configured, expires its associated directories, removes its metadata, deletes its file content, and will executable-impartially invoke a callback with the expired digest. This process is repeated until the size is once again below the maximum content size. Delegation . The CFC supports a waterfall delegation to a CAS Interface Object. This Object (currently) sees the following interactions: . | Writes upon expirations of a referencing CFC, with full output streaming. | Reads upon content missing from a referencing CFC, with read through insertion of the referencing CFC as an entry | FindMissingBlobs for filtered entries that remain missing from a referencing CFC. | . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/CASFileCache/#behaviors",
    "relUrl": "/docs/architecture/CASFileCache/#behaviors"
  },"5": {
    "doc": "CASFileCache",
    "title": "CASFileCache",
    "content": "The Content Addressable Storage File Cache (CFC) is the functional beating heart of all Buildfarm Workers. It should fundamentally be thought of as a rotating set of Content stored in files, indexed by Digests that make up the inputs for Actions to be executed. It also serves several other functions here detailed. This documentation presents it both in terms of functional behavior, as if it were a service or process, as well as with Java semantics, owing to its implementation. A CFC has the additional responsibility in shard workers of presenting a CAS (Java) Interface, which supports the typical Read, Write, FindMissingBlobs, and batch versions of the R/W requests. Each CFC needs a filesystem path to use as its root, and a set of configuration parameters. Under this path, it will store the CAS entries, named by their digests, optionally in a partitioned directory hierarchy by subdividing the first N bytes of the hash. The CAS entries are treated by the CFC in terms of their index (filesystem directory) mapping to an inode (file content and limited metadata). This is an important distinction for the CFC, as it is used to present a substantially deduplicated ephemeral tree of inputs by its corresponding Exec Filesystem when executing actions. This will be described in further detail under the RC-LRU. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/CASFileCache/",
    "relUrl": "/docs/architecture/CASFileCache/"
  },"6": {
    "doc": "Action Cache",
    "title": "Methods",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/action_cache/#methods",
    "relUrl": "/docs/architecture/action_cache/#methods"
  },"7": {
    "doc": "Action Cache",
    "title": "GetActionResult",
    "content": "Essentially the “get” method, which is responsible for finding an ActionResult and retrieving it. Before invoking this method, Bazel client should compute the input tree and the Action message for the action needs to be done. Then Bazel can use this GetActionCache method to see if the action has been completed successfully and, if so, use bytestream.Read to download the outputs. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/action_cache/#getactionresult",
    "relUrl": "/docs/architecture/action_cache/#getactionresult"
  },"8": {
    "doc": "Action Cache",
    "title": "UpdateActionResult",
    "content": "As mentioned above, the ActionCache service doesn’t necessarily need an Execution Service. In this case, a “put” method is required so that an ActionResult can be directly put into the cache. This is what UpdateActionResult is designed for. With this method, Bazel clients running different machines can upload their build results into a cache pool, which can be available to other users through GetActionResult. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/action_cache/#updateactionresult",
    "relUrl": "/docs/architecture/action_cache/#updateactionresult"
  },"9": {
    "doc": "Action Cache",
    "title": "Buildfarm Implementations",
    "content": "Buildfarm provides implementations of both ActionCache and Execution. ActionResult can be populated through Execution service or uploaded by local Bazel clients through UpdataActionCache. The ActionCache service is hosted on the server-side of Buildfarm. When a GetActionResultRequest received through GetActionResult call, by using the instance_name field of the request, the ActionCache service will find the Instance that is supposed to have ActionResult, and it will find the Instance and get the ActionResult asynchronously. Similarly, an UpdateActionResultRequest will be sent to the ActionCache service through UpdateActionResult rpc call. The service will find the Instance that is supposed to have the ActionResult through the instance_name field of the request and update the corresponding instance. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/action_cache/#buildfarm-implementations",
    "relUrl": "/docs/architecture/action_cache/#buildfarm-implementations"
  },"10": {
    "doc": "Action Cache",
    "title": "Action Cache",
    "content": "ActionCahe is a service that can be used to query whether a defined action has already been executed and, if so, download its result. The service API is defined in the Remote Execution API. ActionCache service would require ContentAddressableStorage service to store file data. An Action encapsulates all the information required to execute an action. Such information includes the command, input tree containing subdirectory/file tree, environment variables, platform information. All the information will contribute to the digest computation of an Action so that execution of an Action multiple times will produce the same output. With this, hash of an Action can be used as a key to cached ActionResult, which store result and output of an Action after an Action is completed. ActionResults can be populated in ActionCache service after Actions get completed by an Execution service. They can also come from a local Bazel client that has executed the Actions and put the ActionResults into the cache by using the UpdateActionCache method. In other words, The ActionCache service can be used without using/implementing the Execution service. By leveraging Action definition, ActionCache service is responsible for mapping Actions to the ActionResults. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/action_cache/",
    "relUrl": "/docs/architecture/action_cache/"
  },"11": {
    "doc": "Admin",
    "title": "Admin",
    "content": "How to make your AWS cluster visible to Buildfarm Admin . Buildfarm Admin currently only works with Buildfarm clusters deployed in AWS. All hosts must be properly tagged with values specified in application.properties. | Tag schedulers (servers) with buildfarm.instance_type={deployment.tag.instance.type.server} | Tag CPU workers with buildfarm.worker_type={deployment.tag.instance.type.cpuworker} | Tag GPU workers with buildfarm.worker_type={deployment.tag.instance.type.gpuworker} | Tag all hosts with aws:autoscaling:groupName={} | Tag all hosts with buildfarm.cluster_id={cluster.id} | . REST API Endpoints . | /restart/worker/{instanceId} | /restart/server/{instanceId} | /terminate/{instanceId} | /scale/{autoScaleGroup}/{numInstances} | . Build and Run Buildfarm Admin locally . Create application.properties file and override any default settings from admin/main/src/main/resources/application.properties. Pass your config file location to the optional spring.config.location flag. ` cd admin/main ./mvnw clean package java -jar target/bfadmin.jar -Dspring.config.location=file: ` . Run Latest Docker Container . Replace $HOME with the location of your application.properties file. Make sure the AWS environment variables are set on the host machine for the account where Buildfarm cluster is deployed. ` docker run -p 8080:8080 -v $HOME:/var/lib -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN bazelbuild/buildfarm-admin:latest ` . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/admin/",
    "relUrl": "/docs/admin/"
  },"12": {
    "doc": "Alerts",
    "title": "Alerts",
    "content": "Using the Prometheus metrics you may want to setup alerts on your buildfarm cluster. Below are some example alerts: . High Cpu Queue avg(queue_size{job=\"your_instance\", queue_name=\"cpu_queue\"}) Critical = Triggered when the value is greater than 1000 for 60m Warning = Triggered when the value is greater than 1000 for 30m . High Cluster Utilization avg(execution_slot_usage{job=\"your_instance\", service=\"buildfarm-worker\"}) Critical = Triggered when the value is greater than 85 for 60m Warning = Triggered when the value is greater than 85 for 30m . Multiple Bazel Versions Detected count by (tool_name) (count(dispatched_operations_tools_amount{job=\"your_instance\", service=\"buildfarm-server\"}) without(host_ip, instance)) Warning = Triggered when the value is greater than 1 for 1m . No available workers avg(worker_pool_size{job=\"your_instance\", service=\"buildfarm-server\"}) Critical = Triggered when the value is equal to 0 for 5m Warning = Triggered when the value is equal to 0 for 2m . Stuck CPU Workers . (avg(delta(execution_slot_usage{job=\"your_instance\", service=\"buildfarm-worker\"}[5m])) by (host_ip) == 0) and (avg(execution_slot_usage{job=\"your_instance\", service=\"buildfarm-worker\"}) by (host_ip) &gt; 0) and (avg(execution_slot_usage{job=\"your_instance\", service=\"buildfarm-worker\"}) by (host_ip) &lt; 89) . Critical = Triggered when the value is greater than or equal to 0 for 30m . Key Expirations - Builds May Be Slow . avg(increase(expired_key_total{job=\"your_instance\", service=\"buildfarm-worker\"}[5m])) by (host_ip) . Warning = Triggered when the value is greater than 10000 for 1h . High Prequeue pre_queue_size{job=\"your_instance\", service=\"buildfarm-server\"} Critical = Triggered when the value is greater than 0 for 15m Warning = Triggered when the value is greater than 0 for 5m . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/metrics/alerts/",
    "relUrl": "/docs/metrics/alerts/"
  },"13": {
    "doc": "Architecture",
    "title": "Instances",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/architecture/#instances",
    "relUrl": "/docs/architecture/architecture/#instances"
  },"14": {
    "doc": "Architecture",
    "title": "Definition",
    "content": "An instance is a namespace which represents a pool of resources available to a remote execution client. All requests to the Remote Execution Services are identified with an instance name, and a client is expected to communicate with the same instance between requests in order to accomplish aggregate activities. For instance, a typical client usage of FindMissingBlobs, then one or more Write/BatchUploadBlobs, Execute, and zero or more Read/BatchReadBlobs would all have the same instance name associated with the requests. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/architecture/#definition",
    "relUrl": "/docs/architecture/architecture/#definition"
  },"15": {
    "doc": "Architecture",
    "title": "Implementation",
    "content": "Buildfarm uses modular instances, where an instance is associated with one concrete type that governs its behavior and functionality. The Remote Execution API uses instance names to identify every request made, which allows Buildfarm instances to represent partitions of resources. One endpoint may support many different instances, each with their own name, and each instance will have its own type. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/architecture/#implementation",
    "relUrl": "/docs/architecture/architecture/#implementation"
  },"16": {
    "doc": "Architecture",
    "title": "Typical Deployment",
    "content": ". ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/architecture/#typical-deployment",
    "relUrl": "/docs/architecture/architecture/#typical-deployment"
  },"17": {
    "doc": "Architecture",
    "title": "Workers",
    "content": "Workers are deployed as an autoscaling group in the cloud environment. This group should scale based on the load. This will require monitoring and alerting to be setup, which will trigger the scaling events. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/architecture/#workers",
    "relUrl": "/docs/architecture/architecture/#workers"
  },"18": {
    "doc": "Architecture",
    "title": "Schedulers",
    "content": "Schedulers are deployed as an autoscaling group in the cloud environment. This group should scale based on the load. This will require monitoring and alerting to be setup, which will trigger the scaling events. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/architecture/#schedulers",
    "relUrl": "/docs/architecture/architecture/#schedulers"
  },"19": {
    "doc": "Architecture",
    "title": "Clustered Redis",
    "content": "Clustered Redis should be sized based on the expected load. Typically, no replication is necessary for Buildfarm use as the loss of data stored is not catastrophic. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/architecture/#clustered-redis",
    "relUrl": "/docs/architecture/architecture/#clustered-redis"
  },"20": {
    "doc": "Architecture",
    "title": "Schedulers Network Load Balancer",
    "content": "A network load balancer is set up to target the Schedulers autoscaling group. This will be the primary Buildfarm endpoint. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/architecture/#schedulers-network-load-balancer",
    "relUrl": "/docs/architecture/architecture/#schedulers-network-load-balancer"
  },"21": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/architecture/",
    "relUrl": "/docs/architecture/architecture/"
  },"22": {
    "doc": "bf-cat",
    "title": "bf-cat",
    "content": "bf-cat is a tool provided with buildfarm for investigating the various structures and status of your Buildfarm Cluster. Its basic usage is: . bf-cat &lt;host[:port]&gt; &lt;instance-name&gt; &lt;hash-function&gt; &lt;command&gt; [params...] . instance-name is the name of the specific instance to inquire about, typically configured on schedulers. A literal empty string parameter (i.e. bash: \"\") will use the default instance for a server. hash-function is one of MD5, SHA1, SHA256, etc selected to match the supported digest functions of an instance, and used to compute digests for content retrieved. command is typically one of the following, with digest parameters as /, as typically represented in log entries: . | Action &lt;digest&gt;: Retrieves Action definitions from the CAS and renders them with field identifiers. | Capabilities: Retrieve the capabilities response for an instance. | Command &lt;digest&gt;: Retrieves Command definitions from the CAS and renders them with field identifiers. | Directory &lt;digest&gt;: Retrieves Directory definitions from the CAS and redners them with field identifiers. | File &lt;digest&gt;: Downloads a Blob from the CAS and prints it to stdout. This can be safely redirected to a file, with no additional output interceding | Missing &lt;digest&gt;: Make a findMissingBlobs request, outputting only the digests in the parameter list that are missing from the CAS | Operation &lt;name&gt;: Retrieves current operation statuses and renders them with field identifiers as able. This uses the Operations API and will include rich information about operations in flight, compared to the ‘execute’ function | BackplaneStatus: Retrieve the status of a shard cluster’s operation queues, with discrete information about each provisioned layer of the ready-to-run queue. | TreeLayout &lt;digest&gt;: Retrieves Trees of inputs from a root node. A Tree is printed with indent-levels according to depth in the directory hierarchy with FileNode and DirectoryNode fields with digests for each entry, as well as a weight by byte and % of the sizes of each directory subtree. | WorkerProfile: Retrieve profile information about a worker’s operation, including the size of the CAS and the relative performance of the execution pipeline | Watch &lt;name&gt;: Watch an operation to retrieve status updates about its progress through the operation pipeline | . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/tools/bf-cat/",
    "relUrl": "/docs/tools/bf-cat/"
  },"23": {
    "doc": "Build Without Bytes",
    "title": "Builds Without The Bytes",
    "content": "tl;dr: add --build_request_id=https://host?ENSURE_OUTPUTS_PRESENT=true#$(uuidgen) to your BWOB bazel invocations. As proposed in this issue and the accompanying document, bazel endeavors to provide a mechanism to be ‘content-light’ for remote execution, using only content reference addresses to request action execution and construct successively dependent action definitions. Simply put, Builds Without The Bytes skips the download of action result artifacts until it needs them. This puts BuildFarm in the uncomfortable position of never being able to expire any unique content associated with ActionResults, which is not something easily accomplished. It extends the required lifetime to some duration where bazel retains references, and if it encounters a situation where it either wants them, or to execute an action that depends upon them, and they are not available, it fails. To combat this, you can provide some metadata to buildfarm that will help to limit (but will not remove the possibility of) failed builds. Bazel presents a ‘correlated_invocations_id’ on every request to BuildFarm, including the GetActionResult request, which it uses to retrieve cached results. Since ActionResults are the long tail survivor of actions, being retained for much longer after one executes and produces its content, this represents the most likely position where content may have been removed, and a stale reference might be provided. BuildFarm recognizes this correlated_invocations_id and if it is a URI, can parse its query parameters for behavior control. One such control is ENSURE_OUTPUTS_PRESENT for the GetActionResult request - if this query value is the string “true”, BuildFarm will make a silent FindMissingBlobs check for all of the outputs of an ActionResult before responding with it. If any are missing, BuildFarm will instead return code NOT_FOUND, inspiring the client to see a cache miss, and attempt a [remote] execution. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/builds-without-the-bytes/#builds-without-the-bytes",
    "relUrl": "/docs/execution/builds-without-the-bytes/#builds-without-the-bytes"
  },"24": {
    "doc": "Build Without Bytes",
    "title": "Build Without Bytes",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/builds-without-the-bytes/",
    "relUrl": "/docs/execution/builds-without-the-bytes/"
  },"25": {
    "doc": "CAS",
    "title": "Methods",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/#methods",
    "relUrl": "/docs/architecture/content_addressable_storage/#methods"
  },"26": {
    "doc": "CAS",
    "title": "Reads",
    "content": "A read of content from a CAS is a relatively simple procedure, whether accessed through BatchReadBlobs, or the ByteStream Read method. The semantics associated with these requests require the support of content availability translation (NOT_FOUND for missing), and seeking to a particular offset to start reading. Clients are expected to behave progressively, since no size limitation nor bandwidth availability is mandated, meaning that they should advance an offset along the length of the content until complete with successive requests, assuming DEADLINE_EXCEEDED or other transient errors occur during the download. resource_name for reads within ByteStream Read must be \"{instance_name}/blobs/{hash}/{size}\" . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/#reads",
    "relUrl": "/docs/architecture/content_addressable_storage/#reads"
  },"27": {
    "doc": "CAS",
    "title": "Writes",
    "content": "Writes of content into a CAS require a prior computation of an address with a digest method of choice for all content. A write can be initiated with BatchUpdateBlobs or the ByteStream Write method. A ByteStream Write resource_name must begin with {instance_name}/uploads/{uuid}/blobs/{hash}/{size}, and may have any trailing filename after the size, separated by ‘/’. The trailing content is ignored. The uuid is a client generated identifier for a given write, and may be shared among many digests, but should be strictly client-local. Writes should respect a WriteResponse received at any time after initiating the request of the size of the blob, to indicate that no further WriteRequests are necessary. Writes which fail prior to the receipt of content should be progressive, checking for the offset to resume an upload via ByteStream QueryWriteStatus. Buildfarm implements the CAS in a variety of ways, including an in-memory storage for the reference implementation, as a proxy for an external CAS, an HTTP/1 proxy based on the remote-cache implementation in bazel, and as a persistent on-disk storage for workers, supplementing an execution filesystem for actions as well as participating in a sparsely-sharded distributed store. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/#writes",
    "relUrl": "/docs/architecture/content_addressable_storage/#writes"
  },"28": {
    "doc": "CAS",
    "title": "Buildfarm Implementations",
    "content": "Since these implementations vary in complexity and storage semantics, a common interface was declared within Buildfarm to accommodate substitutions of a CAS, as well as standardize its use. The specifics of these CAS implementations are detailed here. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/#buildfarm-implementations",
    "relUrl": "/docs/architecture/content_addressable_storage/#buildfarm-implementations"
  },"29": {
    "doc": "CAS",
    "title": "Memory",
    "content": "The memory CAS implementation is extremely simple in design, constituting a maximum size with LRU eviction policy. Entry eviction is a registrable event for use as a storage for the delegated ActionCache, and Writes may be completed asynchronously by concurrent independent upload completion of an entry. This is the example presentation of a CAS in the memory instance available here, but for reference, specification in any cas_config field for server or worker will enable the creation of a unique instance. cas_config: { memory: { # limit for CAS total content size in bytes max_size_bytes: 1073741824 # 1024 * 1024 * 1024 } } . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/#memory",
    "relUrl": "/docs/architecture/content_addressable_storage/#memory"
  },"30": {
    "doc": "CAS",
    "title": "GRPC",
    "content": "This is a CAS which completely mirrors a target CAS for all requests, useful as a proxy to be embedded in a full Instance declaration. A grpc config example is available in the alternate instance specification in the memory server example here. For reference: . cas_config: { grpc: { # instance name for CAS resources, default is empty instance_name: \"internal\" # target suitable for netty channel target: \"cas-host.cloud.org:5000\" } } . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/#grpc",
    "relUrl": "/docs/architecture/content_addressable_storage/#grpc"
  },"31": {
    "doc": "CAS",
    "title": "HTTP/1",
    "content": "The HTTP/1 CAS proxy hosts a GRPC service definition for a configurable target HTTP/1 service that it communicates with using an identical implementation to the bazel http remote cache protocol. Since the HTTP/1 proxy is a separate process, there are no configuration options for it. Instead, run the proxy in a known location (address and port), and use a grpc configuration indicated above, pointing to its address and instance name. The proxy can be run with: . bazel run //src/main/java/build/buildfarm:buildfarm-http-proxy -- -p 8081 -c \"http://your-http-endpoint\" . And will result in a listening grpc service on port 8081 on all interfaces, relaying requests to the endpoint in question. Use --help to see more options. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/#http1",
    "relUrl": "/docs/architecture/content_addressable_storage/#http1"
  },"32": {
    "doc": "CAS",
    "title": "Shard",
    "content": "A sharded CAS leverages multiple Worker CAS retention and proxies requests to hosts with isolated CAS shards. These shards register their participation and entry presentation on a ShardBackplane. The backplane maintains a mapping of addresses to the nodes which host them. The sharded CAS is an aggregated proxy for its members, performing each function with fallback as appropriate; FindMissingBlobs requests are cycled through the shards, reducing a list of missing entries, Writes select a target node at random, Reads attempt a request on each advertised shard for an entry with failover on NOT_FOUND or transient grpc error. Reads are optimistic, given that a blob would not be requested that was not expected to be found, the sharded CAS will failover on complete absence of a blob to a whole cluster search for an entry. A shard CAS is the default for the Shard Instance type, with its required backplane specification. Since functionality between Shard CAS, AC, and Execution are mixed in here, the definition is somewhat cluttered, with efforts to refine specific aspects of it underway. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/#shard",
    "relUrl": "/docs/architecture/content_addressable_storage/#shard"
  },"33": {
    "doc": "CAS",
    "title": "Worker CAS",
    "content": "Working hand in hand with the Shard CAS implementation, the Worker CAS leverages a requisite on-disk store to provide a CAS from its CASFileCache. Since the worker maintains a large cache of inputs for use with actions, this CAS is routinely populated from downloads due to operation input fetches in addition to uploads from the Shard frontend. CasFileCache . The worker’s CAS file cache uses persistent disk storage. A strongly recommended filesystem to back this is XFS, due to its high link counts limits per inode. A strongly discouraged filesystem is ext4, which places a hard limit of 65000 link counts per inode. The layout of the files are ordered such that file content, in the form of canonical digest filenames for inode storage, remains on the root of the cache directory, while exec roots and symlinkable directories contain hard links to these inodes. This avoids unnecessary duplication of file contents. Upon worker startup, the worker’s cache instance is initialized in two phases. First, the root is scanned to store file information. Second, the existing directories are traversed to compute their validating identification. Files will be automatically deleted if their file names are invalid for the cache, or if the configured cache size has been exceeded by previous files. Each Worker type’s specification of the CASFileCache is unique. The operationqueue worker defaults to CASFileCache with no option to change it, only configure its root directory and configuration options. The shard worker allows a more flexible specification, with delegates available of the other types to fall back on for CAS expansion, and encapsulated CAS configs . The CASFileCache is also available on MemoryInstance servers, where it can represent a persistent file storage for a CAS. An example configuration for this is: . cas_config: { filesystem: { # the local cache location relative to the 'root', or absolute path: \"cache\" # limit for contents of files retained # from CAS in the cache max_size_bytes: 2147483648 # 2 * 1024 * 1024 * 1024 # limit for content size of files retained # from CAS in the cache max_entry_size_bytes: 2147483648 # 2 * 1024 * 1024 * 1024 } } . CASTest is a standalone tool to load the cache and print status information about it. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/#worker-cas",
    "relUrl": "/docs/architecture/content_addressable_storage/#worker-cas"
  },"34": {
    "doc": "CAS",
    "title": "CAS",
    "content": "A Content Addressable Storage (CAS) is a collection of service endpoints which provide read and creation access to immutable binary large objects (blobs). The core service is declared in the Remote Execution API, and also requires presentation of the ByteStream API with specializations for resource names and behaviors. An entry in the CAS is a sequence of bytes whose computed digest via a hashing function constitutes its address. The address is specified as either a [Digest] message, or by the makeup of a resource name in ByteStream requests. A CAS supports several methods of retrieving and inserting entries, as well as some utility methods for determining presence and iterating through linked hierarchies of directories as Merkle Trees. Functionally within the REAPI, the CAS is the communication plane for Action inputs and outputs, and is used to retain other contents by existing clients, including bazel, like build event information. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/content_addressable_storage/",
    "relUrl": "/docs/architecture/content_addressable_storage/"
  },"35": {
    "doc": "Contribute",
    "title": "Edit the site",
    "content": "The site is rendered with jeykll. To see changes locally run: . | cd _site | bundle install | jekyll serve | . You should see a local URL like: http://127.0.0.1:4000/bazel-buildfarm/ . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/contribute/contribute/#edit-the-site",
    "relUrl": "/docs/contribute/contribute/#edit-the-site"
  },"36": {
    "doc": "Contribute",
    "title": "Contribute",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/contribute/contribute/",
    "relUrl": "/docs/contribute/contribute/"
  },"37": {
    "doc": "Debugging",
    "title": "Debugging Executions",
    "content": "This tutorial is intended to guide you in debugging executions that are run on buildfarm. Problem . Build actions (and more specifically unit tests) often behave differently across environments. This makes it difficult to understand and debug remote action behavior. The execution environment of buildfarm may seem like an inaccessible black-box to the client. The problem is further complicated by the fact that buildfarm’s executor may not be using the same tools locally as the build system (such as bazel’s sandbox or process-wrapper). In buildfarm, these tools are chosen based on the configuration of execution wrappers, and custom execution wrappers exist as well. Buildfarm may also virtualize hardware and apply restrictions on resources such as cpu, memory, and network. Additionally, buildfarm actions do not always run on the same machine. Actions in buildfarm are directed to particular eligible workers based on their platform properties. Despite the platform properties of the worker chosen, the action may run inside a docker container which also affects the execution environment. Capturing debug information (before execution) . There are execution properties available to help you capture debug information dynamically when performing executions. To capture initial information about an execution, you can use the following: . bazel test --remote_default_exec_properties='debug-before-execution=true' \\ --remote_executor=grpc://127.0.0.1:8980 \\ --noremote_accept_cached \\ --nocache_test_results //&lt;TARGET&gt; . Assuming the action is properly queued and reaches the executor, the executor will deliberately fail the action and send you debug information via json through the action’s stderr. You can view this debug information by reading the failure log: . cat source/bazel-out/k8-fastbuild/testlogs/&lt;TARGET&gt;/test.log . You can streamline viewing this information via --test_output=streamed . bazel test \\ --test_output=streamed \\ --remote_default_exec_properties='debug-before-execution=true' \\ --noremote_accept_cached \\ --nocache_test_results \\ //&lt;TARGET&gt; . Capturing debug information (after execution) . A similar property exists that will tell the executor to run the action, and after the action finishes, return debug information. bazel test \\ --test_output=streamed \\ --remote_default_exec_properties='debug-after-execution=true' \\ --noremote_accept_cached \\ --nocache_test_results \\ //&lt;TARGET&gt; . If your action is hanging, you might not be able to get this debug information and should stick with ‘debug-before-execution=true’. If your action is finishing, this would be a ideal to use, as it should provide all the same information that ‘debug-before-execution=true’ does, with additional info about the execution. Configuration . If you see an error like this: . properties are not valid for queue eligibility: [name: \"debug-before-execution\" value: \"true\"] . you will need to configure the queue’s allow_unmatched to true so that the server can still put the action on the queue. Additionally you may want to configure the the worker’s DequeueMatchSettings to also have allow_unmatched to true. That will ensure the action does not fail reaching one of the worker due to the additional exec_property. Debugging Tests . By default, debug-before-execution and debug-after-execution only apply to test actions. The reason being, is that if you wanted to debug a test, but passed a global property like --remote_default_exec_properties='debug-before-execution=true', it would invalidate all the actions, and the test would need rebuilt, but the rebuild of the test would fail, because you would actually be debugging the first build action, and you would never see the debug results of the test action. Debugging tests is more typical, but you can also debug build actions by using --remote_default_exec_properties='debug-tests-only=false'. It is more convenient to debug things by passing the global exec properties, but you could also tag targets specifically in the BUILD files with these debug options. Finding Operations . All buildfarm operations can be found using the following query: ./bazelw run //src/main/java/build/buildfarm:bf-find-operations localhost:8980 shard SHA256 . When you run a build invocation with bazel, bazel will you give you an invocation id. You can use that to query your specific operations: ./bazelw run //src/main/java/build/buildfarm:bf-find-operations localhost:8980 shard SHA256 \"[?(@.operation.metadata.requestMetadata.toolInvocationId == '1877f43a-9b33-4eca-9d6b-aef71b47bf47')]\" . You can find the operation of a specific test name like this: ./bazelw run //src/main/java/build/buildfarm:bf-find-operations localhost:8980 shard SHA256 \"$.command.environmentVariables[?(@.value == '//code/tools/example_tests/bash_hello_world2:main')]\" . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/debugging-executions/#debugging-executions",
    "relUrl": "/docs/execution/debugging-executions/#debugging-executions"
  },"38": {
    "doc": "Debugging",
    "title": "Debugging",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/debugging-executions/",
    "relUrl": "/docs/execution/debugging-executions/"
  },"39": {
    "doc": "Design Documents",
    "title": "Design Documents",
    "content": "Infinite Cache (Storage Workers) . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/contribute/design-documents/",
    "relUrl": "/docs/contribute/design-documents/"
  },"40": {
    "doc": "Enviornment",
    "title": "Enviornment",
    "content": "Since workers are expected to execute programs in a way that makes using remote transparent to build users, there is a great deal of nuance to their definition. Operating System, Distribution, runtimes, compilers, their versions, and standard libraries make this a tricky proposition. While hermeticizing a build by declaring the full set of tools a compilation requires is the ‘right’ solution for a build, it may be too big of a hill to climb for everyone, particularly those trying to ease into remote execution. Under Linux at least, this is made somewhat easier through docker, even if the steps to get to a conformant environment are a bit complicated. Here we will provide an example of creating a target execution environment container capable of running buildfarm, and this should be similar for all major distributions, particularly those with released docker hub bases and standard package managers including java runtimes and toolchain software. You should choose the versions of relevant software at first based on the client environment you want to support. For this example, we’re going to assume a target of ubuntu-20 (focal), with a gcc9 compiler supporting C++, and a java runtime from openjdk-14 supplied by its package manager. We will need: . | a bazel 3.3.1 install (the older version is required by rules_docker at the buildfarm version, this will be updated eventually). | docker daemon running | . First we will pull our intended base image from dockerhub: . docker pull ubuntu:focal . Next we will create our Dockerfile for our customized environment. Here is the content of that Dockerfile . # choose our ubuntu distribution with version 20 (focal) from ubuntu:focal # get our current software suite. apt-get is preferred to apt due to warnings # about a non-stable CLI run apt-get update # install the basic apt-utils to limit warnings due to its absence run DEBIAN_FRONTEND=noninteractive apt-get install --no-install-suggests apt-utils # upgrade to the current software suite versions run DEBIAN_FRONTEND=noninteractive apt-get dist-upgrade -y # install the required packages for our execution environment and buildfarm runtime run DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-suggests \\ g++-9 g++ openjdk-14-jdk-headless . And build it with docker build -t ubuntu20-java14:latest . Sending build context to Docker daemon 2.56kB Step 1/5 : from ubuntu:focal ---&gt; 9140108b62dc ... Removing intermediate container 505236be00e4 ---&gt; 77a8c2ae4d16 Successfully built 77a8c2ae4d16 Successfully tagged ubuntu20-java14:latest . In our example’s case, we want to use this image as a base to build a buildfarm worker container image. Unfortunately, rules_docker does not know how to interact with the docker images hosted by a local docker daemon, only docker registries like dockerhub.io. We will get past this by running the local registry utility container. If you really want to host this base elsewhere, I’ll assume that you know how to translate the host:port references below, and you can skip running your own registry. First we need to tag our image so that it gets pushed to our local registry: . docker tag ubuntu20-java14:latest localhost.localdomain:5000/ubuntu20-java14:latest . Then we can start our registry with a recognizable name for later shutdown: . docker run -d --rm --name local-registry -p 5000:5000 registry . And push our newly tagged image: . docker push localhost.localdomain:5000/ubuntu20-java14:latest . Take note of the sha256:&lt;sha256sum&gt; output that was produced with this command, as we will need to use it to specify our base. Now we have a referent docker image that can be identified as a base to apply buildfarm’s worker installation onto. In a suitable location, create a WORKSPACE containing: . load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") BUILDFARM_EXTERNAL_COMMIT = \"04fc2635f5546a4f5dc19dea35bb1fca5569ce24\" BUILDFARM_EXTERNAL_SHA256 = \"f45215ef075c8aff230b737ca3bc5ba183c1137787fcbbb10dd407463f76edb6\" http_archive( name = \"build_buildfarm\", strip_prefix = \"bazel-buildfarm-%s\" % BUILDFARM_EXTERNAL_COMMIT, sha256 = BUILDFARM_EXTERNAL_SHA256, url = \"https://github.com/bazelbuild/bazel-buildfarm/archive/%s.zip\" % BUILDFARM_EXTERNAL_COMMIT, ) load(\"@build_buildfarm//:deps.bzl\", \"buildfarm_dependencies\") buildfarm_dependencies() load(\"@build_buildfarm//:defs.bzl\", \"buildfarm_init\") buildfarm_init() load(\"@io_bazel_rules_docker//repositories:deps.bzl\", container_deps = \"deps\") load(\"@io_bazel_rules_docker//container:container.bzl\", \"container_pull\") container_deps() container_pull( name = \"ubuntu20_java14_image_base\", digest = \"sha256:&lt;sha256sum&gt;\", registry = \"localhost:5000\", repository = \"ubuntu20-java14\", ) . Be sure to substitute the sha256:&lt;sha256sum&gt; with your content from above. Next we will create a BUILD file to create our target image. We will use the shard variant here, but the operationqueue worker (with supporting server execution) will work as well. The content of the BUILD file should be: . load(\"@io_bazel_rules_docker//container:container.bzl\", \"container_image\") java_image( name = \"buildfarm-shard-worker-ubuntu20-java14\", base = \"@ubuntu20_java14_image_base//image\", files = [ \"@build_buildfarm//src/main/java/build/buildfarm:buildfarm-shard-worker_deploy.jar\", ], ) . And now that this is in place, we can use the following to build the container and make it available to our local docker daemon: . bazel run :buildfarm-shard-worker-ubuntu20-java14 . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/environment/",
    "relUrl": "/docs/execution/environment/"
  },"41": {
    "doc": "Owner",
    "title": "Owner",
    "content": "exec_owner is a global worker configuration option for Shard workers which indicates a username on the worker that all output directories for executions will own. The InputFetchStage uses this value to specify the filesystem owner user id for Action-&gt;Command OutputFiles and OutputDirectories (soon to be OutputPaths in REAPI 2.1) paths created during exec root preparation. Specification of this option requires that the filesystem ownership change is available to the worker process principal. For example, a worker which executes as root under Unix would be capable of changing directory ownership. This option only affects file ownership, not effective user id of the action process. This option can be coupled with a default ExecutionPolicy wrapper that performs an execution persona change (i.e. setuid(2) under Unix), to execute a command with privileges that limit it in writing. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/exec_owner/",
    "relUrl": "/docs/execution/exec_owner/"
  },"42": {
    "doc": "Execution",
    "title": "Execution",
    "content": "Information about the execution of an action on buildfarm. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution/",
    "relUrl": "/docs/execution/execution/"
  },"43": {
    "doc": "Limiting",
    "title": "Limiting",
    "content": "The ExecuteActionStage on Shard Workers can limit the resources available during Command execution per the Platform. This feature currently requires OS-specific cgroups support, available in Linux. The boolean configuration field limit_execution controls the creation of a worker-wide cgroup. This cgroup currently limits cpu resources to the the ExecuteStageWidth (one core per slot), and other worker-wide cgroup controls may be available in the future. Other limiting fields also require this option to be enabled. This does not mandate that all executions take place in this created cgroup. If true, the boolean configuration field limit_global_execution specifies that all executions take place in the above cgroup by default. This currently shrinks the effective cpu area available to be shared by all executions to the width of the execute stage. With limit_execution enabled, a Command may request that it be executed with min-cores available to it (lower bound) and up to max-cores. These are field names in the Platform which are expected to be integers in the string value. After selective consumption of the superscalar execution slots for min-cores, the executing processes will be controlled by cgroups under the above cgroup to give it as many shares of the total (preferred scheduling) for min-cores, and have its scheduling quota limited to a fair number consistent with its max-cores. The effective Platform of an execution can also be affected by the Worker’s default_platform, where it can inherit either of min/max-cores if unspecified. The Worker configuration field only_multicore_tests can further affect the values of min/max-cores by clamping them to 1 if enabled, based on the detection of ‘bazel-like’ test environments for executions: The detection of an environment variable XML_OUTPUT_FILE under the command is the current qualifier . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_limiting/",
    "relUrl": "/docs/execution/execution_limiting/"
  },"44": {
    "doc": "Policies",
    "title": "Core Selection:",
    "content": "min-cores . description: the minimum number of cores needed by an action. Should be set to &gt;= 1 Workers and queues can be configured to behave differently based on this property. max-cores . description: the maximum number of cores needed by an action. Buildfarm will enforce a max. Workers and queues can be configured to behave differently based on this property. cores . description: the minimum &amp; maximum number of cores needed by an action. This sets both min-cores and max-cores accordingly. use case: very often you want unit tests (or all actions in general) to be constrained to a core limit via cgroups. This is relevant for performance and stability of the worker as multiple tests share the same hardware as the worker. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_policies/#core-selection",
    "relUrl": "/docs/execution/execution_policies/#core-selection"
  },"45": {
    "doc": "Policies",
    "title": "Memory Selection:",
    "content": "min-mem . description: the minimum amount of bytes the action may use. max-mem . description: the maximum amount of bytes the action may use. use case: very often you want unit tests (or all actions in general) to be constrained to a memory limit via cgroups. This is relevant for performance and stability of the worker as multiple tests share the same hardware as the worker. Tests that exceed their memory requirements will be killed. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_policies/#memory-selection",
    "relUrl": "/docs/execution/execution_policies/#memory-selection"
  },"46": {
    "doc": "Policies",
    "title": "Execution Settings:",
    "content": "linux-sandbox . description: Use bazel’s linux sandbox as an execution wrapper. block-network . description: Creates a new network namespace. Assumes the usage of the linux sandbox. tmpfs . description: Mounts an empty tmpfs under /tmp for the action. Assumes the usage of the linux sandbox. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_policies/#execution-settings",
    "relUrl": "/docs/execution/execution_policies/#execution-settings"
  },"47": {
    "doc": "Policies",
    "title": "Queue / Pool Selection:",
    "content": "choose-queue . description: place the action directly on the chosen queue (queue name must be known based on buildfarm configuration). use case: Other remote execution solutions have slightly different paradigms on deciding where actions go. They leverage execution properties for selecting a “pool” of machines to send the action. We sort of have a pool of workers waiting on particular queues. For parity with this concept, we support this execution property which will take precedence in deciding queue eligibility. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_policies/#queue--pool-selection",
    "relUrl": "/docs/execution/execution_policies/#queue--pool-selection"
  },"48": {
    "doc": "Policies",
    "title": "Extending Execution:",
    "content": "env-var / env-vars . description: ensure the action is executed with additional environment variables. These variables are applied last in the order given. env-var expects a single key/value like --remote_default_exec_properties=env-var:FOO=VALUE env-vars expects a key/json like --remote_default_exec_properties=env-vars='{\"FOO\": \"VALUE\",\"FOO2\": \"VALUE2\"}' . use case: Users may need to set additional environment variables through exec_properties. Changing code or using --action_env may be less feasible than specifying them through these exec_properties. Additionally, the values of their environment variables may need to be influenced by buildfarm decisions. example: pytorch tests can still see the underlying hardware through /proc/cpuinfo. Despite being given 1 core, they see all of the cpus and decide to spawn that many threads. This essentially starves them and gives poor test performance (we may spoof cpuinfo in the future). Another solution is to use env vars OMP_NUM_THREADS and MKL_NUM_THREADS. This could be done in code, but we can’t trust that developers will do it consistently or keep it in sync with min-cores / max-cores. Allowing these environment variables to be passed the same way as the core settings would be ideal. Standard Example: This test will succeed when env var TESTVAR is foobar, and fail otherwise. #!/bin/bash [ \"$TESTVAR\" = \"foobar\" ] ./bazel test \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main FAIL ./bazel test --remote_default_exec_properties='env-vars={\"TESTVAR\": \"foobar\"}' \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main PASS . Template Example: If you give a range of cores, buildfarm has the authority to decide how many your operation actually claims. You can let buildfarm resolve this value for you (via mustache). #!/bin/bash [ \"$MKL_NUM_THREADS\" = \"1\" ] ./bazel test \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main FAIL ./bazel test \\ --remote_default_exec_properties='env-vars=\"MKL_NUM_THREADS\": \"\"' \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main PASS . Available Templates: : what buildfarm has decided is a valid min core count for the action. : what buildfarm has decided is a valid max core count for the action. ``: buildfarm’s decision on how many cores your action should claim. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_policies/#extending-execution",
    "relUrl": "/docs/execution/execution_policies/#extending-execution"
  },"49": {
    "doc": "Policies",
    "title": "Debugging Execution:",
    "content": "debug-before-execution . description: Fails the execution with important debug information on how the execution will be performed. use case: Sometimes you want to know the exact execution context and cli that the action is going to be run with. This can help any situation where local action behavior seems different than remote action behavior. debug-after-execution . description: Runs the execution, but fails it afterward with important debug information on how the execution was performed. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_policies/#debugging-execution",
    "relUrl": "/docs/execution/execution_policies/#debugging-execution"
  },"50": {
    "doc": "Policies",
    "title": "Additional Information",
    "content": "Custom properties can also be added to buildfarm’s configuration in order to facilitate queue matching (see Platform Queues). Please note that not all execution properties may be relevant to you or the best option depending on your build client. For example, some execution properties were created to facilitate behavior before bazel had a better solution in place. Buildfarm’s configuration for accepting execution properties can be strict or flexible. Buildfarm has been used alongside other remote execution tools and allowing increased flexibility on these properties is necessary so the solutions can coexist for the same targets. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_policies/#additional-information",
    "relUrl": "/docs/execution/execution_policies/#additional-information"
  },"51": {
    "doc": "Policies",
    "title": "Policies",
    "content": "This page contains all of the execution properties supported by Buildfarm. Users can also customize buildfarm to understand additional properties that are not listed here (This is often done when configuring the Operation Queue). ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_policies/",
    "relUrl": "/docs/execution/execution_policies/"
  },"52": {
    "doc": "Properties",
    "title": "Core Selection:",
    "content": "min-cores . description: the minimum number of cores needed by an action. Should be set to &gt;= 1 Workers and queues can be configured to behave differently based on this property. max-cores . description: the maximum number of cores needed by an action. Buildfarm will enforce a max. Workers and queues can be configured to behave differently based on this property. use case: very often you want unit tests (or all actions in general) to be constrained to a core limit via cgroups. This is relevant for performance and stability of the worker as multiple tests share the same hardware as the worker. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_properties/#core-selection",
    "relUrl": "/docs/execution/execution_properties/#core-selection"
  },"53": {
    "doc": "Properties",
    "title": "Queue / Pool Selection:",
    "content": "choose-queue . description: place the action directly on the chosen queue (queue name must be known based on buildfarm configuration). use case: Other remote execution solutions have slightly different paradigms on deciding where actions go. They leverage execution properties for selecting a “pool” of machines to send the action. We sort of have a pool of workers waiting on particular queues. For parity with this concept, we support this execution property which will take precedence in deciding queue eligibility. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_properties/#queue--pool-selection",
    "relUrl": "/docs/execution/execution_properties/#queue--pool-selection"
  },"54": {
    "doc": "Properties",
    "title": "Extending Execution:",
    "content": "env-var / env-vars . description: ensure the action is executed with additional environment variables. These variables are applied last in the order given. env-var expects a single key/value like --remote_default_exec_properties=env-var:FOO=VALUE env-vars expects a key/json like --remote_default_exec_properties=env-vars='{\"FOO\": \"VALUE\",\"FOO2\": \"VALUE2\"}' . use case: Users may need to set additional environment variables through exec_properties. Changing code or using --action_env may be less feasible than specifying them through these exec_properties. Additionally, the values of their environment variables may need to be influenced by buildfarm decisions. example: pytorch tests can still see the underlying hardware through /proc/cpuinfo. Despite being given 1 core, they see all of the cpus and decide to spawn that many threads. This essentially starves them and gives poor test performance (we may spoof cpuinfo in the future). Another solution is to use env vars OMP_NUM_THREADS and MKL_NUM_THREADS. This could be done in code, but we can’t trust that developers will do it consistently or keep it in sync with min-cores / max-cores. Allowing these environment variables to be passed the same way as the core settings would be ideal. Standard Example: This test will succeed when env var TESTVAR is foobar, and fail otherwise. #!/bin/bash [ \"$TESTVAR\" = \"foobar\" ] ./bazel test \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main FAIL ./bazel test --remote_default_exec_properties='env-vars={\"TESTVAR\": \"foobar\"}' \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main PASS . Template Example: If you give a range of cores, buildfarm has the authority to decide how many your operation actually claims. You can let buildfarm resolve this value for you (via mustache). #!/bin/bash [ \"$MKL_NUM_THREADS\" = \"1\" ] ./bazel test \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main FAIL ./bazel test \\ --remote_default_exec_properties='env-vars=\"MKL_NUM_THREADS\": \"\"' \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main PASS . Available Templates: : what buildfarm has decided is a valid min core count for the action. : what buildfarm has decided is a valid max core count for the action. ``: buildfarm’s decision on how many cores your action should claim. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_properties/#extending-execution",
    "relUrl": "/docs/execution/execution_properties/#extending-execution"
  },"55": {
    "doc": "Properties",
    "title": "Debugging Execution:",
    "content": "debug-before-execution . description: Fails the execution with important debug information on how the execution will be performed. use case: Sometimes you want to know the exact execution context and cli that the action is going to be run with. This can help any situation where local action behavior seems different than remote action behavior. debug-after-execution . description: Runs the execution, but fails it afterward with important debug information on how the execution was performed. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_properties/#debugging-execution",
    "relUrl": "/docs/execution/execution_properties/#debugging-execution"
  },"56": {
    "doc": "Properties",
    "title": "Additional Information",
    "content": "Custom properties can also be added to buildfarm’s configuration in order to facilitate queue matching (see Platform Queues). Please note that not all execution properties may be relevant to you or the best option depending on your build client. For example, some execution properties were created to facilitate behavior before bazel had a better solution in place. Buildfarm’s configuration for accepting execution properties can be strict or flexible. Buildfarm has been used alongside other remote execution tools and allowing increased flexibility on these properties is necessary so the solutions can coexist for the same targets. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_properties/#additional-information",
    "relUrl": "/docs/execution/execution_properties/#additional-information"
  },"57": {
    "doc": "Properties",
    "title": "Properties",
    "content": "This page contains all of the execution properties supported by Buildfarm. Users can customize buildfarm to understand additional properties that are not listed here. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/execution/execution_properties/",
    "relUrl": "/docs/execution/execution_properties/"
  },"58": {
    "doc": "Features",
    "title": "General Features",
    "content": "Buildfarm has endeavored to support a wide variety of features implied or mandated by the Remote Execution API, including those currently not in use or worked around by bazel or other clients. Most notably, buildfarm has universal support for: . | configurable instances with specific instance types | progressive and flow controlled CAS reads and writes | pluggable external CAS endpoints | RequestMetadata behavior attribution | . Bazel Client Feature Usage: . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/features/#general-features",
    "relUrl": "/docs/features/#general-features"
  },"59": {
    "doc": "Features",
    "title": "Fetch API - Planned",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/features/#fetch-api---planned",
    "relUrl": "/docs/features/#fetch-api---planned"
  },"60": {
    "doc": "Features",
    "title": "Builds Without The Bytes - Read This",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/features/#builds-without-the-bytes---read-this",
    "relUrl": "/docs/features/#builds-without-the-bytes---read-this"
  },"61": {
    "doc": "Features",
    "title": "Features",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/features/",
    "relUrl": "/docs/features/"
  },"62": {
    "doc": "Home",
    "title": "Bazel Buildfarm",
    "content": "Remote Execution . Get started now View it on GitHub . ",
    "url": "http://localhost:4000/bazel-buildfarm/#bazel-buildfarm",
    "relUrl": "/#bazel-buildfarm"
  },"63": {
    "doc": "Home",
    "title": "What is buildfarm?",
    "content": "Buildfarm is a service software stack which presents an implementation of the Remote Execution API. This means it can be used by any client of that API to retain content ContentAddressableStorage, cache ActionResults by a key ActionCache, and execute actions asynchronously Execution. Buildfarm is cross-platform has been heavily tested with bazel as a client. This documentation is a comprehensive description of the architecture, features, functionality, and operation, as well as a guide to a smoothly installing and running the software. Familiarity with the Remote Execution API is expected, and references to it will be provided as needed. ",
    "url": "http://localhost:4000/bazel-buildfarm/#what-is-buildfarm",
    "relUrl": "/#what-is-buildfarm"
  },"64": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/",
    "relUrl": "/"
  },"65": {
    "doc": "Instance Types",
    "title": "Instance Types",
    "content": "These are the supported instance types selectable for concrete definition of an instance declared for servers in config. The types here are implementations of the Instance interface. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/instance_types/",
    "relUrl": "/docs/architecture/instance_types/"
  },"66": {
    "doc": "Instance Types",
    "title": "Memory",
    "content": "This is a reference implementation of the Remote Execution API which provides an in-memory CAS, AC, and OperationQueue. It has no persistent retention of its state internally, but can be configured to use an external gRPC endpoint for CAS operations, allowing it to act as a proxy. Instances of this type cannot share Operation information across multiple servers/hosts. It presents a matching interface for workers via an OperationQueue service definition, which provides blocking queue take functionality, as well as a put for results, and it maintains watchdogs for all outstanding operations, with expiration resulting in reentrance to the queue, assuming that all input preconditions are still met at that time. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/instance_types/#memory",
    "relUrl": "/docs/architecture/instance_types/#memory"
  },"67": {
    "doc": "Instance Types",
    "title": "Shard",
    "content": "The shard instance type is a frontend for a set of common backplane operations, allowing for wide distribution of retention and execution. The backplane serves as a registry of shard workers, the storage for the AC, the various queues and event monitors used in Operation processing, and the CAS index. The only current backplane implementation uses redis, but the backplane interface is strongly decoupled from its usage, and any single or composite communication layer may be used to satisfy its requirements. Sharded instances select arbitrary members of the Workers set for CAS writes, and reference the backplane CAS index for CAS reads, selecting any of a set of shards that advertise content to retrieve it. These shards can be any gRPC CAS compatible service endpoint. Executions on shard are transformed into a worker queue through a processor built into the instance. A server which runs the instance (configurably) participates in this pool, reducing load on a directly connected service to the client. This allows an entire cluster of servers to participate evenly in populating the worker queue with heavyweight operation definitions, even if a client is only communicating with a single host. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/instance_types/#shard",
    "relUrl": "/docs/architecture/instance_types/#shard"
  },"68": {
    "doc": "Metrics",
    "title": "Prometheus Configuration",
    "content": "To enable emitting of Prometheus metrics, add the following configuration to your configuration file: . ` prometheus_config: { port: 9090 } ` . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/metrics/metrics/#prometheus-configuration",
    "relUrl": "/docs/metrics/metrics/#prometheus-configuration"
  },"69": {
    "doc": "Metrics",
    "title": "Available Prometheus Metrics",
    "content": "remote_invocations . Counter for the number of invocations of the capabilities service . expired_key . Counter for key expirations . execution_success . Counter for the number of successful executions . pre_queue_size . Gauge of a number of items in prequeue . dispatched_operations_size . Gauge of the number of dispatched operations . dispatched_operations_build_amount . Gauge for the number of dispatched operations that are build actions . dispatched_operations_test_amount . Gauge for the number of dispatched operations that are test actions . dispatched_operations_unknown_amount . Gauge for the number of dispatched operations that could not be identified as build / test . dispatched_operations_from_queue_amount . Gauge for the number of dispatched operations that came from each queue (using “queue_name” as label) . dispatched_operations_tools_amount . Gauge for the number of dispatched operations by tool name (using “tool_name” as label) . dispatched_operations_mnemonics_amount . Gauge for the number of dispatched operations by mnemonic (using “mnemonic” as label) . dispatched_operations_command_tools . Gauge for the number of dispatched operations by cli tool (using “tool” as label) . dispatched_operations_targets_amount . Gauge for the number of dispatched operations by target (using “target” as label) . dispatched_operations_config_amount . Gauge for the number of dispatched operations by config (using “config” as label) . dispatched_operations_platform_properties . Gauge for the number of dispatched operations by platform properties (using “config” as label) . dispatched_operations_clients_being_served . The number of build clients currently being served . dispatched_operations_requeued_operations_amount . The number of dispatched operations that have been requeued . worker_pool_size . Gauge of the number of workers available . queue_size . Gauge of the size of the queue (using a queue_name label for each individual queue) . blocked_actions_size . Gauge of the number of blocked actions . blocked_invocations_size . Gauge of the number of blocked invocations . actions . Counter for the number of actions processed . operations_stage_load . Gauge for the number of operations in each stage (using a stage_name for each individual stage) . operation_status . Gauge for the completed operations status (using a status_code label for each individual GRPC code) . operation_worker . Gauge for the number of operations executed on each worker (using a worker_name label for each individual worker) . action_results . Counter for the number of action results . missing_blobs . Histogram for the number of missing blobs . execution_slot_usage . Gauge for the number of execution slots used on each worker . execution_time_ms . Histogram for the execution time on a worker (in milliseconds) . execution_stall_time_ms . Histogram for the execution stall time on a worker (in milliseconds) . input_fetch_slot_usage . Gauge for the number of input fetch slots used on each worker . input_fetch_time_ms . Histogram for the input fetch time on a worker (in milliseconds) . input_fetch_stall_time_ms . Histogram for the input fetch stall time on a worker (in milliseconds) . queued_time_ms . Summary for the operation queued time (in milliseconds) . output_upload_time_ms . Summary for the output upload time (in milliseconds) . completed_operations . Counter for the number of completed operations . operation_poller . Counter for the number of operations being polled . io_bytes . Histogram for the bytes read/written to get system I/O . health_check . Counter showing service restarts . cas_size . Total size of the worker’s CAS in bytes . cas_entry_count . The total number of entries in the worker’s CAS . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/metrics/metrics/#available-prometheus-metrics",
    "relUrl": "/docs/metrics/metrics/#available-prometheus-metrics"
  },"70": {
    "doc": "Metrics",
    "title": "Metrics",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/metrics/metrics/",
    "relUrl": "/docs/metrics/metrics/"
  },"71": {
    "doc": "Queues",
    "title": "Operation Queue",
    "content": "This section discusses the purpose and design of the Operation queue. It also discusses how it can be customized depending on the type of operations you wish to support, and how you wish to distribute them among workers. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/queues/#operation-queue",
    "relUrl": "/docs/architecture/queues/#operation-queue"
  },"72": {
    "doc": "Queues",
    "title": "Quick Summary",
    "content": "Some time after an Action execute request occurs, the longrunning operation it corresponds to will enter the QUEUED state, and will receive an update to that effect on the operation response stream. An operation in the QUEUED state is present in an Operation Queue, which holds the operations in sequence until a worker is available to execute it. Schedulers put operations on the queue. Workers take them off. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/queues/#quick-summary",
    "relUrl": "/docs/architecture/queues/#quick-summary"
  },"73": {
    "doc": "Queues",
    "title": "Working with different platform requirements",
    "content": "Some operations’ Actions may have specific platform requirements in order to execute. Likewise, specific workers may only want to take on work that they deem eligible. To solve this, the operation queue can be customized to divide work into separate provisioned queues so that specific workers can choose which queue to read from. Provision queues are intended to represent particular operations that should only be processed by particular workers. An example use case for this would be to have two dedicated provision queues for CPU and GPU operations. CPU/GPU requirements would be determined through the remote api’s command platform properties. We designate provision queues to have a set of “required provisions” (which match the platform properties). This allows the scheduler to distribute operations by their properties and allows workers to dequeue from particular queues. If your configuration file does not specify any provisioned queues, buildfarm will automatically provide a default queue with full eligibility on all operations. This will ensure the expected behavior for the paradigm in which all work is put on the same queue. Matching Algorithm . The matching algorithm is performed by the operation queue when the caller is requesting to push or pop elements. The matching algorithm is designed to find the appropriate queue to perform these actions on. On the scheduler side, the action’s platform properties are used for matching. On the worker side, the dequeue_match_settings are used. This is how the matching algorithm works: Each provision queue is checked in the order that it is configured. The first provision queue that is deemed eligible is chosen and used. When deciding if an action is eligible for the provision queue, each platform property is checked individually. By default, there must be a perfect match on each key/value. Wildcards (“*”) can be used to avoid the need of a perfect match. Additionally, if the action contains any platform properties is not mentioned by the provision queue, it will be deemed ineligible. setting allow_unmatched: true can be used to allow a superset of action properties as long as a subset matches the provision queue. If no provision queues can be matched, the operation queue will provide an analysis on why none of the queues were eligible. When taking elements off of the operation queue, the matching algorithm behaves a similar way. The worker’s DequeueMatchSettings also have an allow_unmatched property. Workers also have the ability to reject an operation after matching with a provision queue and dequeuing a value. To avoid any of these rejections by the worker, you can use accept_everything: true. When configuring your worker, consider the following decisions: First, if the accept_everything setting is true, the job is accepted. Otherwise, if any execution property for the queue has a wildcard key, the job is accepted. Otherwise, if the allow_unmatched setting is true, each key present in the queue’s properties must be a wildcard or exist in the execution request’s properties with an equal value. Otherwise, the execution request’s properties must have exactly the same set of keys as the queue’s execution properties, and the request’s value for each property must equal the queue’s if the queue’s value for this property is not a wildcard. Server Example . In this example the scheduler declares a GPU queue and CPU queue: . redis_shard_backplane_config: { provisioned_queues: { queues: [ { name: \"gpu_queue\" platform: { properties: [{name: \"gpu\" value: \"1\"}] } }, { name: \"cpu_queue\" } ] } } . Worker Example . Queues are defined similarly on Workers: . redis_shard_backplane_config: { provisioned_queues: { queues: [ { name: \"gpu_queue\" platform: { properties: [{name: \"gpu\" value: \"1\"}] } }, { name: \"cpu_queue\" } ] } } . In addition to the queue declaration, the worker must specify from which queue tasks are obtained (otherwise the worker process will refuse to start): . dequeue_match_settings: { platform: { properties: [{name: \"gpu\" value: \"1\"}] } } . Note: in both of these examples we specify the CPU queue last. Since operation queues consist of multiple provisioned queues in which the order dictates the eligibility and placement of operations, it is recommended to have a final provision queue with no actual platform requirements. This ensures that all operations are eligible for the final queue. Note: make sure that all workers can communicate with each other before trying these examples . Bazel Perspective . Bazel targets can pass these platform properties to buildfarm via exec_properties. Here is for example how to run a remote build for the GPU queue example above: . bazel build --remote_executor=grpc://server:port --remote_default_exec_properties=gpu=1 //... ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/queues/#working-with-different-platform-requirements",
    "relUrl": "/docs/architecture/queues/#working-with-different-platform-requirements"
  },"74": {
    "doc": "Queues",
    "title": "Queues",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/queues/",
    "relUrl": "/docs/architecture/queues/"
  },"75": {
    "doc": "Quick Start",
    "title": "Quick Start",
    "content": "Here we describe how to use bazel remote caching or remote execution with buildfarm. We’ll start by creating a single workspace that can be used for both. Let’s start with a bazel workspace with a single file to compile into an executable: . Create a new directory for our workspace and add the following files: . main.cc: . #include &lt;iostream&gt; int main( int argc, char *argv[] ) { std::cout &lt;&lt; \"Hello, World!\" &lt;&lt; std::endl; } . BUILD: . cc_binary( name = \"main\", srcs = [\"main.cc\"], ) . And an empty WORKSPACE file. As a test, verify that bazel run :main builds your main program and runs it, and prints Hello, World!. This will ensure that you have properly installed bazel and a C++ compiler, and have a working target before moving on to remote execution. Download and extract the buildfarm repository. Each command sequence below will have the intended working directory indicated, between the client (workspace running bazel), and buildfarm. This tutorial assumes that you have a bazel binary in your path and you are in the root of your buildfarm clone/release, and has been tested to work with bash on linux. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/quick_start/",
    "relUrl": "/docs/quick_start/"
  },"76": {
    "doc": "Quick Start",
    "title": "Remote Caching",
    "content": "A Buildfarm server with an instance can be used strictly as an ActionCache and ContentAddressableStorage to improve build performance. This is an example of running a bazel client that will retrieve results if available, and store them if the cache is missed and the execution needs to run locally. Download the buildfarm repository and change into its directory, then: . run bazel run src/main/java/build/buildfarm:buildfarm-server $PWD/examples/server.config.example . This will wait while the server runs, indicating that it is ready for requests. From another prompt (i.e. a separate terminal) in your newly created workspace directory from above: . run bazel clean run bazel run --remote_cache=grpc://localhost:8980 :main . Why do we clean here? Since we’re verifying re-execution and caching, this ensures that we will execute any actions in the run step and interact with the remote cache. We should be attempting to retrieve cached results, and then when we miss - since we just started this memory resident server - bazel will upload the results of the execution for later use. There will be no change in the output of this bazel run if everything worked, since bazel does not provide output each time it uploads results. To prove that we have placed something in the action cache, we need to do the following: . run bazel clean run bazel run --remote_cache=localhost:8980 :main . This should now print statistics on the processes line that indicate that you’ve retrieved results from the cache for your actions: . INFO: 2 processes: 2 remote cache hit. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/quick_start/#remote-caching",
    "relUrl": "/docs/quick_start/#remote-caching"
  },"77": {
    "doc": "Quick Start",
    "title": "Remote Execution (and caching)",
    "content": "Now we will use buildfarm for remote execution with a minimal configuration - a single memory instance, with a host-colocated worker that can execute a single process at a time - via a bazel invocation on our workspace. First, we should restart the buildfarm server to ensure that we get remote execution (this can also be forced from the client by using --noremote_accept_cached). From the buildfarm server prompt and directory: . interrupt a running buildfarm-server run bazel run src/main/java/build/buildfarm:buildfarm-server $PWD/examples/server.config.example . From another prompt in the buildfarm repository directory: . run bazel run src/main/java/build/buildfarm:buildfarm-operationqueue-worker $PWD/examples/worker.config.example . From another prompt, in your client workspace: . run bazel run --remote_executor=grpc://localhost:8980 :main . Your build should now print out the following on its processes summary line: . INFO: 2 processes: 2 remote. That 2 remote indicates that your compile and link ran remotely. Congratulations, you just build something through remote execution! . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/quick_start/#remote-execution-and-caching",
    "relUrl": "/docs/quick_start/#remote-execution-and-caching"
  },"78": {
    "doc": "Quick Start",
    "title": "Buildfarm Manager",
    "content": "You can now easily launch a new Buildfarm cluster locally or in AWS using an open sourced Buildfarm Manager. wget https://github.com/80degreeswest/bfmgr/releases/download/1.0.7/bfmgr-1.0.7.jar java -jar bfmgr-1.0.7.jar Navigate to http://localhost . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/quick_start/#buildfarm-manager",
    "relUrl": "/docs/quick_start/#buildfarm-manager"
  },"79": {
    "doc": "Redis",
    "title": "Redis",
    "content": "Redis is used as the in-memory database between sharded actors. Buildfarm’s backplane uses a Jedis Cluster for various abstractions. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/redis/",
    "relUrl": "/docs/architecture/redis/"
  },"80": {
    "doc": "Redis",
    "title": "Balanced Queues",
    "content": "To balance CPU utilization across multiple nodes in a redis cluster, we distribute operations through redis hashtags. We have a conceptual queue that uses multiple redis lists in its implementation. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/redis/#balanced-queues",
    "relUrl": "/docs/architecture/redis/#balanced-queues"
  },"81": {
    "doc": "Schedulers",
    "title": "Schedulers",
    "content": "Information about schedulers. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/schedulers/",
    "relUrl": "/docs/architecture/schedulers/"
  },"82": {
    "doc": "Tools",
    "title": "Tools",
    "content": "Information about tools that can be used alongside buildfarm. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/tools/tools/",
    "relUrl": "/docs/tools/tools/"
  },"83": {
    "doc": "Troubleshooting",
    "title": "bazel logging",
    "content": "Use bazel [build|run|test] --experimental_remote_grpc_log=&lt;filename&gt; to produce a binary log of all of the grpc activity bazel performs during an invocation. This log is written to at the completion of each request, and may not contain a complete picture if a build is interrupted, or a request is currently ongoing. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/tools/troubleshooting-bazel-remote-execution/#bazel-logging",
    "relUrl": "/docs/tools/troubleshooting-bazel-remote-execution/#bazel-logging"
  },"84": {
    "doc": "Troubleshooting",
    "title": "Dumping the log",
    "content": "Use tools_remote. Build the remote_client in it with bazel build :remote_client. Since we’re going to point to a local file, it might be easiest to use the bazel-bin/remote_client executable to perform a text dump of the previously generated bazel log file: . $ bazel-bin/remote_client --grpc_log=&lt;filename&gt; printlog . Warning: the original log file may be big, and the corresponding text output may be similarly big. This text output is invaluable, however, when trying to associate or distinguish the activities of a single build, and we strongly recommend that users report bugs with these logs attached, in either form. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/tools/troubleshooting-bazel-remote-execution/#dumping-the-log",
    "relUrl": "/docs/tools/troubleshooting-bazel-remote-execution/#dumping-the-log"
  },"85": {
    "doc": "Troubleshooting",
    "title": "The log file",
    "content": "The log file retains all of the Calls (i.e. Requests and Responses) for (at least) content related to remote execution. These calls will have either no status code response (0, OK, with the default proto value for ints omitted), or a numeric status code. Use https://github.com/grpc/grpc/blob/master/doc/statuscodes.md to reference these until tools_remote can print a nicer representation. Also included in each call is the timestamp of the call, its final response, and for streaming calls, some information about how the call proceeded through retries and progressive activity. There is also RequestMetadata information printed for each request to indicate which correlated build id, build id, and action a call is associated with. The action is consistent across all of the requests for a single action, and is vital to group the disparate calls within a log to the same action context. It also helps to establish an expected flow of the sequence of calls. You will see some (or none) of the following Calls in this log file: . Capabilities::GetCapabilities - asking the remote service what is supported. ActionCache::GetActionResult - a call to the action cache service to retrieve an action result. This takes a ‘digest key’, a hex number whose size depends on your hash policy, likely the default SHA256, packaged with the size of the original content, looking something like a948904f2f0f479b8f8197694b30184b0d2ed1c1cd2a1ec0fb85d299a192a447/42. The responses, if successful, will contain the results of an action execution, and constitute a cache HIT. A NOT_FOUND status response corresponds to a cache MISS. ByteStream::Read - Each one of these is a download for a digest that, very likely, was reported in an action result. These likewise use digests in their resource_names, and if you correspond the digest in a Read to a digest in an Action Result, you will learn the name of a file being downloaded. ContentAddressableStorage::FindMissingBlob - The client is likely asking the remote system which of a set of digests does NOT exist in the CAS. An empty response means that everything is present remotely already. Any blobs returned will need to be uploaded by the client. You can find the action key in this set, at a minimum. This call occurs before any uploads of content, because it drives those calls, within an action’s context. ByteStream::Write - Each of these is an upload for a digest that, very likely, was returned in a FindMissingBlobs response. Digests are in their resource_names again, with an additional uuid to indicate a particular client’s upload of a file. Execution Execute - this is the call that directs the remote execution system to execute an action. Each response will be listed as the execution moves through one of several states on the remote side, likely culminating in COMPLETED. Note that this does not mean a successful execution, only that an operation is done executing, and now has its exit status, and outputs, if any, available for download in the CAS. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/tools/troubleshooting-bazel-remote-execution/#the-log-file",
    "relUrl": "/docs/tools/troubleshooting-bazel-remote-execution/#the-log-file"
  },"86": {
    "doc": "Troubleshooting",
    "title": "Troubleshooting",
    "content": "Remote execution is sometimes an exercise fraught with problems, whether they be client definition, specification of remote endpoints, interactions with layered systems, and the complex operation of running a configured remote installation. This guide serves to give you some of the tools to debug your problems, from a bazel client point of view. A typical use case: Something works locally, but breaks when remote execution is introduced. Beyond stdout, and execution instrumentation, you will probably want to find out what step along the path to remote operation responses. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/tools/troubleshooting-bazel-remote-execution/",
    "relUrl": "/docs/tools/troubleshooting-bazel-remote-execution/"
  },"87": {
    "doc": "Workers",
    "title": "Workers",
    "content": "Since workers are expected to execute programs in a way that makes using remote transparent to build users, there is a great deal of nuance to their definition. Operating System, Distribution, runtimes, compilers, their versions, and standard libraries make this a tricky proposition. While hermeticizing a build by declaring the full set of tools a compilation requires is the ‘right’ solution for a build, it may be too big of a hill to climb for everyone, particularly those trying to ease into remote execution. Under Linux at least, this is made somewhat easier through docker, even if the steps to get to a conformant environment are a bit complicated. Here we will provide an example of creating a target execution environment container capable of running buildfarm, and this should be similar for all major distributions, particularly those with released docker hub bases and standard package managers including java runtimes and toolchain software. You should choose the versions of relevant software at first based on the client environment you want to support. For this example, we’re going to assume a target of ubuntu-20 (focal), with a gcc9 compiler supporting C++, and a java runtime from openjdk-14 supplied by its package manager. We will need: . | a bazel 3.3.1 install (the older version is required by rules_docker at the buildfarm version, this will be updated eventually). | docker daemon running | . First we will pull our intended base image from dockerhub: . docker pull ubuntu:focal . Next we will create our Dockerfile for our customized environment. Here is the content of that Dockerfile . # choose our ubuntu distribution with version 20 (focal) from ubuntu:focal # get our current software suite. apt-get is preferred to apt due to warnings # about a non-stable CLI run apt-get update # install the basic apt-utils to limit warnings due to its absence run DEBIAN_FRONTEND=noninteractive apt-get install --no-install-suggests apt-utils # upgrade to the current software suite versions run DEBIAN_FRONTEND=noninteractive apt-get dist-upgrade -y # install the required packages for our execution environment and buildfarm runtime run DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-suggests \\ g++-9 g++ openjdk-14-jdk-headless . And build it with docker build -t ubuntu20-java14:latest . Sending build context to Docker daemon 2.56kB Step 1/5 : from ubuntu:focal ---&gt; 9140108b62dc ... Removing intermediate container 505236be00e4 ---&gt; 77a8c2ae4d16 Successfully built 77a8c2ae4d16 Successfully tagged ubuntu20-java14:latest . In our example’s case, we want to use this image as a base to build a buildfarm worker container image. Unfortunately, rules_docker does not know how to interact with the docker images hosted by a local docker daemon, only docker registries like dockerhub.io. We will get past this by running the local registry utility container. If you really want to host this base elsewhere, I’ll assume that you know how to translate the host:port references below, and you can skip running your own registry. First we need to tag our image so that it gets pushed to our local registry: . docker tag ubuntu20-java14:latest localhost.localdomain:5000/ubuntu20-java14:latest . Then we can start our registry with a recognizable name for later shutdown: . docker run -d --rm --name local-registry -p 5000:5000 registry . And push our newly tagged image: . docker push localhost.localdomain:5000/ubuntu20-java14:latest . Take note of the sha256:&lt;sha256sum&gt; output that was produced with this command, as we will need to use it to specify our base. Now we have a referent docker image that can be identified as a base to apply buildfarm’s worker installation onto. In a suitable location, create a WORKSPACE containing: . load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") BUILDFARM_EXTERNAL_COMMIT = \"04fc2635f5546a4f5dc19dea35bb1fca5569ce24\" BUILDFARM_EXTERNAL_SHA256 = \"f45215ef075c8aff230b737ca3bc5ba183c1137787fcbbb10dd407463f76edb6\" http_archive( name = \"build_buildfarm\", strip_prefix = \"bazel-buildfarm-%s\" % BUILDFARM_EXTERNAL_COMMIT, sha256 = BUILDFARM_EXTERNAL_SHA256, url = \"https://github.com/bazelbuild/bazel-buildfarm/archive/%s.zip\" % BUILDFARM_EXTERNAL_COMMIT, ) load(\"@build_buildfarm//:deps.bzl\", \"buildfarm_dependencies\") buildfarm_dependencies() load(\"@build_buildfarm//:defs.bzl\", \"buildfarm_init\") buildfarm_init() load(\"@io_bazel_rules_docker//repositories:deps.bzl\", container_deps = \"deps\") load(\"@io_bazel_rules_docker//container:container.bzl\", \"container_pull\") container_deps() container_pull( name = \"ubuntu20_java14_image_base\", digest = \"sha256:&lt;sha256sum&gt;\", registry = \"localhost:5000\", repository = \"ubuntu20-java14\", ) . Be sure to substitute the sha256:&lt;sha256sum&gt; with your content from above. Next we will create a BUILD file to create our target image. We will use the shard variant here, but the memory worker (with supporting server execution) will work as well. The content of the BUILD file should be: . load(\"@io_bazel_rules_docker//java:image.bzl\", \"java_image\") java_image( name = \"buildfarm-shard-worker-ubuntu20-java14\", base = \"@ubuntu20_java14_image_base//image\", deps = [\"//src/main/java/build/buildfarm:buildfarm-shard-worker\"], main_class = \"build.buildfarm.worker.shard.Worker\", ) . And now that this is in place, we can use the following to build the container and make it available to our local docker daemon: . bazel run :buildfarm-shard-worker-ubuntu20-java14 . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/worker-execution-environment/",
    "relUrl": "/docs/architecture/worker-execution-environment/"
  },"88": {
    "doc": "Workers",
    "title": "Workers",
    "content": "Workers of all types throughout buildfarm are responsible for presenting execution roots to operations that they are matched with, fetching content from a CAS, executing those processes, and reporting the outputs and results of executions. Additionally, buildfarm supports some common behaviors across worker types: . | ExecutionPolicies, which allow for explicit and implicit behaviors to control execution. | A CAS FileCache, which is capable of reading through content for Digests of files or directories, and efficiently presenting those contents based on usage and reference counting, as well as support for cascading into delegate CASs. | Concurrent pipelined execution of operations, with support for superscalar stages at input fetch and execution. | Operation exclusivity, preventing the same operation from running through the worker pipeline concurrently. | . ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/workers/",
    "relUrl": "/docs/architecture/workers/"
  },"89": {
    "doc": "Workers",
    "title": "Worker Types",
    "content": " ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/workers/#worker-types",
    "relUrl": "/docs/architecture/workers/#worker-types"
  },"90": {
    "doc": "Workers",
    "title": "Operation Queue",
    "content": "Operation Queue workers are responsible for taking operations from the Memory OperationQueue service and reporting their contents via external CAS and AC services. Executions are the only driving force for their CAS FileCache. For more details on configuring the operation queue, see here. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/workers/#operation-queue",
    "relUrl": "/docs/architecture/workers/#operation-queue"
  },"91": {
    "doc": "Workers",
    "title": "Shard",
    "content": "Sharded workers interact with the shard backplane for both execution and CAS presentation. Their CAS FileCache serves a CAS gRPC interface as well as the execution root factory. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/workers/#shard",
    "relUrl": "/docs/architecture/workers/#shard"
  },"92": {
    "doc": "Workers",
    "title": "Pipelines",
    "content": "A pipeline handles operations as they arrive and are processed on a worker. Each stage of the pipeline performs its task on an operation, and holds that task until the subsequent stage can take over. This creates backpressure to mitigate risk and limit resource consumption. Since a worker only takes on enough work to exhaust the most limited resource at a time (CPU/IO/Bandwidth), losing that worker due to unforeseen failures is not disruptive to the rest of the cluster. Keeping work in each stage holds resources for an operation as well, so preventing operations from piling up in an earlier stage due to a longer running later stage reduces the overall resource footprint required for the worker. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/workers/#pipelines",
    "relUrl": "/docs/architecture/workers/#pipelines"
  },"93": {
    "doc": "Workers",
    "title": "Stages",
    "content": ". Stages have access to a WorkerContext provided to them by their Worker implementation (OperationQueue or Shard) which is used for all activity common across Worker types. Each stage must claim the subsequent stage before submitting it for processing. This allows measurement of processing time and latency for each operation per stage without any interleaving. Superscalar stages have a configurable number of slots for their activity. Claims on these stages block until they are full, and have a number of slots to claim exclusively for activity. Match . The Match stage is responsible for dequeuing an operation from the Ready-To-Run queue. This operation is dequeued as a QueueEntry which contains the ExecuteEntry and a Digest for the transformed QueuedOperation. The ExecuteEntry contains a Platform definition which must match the worker’s provided platform manifest in order to proceed. A rejected QueueEntry for this reason will be reinserted into the Ready-To-Run queue. The Match stage is unique in that it claims a slot in the Input Fetch stage prior to its iteration. This removes a polling requirement for the active operation present in other stages while waiting to feed the interstage, reducing the stage’s complexity. Input Fetch . Input Fetch is a superscalar stage responsible for downloading the QueuedOperation from the CAS, and creating the execution directory for the Operation. This is the worker ingress bandwidth, and likely the disk IO write, consuming stage. Its configured concurrency is available in the worker config as input_fetch_stage_width. The ownership of output directories is configurable with [[exec_owner]]. Execution . Execution is a superscalar stage which initiates operation executions, applying any ExecutionPolicies. The operation transitions to the EXECUTING state when it reaches this stage. After spawning the process, it intercepts writes to stdout and stderr, and will terminate the process if it runs longer than its Action specified timeout. Its configured concurrency is available in the worker config as execute_stage_width. [[Execution Limiting]] is available as a configuration option under cgroups, if supported. Report Result . The Report Result stage injects any outputs from the operation into the CAS, and populates the ActionResult from from the results of the execution. It can inject into the ActionCache for cacheable actions, and can record an action in the blacklist if it violates output policy. The operation transitions to COMPLETED state after the outputs are recorded. After this stage is complete, the execution directory is destroyed, and the Operation exits the worker. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/workers/#stages",
    "relUrl": "/docs/architecture/workers/#stages"
  },"94": {
    "doc": "Workers",
    "title": "Exec Filesystem",
    "content": "Workers must present Exec Filesystems for actions, and manage their existence for the lifetime of an operation’s presence within the pipeline. The realization of an operation’s execution root with the execution filesystem constitutes a transaction that the operating directory for an action will appear, be writable for outputs, and released and be made unavailable as it proceeds and exits the pipeline. This means that an action’s entire input directory must be available on a filesystem from a unique location per operation - the Operation Action Input Root, or just Root. Each input file within the Root must contain the content of the inputs, its requested executability via FileNode, and each directory must contain at the outset, child input files and directories. The filesystem is free to handle unspecified outputs as it sees fit, but the directory hierarchy of output files from the Root must be created before execution, and writable during it. When execution and observation of the outputs is completed, the exec filesystem will be asked to destroy the Root and release any associated resources from its retention. There are two implementations of Execution Filesystem in Buildfarm. Choosing either a filesystem or fuse cas type in the worker config as the first cas entry will choose the CASFileCache or FuseCAS implementations, respectively. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/workers/#exec-filesystem",
    "relUrl": "/docs/architecture/workers/#exec-filesystem"
  },"95": {
    "doc": "Workers",
    "title": "CASFileCache/CFCExecFilesystem",
    "content": "The CASFileCache provides an Exec Filesystem via CFCExecFilesystem. The (CASFileCache)’s retention of paths is used to reflect individual files, with these paths hard-linked in CFCExecFilesystem under representative directories of the input root to signify usage. The CASFileCache directory retention system is also used to provide a configurable utilization of entire directory trees as a symlink, which was a heuristic optimization applied when substantial cost was observed setting up static trees of input links for operations compared to their execution time. link_input_directories in the common Worker configuration will enable this heuristic. Outputs of actions are physically streamed into CAS writes when they are observed after an action execution. The CASFileCache’s persistence in the filesystem and the availability of common POSIX features like symlinks and inode-based reference counts on almost any filesystem implementation have made it a solid choice for extremely large CAS installations - it scales to multi-TB host attached storages with millions of entries with relative ease. There are plans to improve CASFileCache that will be reflected in improved performance and memory footprint for the features used by CFCExecFilesystem. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/workers/#casfilecachecfcexecfilesystem",
    "relUrl": "/docs/architecture/workers/#casfilecachecfcexecfilesystem"
  },"96": {
    "doc": "Workers",
    "title": "Fuse",
    "content": "A fuse implementation to provide Roots exists and is specifiable as well. This was an experiment to discover the capacity of a fuse to represent Roots transparently with a ContentAddressableStorage backing, and has not been fully vetted to provide the same reliability as the CFCExecFilesystem. This system is capable of blinking entire trees into existence with ease, as well as supporting write-throughs for outputs suitable for general purpose execution. Some problems with this type were initially observed and never completely resolved, including guaranteed resource release on Root destruction. This implementation is also only built to be backed by its own Memory CAS, with no general purpose CAS support added due to the difficulty of supporting a transaction model for an input tree to enforce the contract of availability. It remains unoptimized yet functional, but difficulties with integrating libfuse 3 into the bazel build, as well as time constraints, have kept it from being scaled and expanded as the rest of Buildfarm has grown. There are plans to revisit this implementation and bring it back into viability with a CASFileCache-like backing. ",
    "url": "http://localhost:4000/bazel-buildfarm/docs/architecture/workers/#fuse",
    "relUrl": "/docs/architecture/workers/#fuse"
  }
}
