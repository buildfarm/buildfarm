{"0": {
    "doc": "CASFileCache",
    "title": "File Metadata",
    "content": "Due to limitations of posix filesystem presentations, input FileNodes with ‘executable’ specifications require a unique inode from their non-executable counterparts. The CFC considers a CAS entry keyed with its path and executability uniquely, and their files are named as such: &lt;hash&gt; for the non-executable content, &lt;hash&gt;_exec for executable content. Each is charged with its own size to the storage pool, can be used for executable-idempotent requests on equal footing, and can be expired independently. Using any digest-based interface method without the option for an executable specification will result in executable key impartiality. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/CASFileCache/#file-metadata",
    "relUrl": "/docs/architecture/CASFileCache/#file-metadata"
  },"1": {
    "doc": "CASFileCache",
    "title": "Operations",
    "content": "Read(digest, offset, limit): The CFC will locate the indexed file for a digest with executable impartiality, attempt to open it, seek to the offset requested, and present a limited stream of the content’s bytes. A missing digest will result in predefined behavior per the CAS Interface, as well as an invalidation of its process index entry, if it exists. An access for the entry is inserted into the Access Queue. A read is (currently) impermeable to expiration, and can result in overcharge of filesystem content in such a case. Write(id): The CFC provides a Write-interface object that can be used to insert content into the CAS. Service and Operation lifetime implementations use this to inject content as requested, and leverage its restartable and asynchronous completion capacities. Writes may be rejected out of hand due to configurable CAS entry size limitations. Under the hood, there are several constraints present per-write: . | A Write is uniquely identified by its digest and a uuid. | A Write will charge the storage pool size with its full content size upon opening a stream via getOutput(). | A Write’s OutputStream is a mutually exclusive transaction, with a premature close() restoring the stream’s capacity for representation. | A close() upon completion of a Write’s OutputStream will initiate a validation for it. | Upon successful validation (size and content are compared to the expected digest), a commit is effected, making it available as a CAS entry with a recorded access placing it at the head of the RC-LRU. | Within a CFC, concurrent Writes to a single digest will compete for completion, with the first one to commit cancelling all other Writes’ OutputStreams to the same digest. | Upon a commit, a shared future for all Writes to a single digest is completed, allowing premature indication of committed content receipt. | . FindMissingBlobs(digests): The process index is queried for each requested digest with executable impartiality, to filter for only those entries that do not exist in the index. No file interactions take place during this request, and no invalidation of content occurs upon this request. Accesses for all existing entries enter the Access Queue. Batch Read/Write: These requests are sequenced for each entry, resulting in only latency reduction, corresponding to each individual operation. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/CASFileCache/#operations",
    "relUrl": "/docs/architecture/CASFileCache/#operations"
  },"2": {
    "doc": "CASFileCache",
    "title": "Structures",
    "content": "Entry Index . This is a Map of CFC Entry Keys ({digest, executability}) to accounting entry metadata. Reference counts, directories containing the entry, and file metadata ttl are metadata fields for the entry, as well as support fields for the RC-LRU. RC-LRU . The Reference Counted Least Recently Used list is a composite structure with the Process Index. Entries are either in or out of the RC-LRU, with only those entries in the RC-LRU considered for expiration. All entries with reference counts &gt; 0 are out of the RC-LRU. Manipulation of the RC-LRU is synchronized for the entire CFC, with the actors being the transactive put[Directory]() and decrementReferences() methods, as well as the Access Recorder. Directory Index . Directories can be maintained by the CFC to provide several important heuristic optimizations, both in terms of storage and IO complexity. For a given directory entry, a key of its digest is used to refer to all of its flattened content digests. This is complemented by a filesystem directory named for the digest with a &lt;hash&gt;_dir name, where it will contain a) real directories underneath it named for each level of DirectoryNode in its REAPI specification, and b) files with names and executability from a FileNode for each Directory’s files, the inodes for which are all CAS entry files (or empty files, in that special size case). The CFC also maintains a reverse index for each CAS entry to all of the directories it exists under. Upon expiration of an entry, each directory it is contained in is also expired, with both Directory Index and filesystem directories being deleted. This is the only growth bound check on any directory, being the only path to expiration. Access Queue . A simple queue of records of digest set accesses, meant to indicate the entries more desired than not represented. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/CASFileCache/#structures",
    "relUrl": "/docs/architecture/CASFileCache/#structures"
  },"3": {
    "doc": "CASFileCache",
    "title": "Agents",
    "content": "Startup Scanner . Upon start(), a CFC scans the contents of its root to populate its indices. It first locates and injects all entry inodes, filtering for valid entry names and filesystem permissions, populating the Entry Index. Invalid names are deleted. It then scans directories, reproducing a Directory tree definition for each toplevel dir for validation, and populates the Directory Index. Invalid directories are deleted. A callback for the set of valid entry digests discovered is invoked. This procedure is (currently) blocking for valid entries, and operates asynchronously during invalid content removal. Access Recorder . The Access Queue is depleted by the Access Recorder, active after start() completion, to perform a simple reordering for accessed entries into the front of the RC-LRU. An entry that is out of the RC-LRU remains out after being processed by the recorder, and an entry that is in is moved to the head. A single thread runs the Access Recorder, and it is terminated when stop() is called on the CFC. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/CASFileCache/#agents",
    "relUrl": "/docs/architecture/CASFileCache/#agents"
  },"4": {
    "doc": "CASFileCache",
    "title": "Behaviors",
    "content": "Reference/Dereference . Entries can have their reference counts incremented, with 0-&gt;1 moving them out of the RC-LRU, or decremented, with 1-&gt;0 moving them in to the RC-LRU, being placed at the head. Directory {,de}references operate in batch over their contained entries, with directory digests functioning as extreme space-saving shorthands for digest sets. Asynchronous Filesystem Monitor . Entry and Directory metadata content Time To Live checks provide convergent consistency in the event of asynchronous filesystem manipulation - in the event of an unexpected availability or permission change to the filesystem, the CFC eventually discovers and presents corrected state. Expiration . As a result of any intended insertion into the CFC, the total content size of the CAS is ‘charged’ by adding the size of the new content. A test is performed against this size. If the size exceeds a configurable maximum content size, expiration occurs and blocks the insertion until complete. To expire content, the CFC removes the tail of the RC-LRU, attempts a delegate write if configured, expires its associated directories, removes its metadata, deletes its file content, and will executable-impartially invoke a callback with the expired digest. This process is repeated until the size is once again below the maximum content size. Delegation . The CFC supports a waterfall delegation to a CAS Interface Object. This Object (currently) sees the following interactions: . | Writes upon expirations of a referencing CFC, with full output streaming. | Reads upon content missing from a referencing CFC, with read through insertion of the referencing CFC as an entry | FindMissingBlobs for filtered entries that remain missing from a referencing CFC. | . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/CASFileCache/#behaviors",
    "relUrl": "/docs/architecture/CASFileCache/#behaviors"
  },"5": {
    "doc": "CASFileCache",
    "title": "CASFileCache",
    "content": "The Content Addressable Storage File Cache (CFC) is the functional beating heart of all Buildfarm Workers. It should fundamentally be thought of as a rotating set of Content stored in files, indexed by Digests that make up the inputs for Actions to be executed. It also serves several other functions here detailed. This documentation presents it both in terms of functional behavior, as if it were a service or process, as well as with Java semantics, owing to its implementation. A CFC has the additional responsibility in shard workers of presenting a CAS (Java) Interface, which supports the typical Read, Write, FindMissingBlobs, and batch versions of the R/W requests. Each CFC needs a filesystem path to use as its root, and a set of configuration parameters. Under this path, it will store the CAS entries, named by their digests, optionally in a partitioned directory hierarchy by subdividing the first N bytes of the hash. The CAS entries are treated by the CFC in terms of their index (filesystem directory) mapping to an inode (file content and limited metadata). This is an important distinction for the CFC, as it is used to present a substantially deduplicated ephemeral tree of inputs by its corresponding Exec Filesystem when executing actions. This will be described in further detail under the RC-LRU. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/CASFileCache/",
    "relUrl": "/docs/architecture/CASFileCache/"
  },"6": {
    "doc": "Action Cache",
    "title": "Methods",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/action_cache/#methods",
    "relUrl": "/docs/architecture/action_cache/#methods"
  },"7": {
    "doc": "Action Cache",
    "title": "GetActionResult",
    "content": "Essentially the “get” method, which is responsible for finding an ActionResult and retrieving it. Before invoking this method, Bazel client should compute the input tree and the Action message for the action needs to be done. Then Bazel can use this GetActionCache method to see if the action has been completed successfully and, if so, use bytestream.Read to download the outputs. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/action_cache/#getactionresult",
    "relUrl": "/docs/architecture/action_cache/#getactionresult"
  },"8": {
    "doc": "Action Cache",
    "title": "UpdateActionResult",
    "content": "As mentioned above, the ActionCache service doesn’t necessarily need an Execution Service. In this case, a “put” method is required so that an ActionResult can be directly put into the cache. This is what UpdateActionResult is designed for. With this method, Bazel clients running different machines can upload their build results into a cache pool, which can be available to other users through GetActionResult. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/action_cache/#updateactionresult",
    "relUrl": "/docs/architecture/action_cache/#updateactionresult"
  },"9": {
    "doc": "Action Cache",
    "title": "Buildfarm Implementations",
    "content": "Buildfarm provides implementations of both ActionCache and Execution. ActionResult can be populated through Execution service or uploaded by local Bazel clients through UpdateActionCache. The ActionCache service is hosted on the server-side of Buildfarm. When a GetActionResultRequest received through GetActionResult call, by using the instance_name field of the request, the ActionCache service will find the Instance that is supposed to have ActionResult, and it will find the Instance and get the ActionResult asynchronously. Similarly, an UpdateActionResultRequest will be sent to the ActionCache service through UpdateActionResult rpc call. The service will find the Instance that is supposed to have the ActionResult through the instance_name field of the request and update the corresponding instance. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/action_cache/#buildfarm-implementations",
    "relUrl": "/docs/architecture/action_cache/#buildfarm-implementations"
  },"10": {
    "doc": "Action Cache",
    "title": "Action Cache",
    "content": "ActionCache is a service that can be used to query whether a defined action has already been executed and, if so, download its result. The service API is defined in the Remote Execution API. ActionCache service would require ContentAddressableStorage service to store file data. An Action encapsulates all the information required to execute an action. Such information includes the command, input tree containing subdirectory/file tree, environment variables, platform information. All the information will contribute to the digest computation of an Action so that execution of an Action multiple times will produce the same output. With this, hash of an Action can be used as a key to cached ActionResult, which store result and output of an Action after an Action is completed. ActionResults can be populated in ActionCache service after Actions get completed by an Execution service. They can also come from a local Bazel client that has executed the Actions and put the ActionResults into the cache by using the UpdateActionCache method. In other words, The ActionCache service can be used without using/implementing the Execution service. By leveraging Action definition, ActionCache service is responsible for mapping Actions to the ActionResults. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/action_cache/",
    "relUrl": "/docs/architecture/action_cache/"
  },"11": {
    "doc": "Admin",
    "title": "Admin",
    "content": "How to make your AWS cluster visible to Buildfarm Admin . Buildfarm Admin currently only works with Buildfarm clusters deployed in AWS. All hosts must be properly tagged with values specified in application.properties. | Tag schedulers (servers) with buildfarm.instance_type={deployment.tag.instance.type.server} | Tag CPU workers with buildfarm.worker_type={deployment.tag.instance.type.cpuworker} | Tag GPU workers with buildfarm.worker_type={deployment.tag.instance.type.gpuworker} | Tag all hosts with aws:autoscaling:groupName={} | Tag all hosts with buildfarm.cluster_id={cluster.id} | . REST API Endpoints . | /restart/worker/{instanceId} | /restart/server/{instanceId} | /terminate/{instanceId} | /scale/{autoScaleGroup}/{numInstances} | . Build and Run Buildfarm Admin locally . Create application.properties file and override any default settings from admin/main/src/main/resources/application.properties. Pass your config file location to the optional spring.config.location flag. ` cd admin/main ./mvnw clean package java -jar target/bfadmin.jar -Dspring.config.location=file: ` . Run Latest Docker Container . Replace $HOME with the location of your application.properties file. Make sure the AWS environment variables are set on the host machine for the account where Buildfarm cluster is deployed. ` docker run -p 8080:8080 -v $HOME:/var/lib -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN bazelbuild/buildfarm-admin:latest ` . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/admin/",
    "relUrl": "/docs/admin/"
  },"12": {
    "doc": "Alerts",
    "title": "Alerts",
    "content": "Using the Prometheus metrics you may want to setup alerts on your buildfarm cluster. Below are some example alerts: . High Cpu Queue avg(queue_size{job=\"your_instance\", queue_name=\"cpu_queue\"}) Critical = Triggered when the value is greater than 1000 for 60m Warning = Triggered when the value is greater than 1000 for 30m . High Cluster Utilization avg(execution_slot_usage{job=\"your_instance\", service=\"buildfarm-worker\"}) Critical = Triggered when the value is greater than 85 for 60m Warning = Triggered when the value is greater than 85 for 30m . Multiple Bazel Versions Detected count by (tool_name) (count(dispatched_operations_tools_amount{job=\"your_instance\", service=\"buildfarm-server\"}) without(host_ip, instance)) Warning = Triggered when the value is greater than 1 for 1m . No available workers avg(worker_pool_size{job=\"your_instance\", service=\"buildfarm-server\"}) Critical = Triggered when the value is equal to 0 for 5m Warning = Triggered when the value is equal to 0 for 2m . Stuck CPU Workers . (avg(delta(execution_slot_usage{job=\"your_instance\", service=\"buildfarm-worker\"}[5m])) by (host_ip) == 0) and (avg(execution_slot_usage{job=\"your_instance\", service=\"buildfarm-worker\"}) by (host_ip) &gt; 0) and (avg(execution_slot_usage{job=\"your_instance\", service=\"buildfarm-worker\"}) by (host_ip) &lt; 89) . Critical = Triggered when the value is greater than or equal to 0 for 30m . Key Expirations - Builds May Be Slow . avg(increase(expired_key_total{job=\"your_instance\", service=\"buildfarm-worker\"}[5m])) by (host_ip) . Warning = Triggered when the value is greater than 10000 for 1h . High Prequeue pre_queue_size{job=\"your_instance\", service=\"buildfarm-server\"} Critical = Triggered when the value is greater than 0 for 15m Warning = Triggered when the value is greater than 0 for 5m . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/metrics/alerts/",
    "relUrl": "/docs/metrics/alerts/"
  },"13": {
    "doc": "Architecture",
    "title": "Instances",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/architecture/#instances",
    "relUrl": "/docs/architecture/architecture/#instances"
  },"14": {
    "doc": "Architecture",
    "title": "Definition",
    "content": "An instance is a namespace which represents a pool of resources available to a remote execution client. All requests to the Remote Execution Services are identified with an instance name, and a client is expected to communicate with the same instance between requests in order to accomplish aggregate activities. For instance, a typical client usage of FindMissingBlobs, then one or more Write/BatchUploadBlobs, Execute, and zero or more Read/BatchReadBlobs would all have the same instance name associated with the requests. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/architecture/#definition",
    "relUrl": "/docs/architecture/architecture/#definition"
  },"15": {
    "doc": "Architecture",
    "title": "Implementation",
    "content": "Buildfarm uses modular instances, where an instance is associated with one concrete type that governs its behavior and functionality. The Remote Execution API uses instance names to identify every request made, which allows Buildfarm instances to represent partitions of resources. One endpoint may support many different instances, each with their own name, and each instance will have its own type. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/architecture/#implementation",
    "relUrl": "/docs/architecture/architecture/#implementation"
  },"16": {
    "doc": "Architecture",
    "title": "Typical Deployment",
    "content": ". ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/architecture/#typical-deployment",
    "relUrl": "/docs/architecture/architecture/#typical-deployment"
  },"17": {
    "doc": "Architecture",
    "title": "Workers",
    "content": "Workers are deployed as an autoscaling group in the cloud environment. This group should scale based on the load. This will require monitoring and alerting to be setup, which will trigger the scaling events. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/architecture/#workers",
    "relUrl": "/docs/architecture/architecture/#workers"
  },"18": {
    "doc": "Architecture",
    "title": "Schedulers",
    "content": "Schedulers are deployed as an autoscaling group in the cloud environment. This group should scale based on the load. This will require monitoring and alerting to be setup, which will trigger the scaling events. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/architecture/#schedulers",
    "relUrl": "/docs/architecture/architecture/#schedulers"
  },"19": {
    "doc": "Architecture",
    "title": "Clustered Redis",
    "content": "Clustered Redis should be sized based on the expected load. Typically, no replication is necessary for Buildfarm use as the loss of data stored is not catastrophic. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/architecture/#clustered-redis",
    "relUrl": "/docs/architecture/architecture/#clustered-redis"
  },"20": {
    "doc": "Architecture",
    "title": "Schedulers Network Load Balancer",
    "content": "A network load balancer is set up to target the Schedulers autoscaling group. This will be the primary Buildfarm endpoint. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/architecture/#schedulers-network-load-balancer",
    "relUrl": "/docs/architecture/architecture/#schedulers-network-load-balancer"
  },"21": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/architecture/",
    "relUrl": "/docs/architecture/architecture/"
  },"22": {
    "doc": "Authentication",
    "title": "Generated TLS self-signed certificates",
    "content": "After running the above script, you will have the following files: . ls /tmp/sslcert ca.crt ca.key client.crt client.csr client.key client.pem server.crt server.csr server.key server.pem . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/configuration/authentication/#generated-tls-self-signed-certificates",
    "relUrl": "/docs/configuration/authentication/#generated-tls-self-signed-certificates"
  },"23": {
    "doc": "Authentication",
    "title": "Configuring the buildfarm server",
    "content": "The certificate and private key can be referenced in buildfarm’s config as followed: . server: sslCertificatePath: /tmp/sslcert/server.crt sslPrivateKeyPath: /tmp/sslcert/server.pem . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/configuration/authentication/#configuring-the-buildfarm-server",
    "relUrl": "/docs/configuration/authentication/#configuring-the-buildfarm-server"
  },"24": {
    "doc": "Authentication",
    "title": "Configuring the buildfarm client",
    "content": "When calling the bazel client, pass the certificate via --tls_certificate: . bazel build --remote_executor=grpcs://localhost:8980 \\ --tls_certificate=/tmp/sslcert/ca.crt \\ &lt;target&gt; . Don’t forget to use grpcs:// instead of grpc://. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/configuration/authentication/#configuring-the-buildfarm-client",
    "relUrl": "/docs/configuration/authentication/#configuring-the-buildfarm-client"
  },"25": {
    "doc": "Authentication",
    "title": "Combined credentials",
    "content": "You may have certificate and private keys in a combined pem file. For example, if you were to run cat /tmp/sslcert/server.crt /tmp/sslcert/server.pem &gt; combined.pem you could provide combined.pem to both sslCertificatePath and sslPrivateKeyPath and get the same results. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/configuration/authentication/#combined-credentials",
    "relUrl": "/docs/configuration/authentication/#combined-credentials"
  },"26": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": "This is an overview of a simple way to create a self signed TLS key pair for client / server authentication. We recommend you look at the script used by bazel for certificate generation. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/configuration/authentication/",
    "relUrl": "/docs/configuration/authentication/"
  },"27": {
    "doc": "bf-cat",
    "title": "bf-cat",
    "content": "bf-cat is a tool provided with buildfarm for investigating the various structures and status of your Buildfarm Cluster. Build or run it with bazel build //src/main/java/build/buildfarm/tools:bf-cat . Its basic usage is: . bf-cat &lt;host[:port]&gt; &lt;instance-name&gt; &lt;command&gt; [params...] . instance-name is the name of the specific instance to inquire about, typically configured on schedulers. A literal empty string parameter (i.e. bash: \"\") will use the default instance for a server. command is typically one of the following, with digest parameters as /, as typically represented in log entries: . | Action &lt;digest&gt;: Retrieves Action definitions from the CAS and renders them with field identifiers. | Capabilities: Retrieve the capabilities response for an instance. | Command &lt;digest&gt;: Retrieves Command definitions from the CAS and renders them with field identifiers. | Directory &lt;digest&gt;: Retrieves Directory definitions from the CAS and renders them with field identifiers. | Fetch &lt;uris&gt;...: Request an uri fetch via the assets API. | File &lt;digest&gt;: Downloads a Blob from the CAS and prints it to stdout. This can be safely redirected to a file, with no additional output interceding | Missing &lt;digest&gt;...: Make a findMissingBlobs request, outputting only the digests in the parameter list that are missing from the CAS | Operation &lt;name&gt;...: Retrieves current operation statuses and renders them with field identifiers as able. This uses the Operations API and will include rich information about operations in flight, compared to the ‘execute’ function | Operations &lt;filter&gt; [name]: Retrieves a list of operations pertaining to a filter and a name scope: . | toolInvocationId=&lt;uuid&gt;: list executions in a client invocation group | correlatedInvocationsId=&lt;uuid&gt; toolInvocations: list the invocations in a client correlated list | status=dispatched: list the currently dispatched executions | . | BackplaneStatus: Retrieve the status of a shard cluster’s operation queues, with discrete information about each provisioned layer of the ready-to-run queue. | TreeLayout &lt;digest&gt;: Retrieves Trees of inputs from a root node. A Tree is printed with indent-levels according to depth in the directory hierarchy with FileNode and DirectoryNode fields with digests for each entry, as well as a weight by byte and % of the sizes of each directory subtree. | WorkerProfile: Retrieve profile information about a worker’s operation, including the size of the CAS and the relative performance of the execution pipeline | Watch &lt;name&gt;: Watch an operation to retrieve status updates about its progress through the operation pipeline | . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/tools/bf-cat/",
    "relUrl": "/docs/tools/bf-cat/"
  },"28": {
    "doc": "Build Without Bytes",
    "title": "Builds Without The Bytes",
    "content": "TL;DR: BwoB is supported by Buildfarm. Build with --experimental_remote_cache_eviction_retries=5. As proposed in this issue and the accompanying document, bazel endeavors to provide a mechanism to be ‘content-light’ for remote execution, using only content reference addresses to request action execution and construct successively dependent action definitions. Builds Without The Bytes skips the download of action output files until it needs them. Bazel now (&gt;= 7) also applies a wall-clock ttl to every action result it downloads, depending upon current settings. See bazel documentation for the default. If this ttl has not expired, regardless of bazel daemon uptime, the remote cache will not be queried for an action result. The stored ActionResult also has no source - if you switch bazel --remote_cache or --remote_executor targets inside of the ttl, bazel will blindly assume the current target has ActionResult contents. If ActionResult contents expire on Buildfarm, when bazel requests them, it will fail, hopefully with the special REMOTE_CACHE_EVICTED (39) exit code. The flag --experimental_remote_cache_eviction_retries will cause bazel to restart a build with no intervention required, for the specified limit # of times. Buildfarm (since 2.11) defaults to ensureOutputsPresent: true in server configs. When bazel requests an ActionResult, it will be NOT_FOUND (cache miss) unless all of its contents exist in the CAS. This also extends the lifetime of those contents (without shard loss) to the minimum ttl before expiration. With ensureOutputsPresent: false, there is also a mechanism to control this check on a per-request level. The REAPI presents a ‘correlated_invocations_id’ in the RequestMetadata settable on headers for every request to BuildFarm, including the GetActionResult request, which it uses to retrieve cached results. BuildFarm recognizes this correlated_invocations_id and if it is a URI, can parse its query parameters for behavior control. One such control is ENSURE_OUTPUTS_PRESENT for the GetActionResult request - if this query value is the string “true”, BuildFarm will make a silent FindMissingBlobs check for all of the outputs of an ActionResult before responding with it. If any are missing, BuildFarm will instead return code NOT_FOUND, inspiring the client to see a cache miss, and attempt a [remote] execution. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/builds-without-the-bytes/#builds-without-the-bytes",
    "relUrl": "/docs/execution/builds-without-the-bytes/#builds-without-the-bytes"
  },"29": {
    "doc": "Build Without Bytes",
    "title": "Build Without Bytes",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/builds-without-the-bytes/",
    "relUrl": "/docs/execution/builds-without-the-bytes/"
  },"30": {
    "doc": "Configuration",
    "title": "All Configurations",
    "content": "Common . | Configuration | Accepted and Default Values | Command Line Argument | Description | . | digestFunction | SHA256, SHA1 |   | Digest function for this implementation | . | defaultActionTimeout | Integer, 600 |   | Default timeout value for an action (seconds) | . | maximumActionTimeout | Integer, 3600 |   | Maximum allowed action timeout (seconds) | . | maxEntrySizeBytes | Long, 2147483648 |   | Maximum size of a single blob accepted (bytes) | . | prometheusPort | Integer, 9090 | –prometheus_port | Listening port of the Prometheus metrics endpoint | . | allowSymlinkTargetAbsolute | boolean, false |   | Permit inputs to contain symlinks with absolute path targets | . Example: . digestFunction: SHA1 defaultActionTimeout: 1800 maximumActionTimeout: 1800 prometheusPort: 9090 server: ... worker: ... Server . | Configuration | Accepted and Default Values | Environment Var | Description | . | instanceType | SHARD |   | Type of implementation (SHARD is the only one supported) | . | name | String, shard |   | Implementation name | . | publicName | String, DERIVED:port | INSTANCE_NAME | Host:port of the GRPC server, required to be accessible by all servers | . | actionCacheReadOnly | boolean, false |   | Allow/Deny writing to action cache | . | port | Integer, 8980 |   | Listening port of the GRPC server | . | bindAddress | String |   | Listening address of the GRPC server, default for Java Grpc (all interface addresses) if unspecified | . | maxInboundMessageSizeBytes | Integer, 0 |   | Byte size limit of GRPC messages, default for Java Grpc if unspecified or 0 | . | maxInboundMetadataSize | Integer, 0 |   | Byte size limit of GRPC metadata, default for Java Grpc if unspecified or 0 | . | casWriteTimeout | Integer, 3600 |   | CAS write timeout (seconds) | . | bytestreamTimeout | Integer, 3600 |   | Byte Stream write timeout (seconds) | . | sslCertificatePath | String, null |   | Absolute path of the SSL certificate (if TLS used) | . | sslPrivateKeyPath | String, null |   | Absolute path of the SSL private key (if TLS used) | . | runDispatchedMonitor | boolean, true |   | Enable an agent to monitor the operation store to ensure that dispatched operations with expired worker leases are requeued | . | dispatchedMonitorIntervalSeconds | Integer, 1 |   | Dispatched monitor’s lease expiration check interval (seconds) | . | runOperationQueuer | boolean, true |   | Acquire execute request entries cooperatively from an arrival queue on the backplane | . | ensureOutputsPresent | boolean, true |   | Ensure ActionResult outputs are present in the CAS on ActionCache::GetActionResults. If any outputs are missing a cache miss is returned | . | maxCpu | Integer, 0 |   | Maximum number of CPU cores that any min/max-cores property may request (0 = unlimited) | . | maxRequeueAttempts | Integer, 5 |   | Maximum number of requeue attempts for an operation | . | useDenyList | boolean, true |   | Allow usage of a deny list when looking up actions and invocations (for cache only it is recommended to disable this check) | . | mergeExecutions | boolean, true |   | Merge executions with matching action_digest by default (overridable with MERGE_EXECUTIONS correlatedInvocationsId URL param) | . | grpcTimeout | Integer, 3600 |   | GRPC request timeout (seconds) | . | executeKeepaliveAfterSeconds | Integer, 60 |   | Execute keep alive (seconds) | . | recordBesEvents | boolean, false |   | Allow recording of BES events | . | clusterId | String, local |   | Buildfarm cluster ID | . | cloudRegion | String, us-east_1 |   | Deployment region in the cloud | . | gracefulShutdownSeconds | Integer, 0 |   | Time in seconds to allow for connections in flight to finish when shutdown signal is received | . Example: . server: instanceType: SHARD name: shard actionCacheReadOnly: true recordBesEvents: true . GRPC Metrics . | Configuration | Accepted and Default Values | Description | . | enabled | boolean, false | Publish basic GRPC metrics to a Prometheus endpoint | . | provideLatencyHistograms | boolean, false | Publish detailed, more expensive to calculate, metrics | . | labelsToReport | List of Strings, [] | Include custom metrics labels in Prometheus metrics | . Example: . server: grpcMetrics: enabled: false provideLatencyHistograms: false labelsToReport: [] . Server Caches . | Configuration | Accepted and Default Values | Description | . | directoryCacheMaxEntries | Long, 64 * 1024 | The max number of entries that the directory cache will hold. | . | commandCacheMaxEntries | Long, 64 * 1024 | The max number of entries that the command cache will hold. | . | digestToActionCacheMaxEntries | Long, 64 * 1024 | The max number of entries that the digest-to-action cache will hold. | . | recentServedExecutionsCacheMaxEntries | Long, 64 * 1024 | The max number of entries that the executions cache will hold. | . Example: . server: caches: directoryCacheMaxEntries: 10000 commandCacheMaxEntries: 10000 digestToActionCacheMaxEntries: 10000 recentServedExecutionsCacheMaxEntries: 10000 . Admin . | Configuration | Accepted and Default Values | Description | . | deploymentEnvironment | String, AWS, GCP | Specify deloyment environment in the cloud | . | clusterEndpoint | String, grpc://localhost | Buildfarm cluster endpoint for Admin use (this is a full buildfarm endpoint) | . Example: . server: admin: deploymentEnvironment: AWS clusterEndpoint: \"grpc://localhost\" . Metrics . | Configuration | Accepted and Default Values | Description | . | publisher | String, aws, gcp, log | Specify publisher type for sending metadata | . | logLevel | String, INFO, FINEST | Specify log level (“log” publisher only, all Java util logging levels are allowed here) | . | topic | String, test | Specify SNS topic name for cloud publishing (“aws” publisher only) | . | topicMaxConnections | Integer, 1000 | Specify maximum number of connections allowed for cloud publishing (“aws” publisher only) | . | secretName | String, test | Specify secret name to pull SNS permissions from (“aws” publisher only) | . Example: . server: metrics: publisher: log logLevel: INFO . server: metrics: publisher: aws topic: buildfarm-metadata-test topicMaxConnections: 1000 secretName: buildfarm-secret . Correlated Invocations Index Scopes . A set of names to extract per-correlatedInvocationsId fields from a valid URI to create indices which will contain the fragment uuid . | Entry | URI Component | . | host | authority:host | . | username | userinfo:username | . | * (any key) | query[key] (one index per specified pair) | . Example: . server: correlatedInvocationsIndexScopes: !!set ? host ? username . Redis Backplane . | Configuration | Accepted and Default Values | Environment Var | Command Line Argument | Description | . | type | SHARD |   |   | Type of backplane. Currently, the only implementation is SHARD utilizing Redis | . | redisUri | String, redis://localhost:6379 | REDIS_URI | –redis_uri | Redis cluster endpoint. This must be a single URI. This can embed a username/password per RFC-3986 Section 3.2.1 and this will take precedence over redisPassword and redisPasswordFile. | . | redisUsername | String, null |   |   | Redis username, if applicable | . | redisPassword | String, null |   |   | Redis password, if applicable | . | redisPasswordFile | String, null |   |   | File to read for a Redis password. If specified, this takes precedence over redisPassword | . | redisNodes | List of Strings, null |   |   | List of individual Redis nodes, if applicable | . | redisCertificateAuthorityFile | String, null |   |   | File to read for Redis connection SSL certificate authority. If specified, this is the exclusive certificate authority bundle for Redis connections. Must be a PEM file. | . | redisAuthWithGoogleCredentials | boolean, false |   |   | If true, use GOOGLE_DEFAULT_CREDENTIALS environment variable to find a service account to authenticate to Redis. useful for Google Memorystore Redis Cluster | . | jedisPoolMaxTotal | Integer, 4000 |   |   | The size of the Redis connection pool | . | jedisPoolMaxIdle | Integer, 8 |   |   | The maximum size of idle connection to Redis | . | jedisPoolMinIdle | Integer, 0 |   |   | The size of guaranteed idle connection to Redis | . | jedisTimeBetweenEvictionRunsMillis | Long, 30000 |   |   | The period for detecting idle connections to Redis. If you set -1, detection will be disabled. | . | workersHashName | String, Workers |   |   | Redis key used to store a hash of registered workers | . | workerChannel | String, WorkerChannel |   |   | Redis pubsub channel key where changes of the cluster membership are announced | . | actionCachePrefix | String, ActionCache |   |   | Redis key prefix for all ActionCache entries | . | actionCacheExpire | Integer, 2419200 |   |   | The TTL maintained for ActionCache entries, refreshed on getActionResult hit | . | actionBlacklistPrefix | String, ActionBlacklist |   |   | Redis key prefix for all blacklisted actions, which are rejected | . | actionBlacklistExpire | Integer, 3600 |   |   | The TTL maintained for action blacklist entries | . | invocationBlacklistPrefix | String, InvocationBlacklist |   |   | Redis key prefix for blacklisted invocations, suffixed with a a tool invocation ID | . | operationPrefix | String, Operation |   |   | Redis key prefix for all operations, suffixed with the operation’s name | . | operationExpire | Integer, 604800 |   |   | The TTL maintained for all executions, updated on each modification | . | actionExecutionExpire | Integer, 21600 |   |   | The TTL maintained for all action -&gt; execution mappings for mergeExecutions | . | preQueuedOperationsListName | String, {Arrival}:PreQueuedOperations |   |   | Redis key used to store a list of ExecuteEntry awaiting transformation into QueryEntry | . | processingListName | String, {Arrival}:ProcessingOperations |   |   | Redis key of a list used to ensure reliable processing of arrival queue entries with operation watch monitoring | . | processingPrefix | String, Processing |   |   | Redis key prefix for operations which are being dequeued from the arrival queue | . | processingTimeoutMillis | Integer, 20000 |   |   | Delay (in ms) used to populate processing operation entries | . | queuedOperationsListName | String, {Execution}:QueuedOperations |   |   | Redis key used to store a list of QueueEntry awaiting execution by workers | . | dispatchingPrefix | String, Dispatching |   |   | Redis key prefix for operations which are being dequeued from the ready to run queue | . | dispatchingTimeoutMillis | Integer, 10000 |   |   | Delay (in ms) used to populate dispatching operation entries | . | dispatchedOperationsHashName | String, DispatchedOperations |   |   | Redis key of a hash of operation names to the worker lease for its execution, which are monitored by the dispatched monitor | . | operationChannelPrefix | String, OperationChannel |   |   | Redis pubsub channel prefix suffixed by an operation name | . | casPrefix | String, ContentAddressableStorage |   |   | Redis key prefix suffixed with a blob digest that maps to a set of workers with that blob’s availability | . | casExpire | Integer, 604800 |   |   | The TTL maintained for CAS entries, which is refreshed on any read access of the blob | . | subscribeToBackplane | boolean, true |   |   | Enable an agent of the backplane client which subscribes to worker channel and operation channel events. If disabled, responsiveness of watchers and CAS are reduced | . | runFailsafeOperation | boolean, true |   |   | Enable an agent in the backplane client which monitors watched operations and ensures they are in a known maintained, or expirable state | . | maxQueueDepth | Integer, 100000 |   |   | Maximum length that the ready to run queue is allowed to reach to control an arrival flow for execution | . | maxPreQueueDepth | Integer, 1000000 |   |   | Maximum lengh that the arrival queue is allowed to reach to control load on the Redis cluster | . | priorityQueue | boolean, false |   |   | Priority queue type allows prioritizing operations based on Bazel’s –remote_execution_priority= flag | . | timeout | Integer, 10000 |   |   | Default timeout | . | maxInvocationIdTimeout | Integer, 604800 |   |   | Maximum TTL (Time-to-Live in second) of invocationId keys in RedisBackplane | . | maxAttempts | Integer, 20 |   |   | Maximum number of execution attempts | . | connectionValidatedOnBorrow | boolean, false |   |   | Whether to validate Redis connections when borrowing from the pool | . Example: . backplane: type: SHARD redisUri: \"redis://localhost:6379\" priorityQueue: true . Execution Queues . | Configuration | Accepted and Default Values | Description | . | name | String | Name of the execution queue (ex: cpu, gpu) | . | allowUnmatched | boolean, true |   | . | properties | List of name/value pairs | Any specification of min/max-cores will be allowed to support CPU controls and worker resource delegation | . Example: . backplane: type: SHARD redisUri: \"redis://localhost:6379\" queues: - name: \"cpu\" allowUnmatched: true properties: - name: \"min-cores\" value: \"*\" - name: \"max-cores\" value: \"*\" . Worker . | Configuration | Accepted and Default Values | Environment Var | Description | . | port | Integer, 8981 |   | Listening port of the worker | . | publicName | String, DERIVED:port | INSTANCE_NAME | Host:port of the GRPC server, required to be accessible by all servers | . | root | String, /tmp/worker |   | Path for all operation content storage | . | inlineContentLimit | Integer, 1048567 |   | Total size in bytes of inline content for action results, output files, stdout, stderr content | . | operationPollPeriod | Integer, 1 |   | Period between poll operations at any stage | . | executeStageWidth | Integer, 0 | EXECUTION_STAGE_WIDTH | Number of CPU cores available for execution (0 = system available cores) | . | executeStageWidthOffset | Integer, 0 |   | Offset number of CPU cores available for execution (to allow for use by other processes) | . | inputFetchStageWidth | Integer, 0 |   | Number of concurrently available slots to fetch inputs (0 = system calculated based on CPU cores) | . | inputFetchDeadline | Integer, 60 |   | Limit on time (seconds) for input fetch stage to fetch inputs | . | reportResultStageWidth | Integer, 1 |   | Number of concurrently available slots to write results and clean up execution directories | . | linkExecFileSystem | boolean, true |   | Use hard links instead of file copies to populate execution directories. Disable on Windows to compensate for shared hard-link deletion semantics for running executables. | . | linkInputDirectories | boolean, true |   | Use an input directory creation strategy which creates a single directory tree at the highest level containing no output paths of any kind, and symlinks that directory into an action’s execroot, saving large amounts of time spent manufacturing the same read-only input hierirchy over multiple actions’ executions | . | execOwner | String, null |   | Create exec trees containing directories that are owned by this user | . | execOwners | List of Strings |   | Create exec trees containing directories that are owned by these users, unique to each execution. Must contain at least the sum of {inputFetchStageWidth, executeStageWidth, reportResultStageWidth} owners. If this option is specified and not empty, execOwner is ignored | . | hexBucketLevels | Integer, 0 |   | Number of levels to create for directory storage by leading byte of the hash | . | defaultMaxCores | Integer, 0 |   | Constrain all executions to this logical core count unless otherwise specified via min/max-cores (0 = no limit) | . | limitGlobalExecution | boolean, false |   | Constrain all executions to a pool of logical cores specified in executeStageWidth | . | onlyMulticoreTests | boolean, false |   | Only permit tests to exceed the default coresvalue for their min/max-cores range specification (only works with non-zero defaultMaxCores) | . | allowBringYourOwnContainer | boolean, false |   | Enable execution in a custom Docker container | . | errorOperationRemainingResources | boolean, false |   |   | . | errorOperationOutputSizeExceeded | boolean, false |   | Operations which produce single output files which exceed maxEntrySizeBytes will fail with a violation type which implies a user error. When disabled, the violation will indicate a transient error, with the action blacklisted. | . | linkedInputDirectories | List of Strings, ^(?!external$).*$ |   | A list of regular expressions matching input directories which will be subject to the effects of linkInputDirectories setting | . | gracefulShutdownSeconds | Integer, 0 |   | Time in seconds to allow for operations in flight to finish when shutdown signal is received | . | createSymlinkOutputs | boolean, false |   | Creates SymlinkNodes for symbolic links discovered in output paths for actions. No verification of the symlink target path occurs. Buildstream, for example, requires this. | . | zstdBufferPoolSize | Integer, 2048 |   | Specifies the maximum number of zstd data buffers that may be in use concurrently by the filesystem CAS. Increase to improve compressed blob throughput, decrease to reduce memory usage. | . | persistentWorkerActionMnemonicAllowlist | Set, _\"*\"_ |   | Controls which Action Mnemonics can run with a persistent worker. Use * for wildcard. | . worker: port: 8981 publicName: \"localhost:8981\" linkedInputDirectories: - \"^path/to/common/directory\" . Capabilities . | Configuration | Accepted and Default Values | Description | . | cas | boolean, true | Enables worker to be a shard of the CAS | . | execution | boolean, true | Enables worker to participate in execution pool | . Example: . worker: capabilities: cas: true execution: true . Sandbox Settings . Using the sandbox can be configurable by the client via exec_properties. However, sometimes it is preferred to enable it via buildfarm config to prevent users from running actions outside the sandbox. | Configuration | Accepted and Default Values | Description | . | alwaysUseSandbox | boolean, false | Whether or not to always use the sandbox when running actions. It may be preferred to enforce sandbox usage than rely on client selection. | . | alwaysUseAsNobody | boolean, false | Whether or not to always use the as-nobody wrapper when running actions. It may be preferred to enforce this wrapper instead of relying on client selection. | . | alwaysUseCgroups | boolean, true | Whether or not to use cgroups when sandboxing actions. It may be preferred to enforce cgroup usage. | . | alwaysUseTmpFs | boolean, false | Whether or not to always use tmpfs when using the sandbox. It may be preferred to enforce sandbox usage than rely on client selection. | . | additionalWritePaths | List of Strings, [] | Additional paths the sandbox is allowed to write to. Suggestions may include: /tmp, /dev/shm | . | tmpFsPaths | List of Strings, [] | Additional paths the sandbox uses for tmpfs. Suggestions may include: /tmp | . | selectForBlockNetwork | boolean, false | If the action requires “block network” use the sandbox to fulfill this request. Otherwise, there may be no alternative solution and the “block network” request will be ignored / implemented differently. | . | selectForTmpFs | boolean, false | If the action requires “tmpfs” use the sandbox to fulfill this request.execution. Otherwise, there may be no alternative solution and the “tmpfs” request will be ignored / implemented differently. | . Example: . worker: sandboxSettings: alwaysUseSandbox: true alwaysUseAsNobody: false alwaysUseCgroups: true alwaysUseTmpFs: true additionalWritePaths: [] tmpFsPaths: [] selectForBlockNetwork: false selectForTmpFs: false . Note: In order for these settings to take effect, you must also configure limitGlobalExecution: true. Dequeue Match . | Configuration | Accepted and Default Values | Description | . | allowUnmatched | boolean, false |   | . | properties | List of name/value pairs | Pairs of provisions available to match against action properties | . Example: . worker: dequeueMatchSettings: allowUnmatched: false properties: - name: \"gpu\" value: \"nvidia RTX 2090\" . Resources . A list of limited resources that are available to the worker to be depleted by actions which execute containing a “resource:\": \"N\" property. The dequeueMatchSettings may also further limit executions to contain \"resource:\" in properties, with either a specific limited resource count as the only accepted value for the action property. The use case here is one where executions are not allowed to request any value other than the one specified. There are no operators currently for asserting an execution requests 'less than' a particular number of resources. The default resource type is SEMAPHORE. The expected use case is that a resource is internally allocated and managed by an execution, and the exhaustion prevents executions from starting which would block or fault if they could not consume the resource. Common examples include licensed software with tokens, and this mechanism can work for singleton resources like one gpu on a worker. The POOL resource type is intended for externally defined resource allotment. The Integer range [0,amount) will be created in a pool of free ids provided to The common use case here is multiple gpus on a system, where the concurrent execution on all gpus can take place, but each execution completion frees a different gpu that must be targetted with, for example, the CUDA_VISIBLE_DEVICES env var. The pool takes a resource out of a queue of available ids, and replaces it in the queue when the execution is complete. Since externally defined resources need to be injected into an execution, the depletion of a pool resource applies an automatic execution policy, named ‘pool-', and supplies transformations for both '' - expanded to all ids in multiple arguments, or '' for 0-N addressable ids, up to the amount requested. Subsequent literal arguments can provide a delimiter for the wrapper. | Configuration | Accepted and Default Values | Description | . | name | string | Resource identifier present on worker | . | type | SEMAPHORE, POOL | Resource count depleted by actions | . | amount | Integer | Resource count depleted by actions | . Example: . worker: dequeueMatchSettings: properties: - name: \"resource:special-compiler-license\" value: \"1\" # only actions which request one compiler license at a time will be accepted resources: name: \"special-compiler-license\" amount: 3 . Worker CAS . Unless specified, options are only relevant for FILESYSTEM type . | Configuration | Accepted and Default Values | Description | . | type | FILESYSTEM, GRPC | Type of CAS used | . | path | String, cache | Local cache location relative to the ‘root’, or absolute | . | maxSizeBytes | Integer, 0 | Limit for contents of files retained from CAS in the cache, value of 0 means to auto-configure to 90% of root/path underlying filesystem space | . | fileDirectoriesIndexInMemory | boolean, false | Determines if the file directories bidirectional mapping should be stored in memory or in sqlite | . | skipLoad | boolean, false | Determines if transient data on the worker should be loaded into CAS on worker startup (affects startup time) | . | target | String, null | For GRPC CAS type, target for external CAS endpoint | . Example: . This definition will create a filesystem-based CAS file cache at the path “/cache\" on the worker that will reject entries over 2GiB in size, and will expire LRU blobs when the aggregate size of all blobs exceeds 2GiB in order to insert additional entries. worker: storages: - type: FILESYSTEM path: \"cache\" maxSizeBytes: 2147483648 # 2 * 1024 * 1024 * 1024 . This definition elides FILESYSTEM configuration with ‘…’, will read-through an external GRPC CAS supporting the REAPI CAS Services into its storage, and will attempt to write expiring entries into the GRPC CAS (i.e. pushing new entries into the head of a worker LRU list will drop the entries from the tail into the GRPC CAS). worker: storages: - type: FILESYSTEM ... - type: GRPC target: \"cas.external.com:1234\" . Execution Policies . | Configuration | Accepted and Default Values | Description | . | name | String | Execution policy name | . | prioritized | Boolean, false | If true, policy will run before built-in policies | . | executionWrapper | Execution wrapper, containing a path and list of arguments | Execution wrapper, its path and a list of arguments for the wrapper | . Example: . worker: executionPolicies: - name: as-nobody prioritized: true executionWrapper: path: /app/build_buildfarm/as-nobody arguments: - \"-u\" - \"&lt;exec-owner&gt;\" - name: unshare executionWrapper: path: /usr/bin/unshare arguments: - \"-n\" - \"-r\" - name: linux-sandbox executionWrapper: path: /app/build_buildfarm/linux-sandbox arguments: # use \"--\" to signal the end of linux-sandbox args. \"--\" should always be last! - \"--\" - name: test executionWrapper: path: /YOUR/WRAPPER arguments: - arg1 - arg2 - \"&lt;platform-property-name&gt;\" . arg1 and arg2 are interpreted literally. &lt;platform-property-value&gt; will be substituted with the value of a property named \"platform-property-name\" from a Command’s Platform or the requested pool resources for the execution. If a matching property or pool resource is not found for a specified name, the entire wrapper will be discarded and have no effect on the execution. &lt;exec-owner&gt; is an automatically provided pool resource when execOwner or execOwners is specified, and will contain the value of the execution’s owner selected for exec tree creation. An execution with as-nobody, unshare, and linux-sandbox execution policies enabled would produce a command line like: . /app/build_buildfarm/as-nobody -u &lt;exec-owner&gt; /usr/bin/unshare -n -r /app/build_buildfarm/linux-sandbox -- /YOUR/WRAPPER arg1 arg2 &lt;platform-property-name&gt; ACTION . where ACTION is the Command from remote execution action. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/configuration/configuration/#all-configurations",
    "relUrl": "/docs/configuration/configuration/#all-configurations"
  },"31": {
    "doc": "Configuration",
    "title": "Configuration",
    "content": "Minimal required: . backplane: redisUri: \"redis://localhost:6379\" queues: - name: \"cpu\" properties: - name: \"min-cores\" value: \"*\" - name: \"max-cores\" value: \"*\" worker: publicName: \"localhost:8981\" . Configuration files also support includes via !include tag: . backplane: !include \"backplane.yml\" server: !include \"server.yml\" worker: !include \"worker.yml\" . The configuration can be provided to the server and worker as a CLI argument or through the environment variable CONFIG_PATH For an example configuration containing all of the configuration values, see examples/config.yml. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/configuration/configuration/",
    "relUrl": "/docs/configuration/configuration/"
  },"32": {
    "doc": "CAS",
    "title": "Methods",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/#methods",
    "relUrl": "/docs/architecture/content_addressable_storage/#methods"
  },"33": {
    "doc": "CAS",
    "title": "Reads",
    "content": "A read of content from a CAS is a relatively simple procedure, whether accessed through BatchReadBlobs, or the ByteStream Read method. The semantics associated with these requests require the support of content availability translation (NOT_FOUND for missing), and seeking to a particular offset to start reading. Clients are expected to behave progressively, since no size limitation nor bandwidth availability is mandated, meaning that they should advance an offset along the length of the content until complete with successive requests, assuming DEADLINE_EXCEEDED or other transient errors occur during the download. resource_name for reads within ByteStream Read must be \"{instance_name}/blobs/{hash}/{size}\" . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/#reads",
    "relUrl": "/docs/architecture/content_addressable_storage/#reads"
  },"34": {
    "doc": "CAS",
    "title": "Writes",
    "content": "Writes of content into a CAS require a prior computation of an address with a digest method of choice for all content. A write can be initiated with BatchUpdateBlobs or the ByteStream Write method. A ByteStream Write resource_name must begin with {instance_name}/uploads/{uuid}/blobs/{hash}/{size}, and may have any trailing filename after the size, separated by ‘/’. The trailing content is ignored. The uuid is a client generated identifier for a given write, and may be shared among many digests, but should be strictly client-local. Writes should respect a WriteResponse received at any time after initiating the request of the size of the blob, to indicate that no further WriteRequests are necessary. Writes which fail prior to the receipt of content should be progressive, checking for the offset to resume an upload via ByteStream QueryWriteStatus. Buildfarm implements the CAS in a variety of ways, including an in-memory storage for the reference implementation, as a proxy for an external CAS, an HTTP/1 proxy based on the remote-cache implementation in bazel, and as a persistent on-disk storage for workers, supplementing an execution filesystem for actions as well as participating in a sparsely-sharded distributed store. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/#writes",
    "relUrl": "/docs/architecture/content_addressable_storage/#writes"
  },"35": {
    "doc": "CAS",
    "title": "Buildfarm Implementations",
    "content": "Since these implementations vary in complexity and storage semantics, a common interface was declared within Buildfarm to accommodate substitutions of a CAS, as well as standardize its use. The specifics of these CAS implementations are detailed here. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/#buildfarm-implementations",
    "relUrl": "/docs/architecture/content_addressable_storage/#buildfarm-implementations"
  },"36": {
    "doc": "CAS",
    "title": "Memory",
    "content": "The memory CAS implementation is extremely simple in design, constituting a maximum size with LRU eviction policy. Entry eviction is a registrable event for use as a storage for the delegated ActionCache, and Writes may be completed asynchronously by concurrent independent upload completion of an entry. This is the example presentation of a CAS in the memory instance available here, but for reference, specification in any cas_config field for server or worker will enable the creation of a unique instance. worker: storages: - type: MEMORY maxSizeBytes: 2147483648 # 2 * 1024 * 1024 * 1024 . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/#memory",
    "relUrl": "/docs/architecture/content_addressable_storage/#memory"
  },"37": {
    "doc": "CAS",
    "title": "GRPC",
    "content": "This is a CAS which completely mirrors a target CAS for all requests, useful as a proxy to be embedded in a full Instance declaration. A grpc config example is available in the alternate instance specification in the memory server example here. For reference: . server: name: shard worker: storages: - type: FILESYSTEM path: \"cache\" - type: GRPC target: . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/#grpc",
    "relUrl": "/docs/architecture/content_addressable_storage/#grpc"
  },"38": {
    "doc": "CAS",
    "title": "HTTP/1",
    "content": "The HTTP/1 CAS proxy hosts a GRPC service definition for a configurable target HTTP/1 service that it communicates with using an identical implementation to the bazel http remote cache protocol. Since the HTTP/1 proxy is a separate process, there are no configuration options for it. Instead, run the proxy in a known location (address and port), and use a grpc configuration indicated above, pointing to its address and instance name. The proxy can be run with: . bazel run src/main/java/build/buildfarm/tools:buildfarm-http-proxy -- -p 8081 -c \"http://your-http-endpoint\" --noreadonly . And will result in a listening grpc service on port 8081 on all interfaces, relaying requests to the endpoint in question. Use --help to see more options. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/#http1",
    "relUrl": "/docs/architecture/content_addressable_storage/#http1"
  },"39": {
    "doc": "CAS",
    "title": "Shard",
    "content": "A sharded CAS leverages multiple Worker CAS retention and proxies requests to hosts with isolated CAS shards. These shards register their participation and entry presentation on a ShardBackplane. The backplane maintains a mapping of addresses to the nodes which host them. The sharded CAS is an aggregated proxy for its members, performing each function with fallback as appropriate; FindMissingBlobs requests are cycled through the shards, reducing a list of missing entries, Writes select a target node at random, Reads attempt a request on each advertised shard for an entry with failover on NOT_FOUND or transient grpc error. Reads are optimistic, given that a blob would not be requested that was not expected to be found, the sharded CAS will failover on complete absence of a blob to a whole cluster search for an entry. A shard CAS is the default for the Shard Instance type, with its required backplane specification. Since functionality between Shard CAS, AC, and Execution are mixed in here, the definition is somewhat cluttered, with efforts to refine specific aspects of it underway. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/#shard",
    "relUrl": "/docs/architecture/content_addressable_storage/#shard"
  },"40": {
    "doc": "CAS",
    "title": "Worker CAS",
    "content": "Working hand in hand with the Shard CAS implementation, the Worker CAS leverages a requisite on-disk store to provide a CAS from its CASFileCache. Since the worker maintains a large cache of inputs for use with actions, this CAS is routinely populated from downloads due to operation input fetches in addition to uploads from the Shard frontend. CasFileCache . The worker’s CAS file cache uses persistent disk storage. A strongly recommended filesystem to back this is XFS, due to its high link counts limits per inode. A strongly discouraged filesystem is ext4, which places a hard limit of 65000 link counts per inode. The layout of the files are ordered such that file content, in the form of canonical digest filenames for inode storage, remains on the root of the cache directory, while exec roots and symlinkable directories contain hard links to these inodes. This avoids unnecessary duplication of file contents. Upon worker startup, the worker’s cache instance is initialized in two phases. First, the root is scanned to store file information. Second, the existing directories are traversed to compute their validating identification. Files will be automatically deleted if their file names are invalid for the cache, or if the configured cache size has been exceeded by previous files. The shard worker allows a flexible specification, with delegates available of the other types to fall back on for CAS expansion, and encapsulated CAS configs . CASTest is a standalone tool to load the cache and print status information about it. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/#worker-cas",
    "relUrl": "/docs/architecture/content_addressable_storage/#worker-cas"
  },"41": {
    "doc": "CAS",
    "title": "CAS",
    "content": "A Content Addressable Storage (CAS) is a collection of service endpoints which provide read and creation access to immutable binary large objects (blobs). The core service is declared in the Remote Execution API, and also requires presentation of the ByteStream API with specializations for resource names and behaviors. An entry in the CAS is a sequence of bytes whose computed digest via a hashing function constitutes its address. The address is specified as either a [Digest] message, or by the makeup of a resource name in ByteStream requests. A CAS supports several methods of retrieving and inserting entries, as well as some utility methods for determining presence and iterating through linked hierarchies of directories as Merkle Trees. Functionally within the REAPI, the CAS is the communication plane for Action inputs and outputs, and is used to retain other contents by existing clients, including bazel, like build event information. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/content_addressable_storage/",
    "relUrl": "/docs/architecture/content_addressable_storage/"
  },"42": {
    "doc": "Contribute",
    "title": "Edit the site",
    "content": "The site is rendered with jeykll. To see changes locally run: . | cd _site | bundle install | jekyll serve | . You should see a local URL like: http://127.0.0.1:4000/buildfarm/ . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/contribute/contribute/#edit-the-site",
    "relUrl": "/docs/contribute/contribute/#edit-the-site"
  },"43": {
    "doc": "Contribute",
    "title": "Contribute",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/contribute/contribute/",
    "relUrl": "/docs/contribute/contribute/"
  },"44": {
    "doc": "Debugging",
    "title": "Debugging Executions",
    "content": "This tutorial is intended to guide you in debugging executions that are run on buildfarm. Problem . Build actions (and more specifically unit tests) often behave differently across environments. This makes it difficult to understand and debug remote action behavior. The execution environment of buildfarm may seem like an inaccessible black-box to the client. The problem is further complicated by the fact that buildfarm’s executor may not be using the same tools locally as the build system (such as bazel’s sandbox or process-wrapper). In buildfarm, these tools are chosen based on the configuration of execution wrappers, and custom execution wrappers exist as well. Buildfarm may also virtualize hardware and apply restrictions on resources such as cpu, memory, and network. Additionally, buildfarm actions do not always run on the same machine. Actions in buildfarm are directed to particular eligible workers based on their platform properties. Despite the platform properties of the worker chosen, the action may run inside a docker container which also affects the execution environment. Capturing debug information (before execution) . There are execution properties available to help you capture debug information dynamically when performing executions. To capture initial information about an execution, you can use the following: . bazel test --remote_default_exec_properties='debug-before-execution=true' \\ --remote_executor=grpc://127.0.0.1:8980 \\ --noremote_accept_cached \\ --nocache_test_results //&lt;TARGET&gt; . Assuming the action is properly queued and reaches the executor, the executor will deliberately fail the action and send you debug information via json through the action’s stderr. You can view this debug information by reading the failure log: . cat source/bazel-out/k8-fastbuild/testlogs/&lt;TARGET&gt;/test.log . You can streamline viewing this information via --test_output=streamed . bazel test \\ --test_output=streamed \\ --remote_default_exec_properties='debug-before-execution=true' \\ --noremote_accept_cached \\ --nocache_test_results \\ //&lt;TARGET&gt; . Capturing debug information (after execution) . A similar property exists that will tell the executor to run the action, and after the action finishes, return debug information. bazel test \\ --test_output=streamed \\ --remote_default_exec_properties='debug-after-execution=true' \\ --noremote_accept_cached \\ --nocache_test_results \\ //&lt;TARGET&gt; . If your action is hanging, you might not be able to get this debug information and should stick with ‘debug-before-execution=true’. If your action is finishing, this would be a ideal to use, as it should provide all the same information that ‘debug-before-execution=true’ does, with additional info about the execution. Configuration . If you see an error like this: . properties are not valid for queue eligibility: [name: \"debug-before-execution\" value: \"true\"] . you will need to configure the queue’s allow_unmatched to true so that the server can still put the action on the queue. Additionally you may want to configure the the worker’s DequeueMatchSettings to also have allow_unmatched to true. That will ensure the action does not fail reaching one of the worker due to the additional exec_property. Debugging Tests . By default, debug-before-execution and debug-after-execution only apply to test actions. The reason being, is that if you wanted to debug a test, but passed a global property like --remote_default_exec_properties='debug-before-execution=true', it would invalidate all the actions, and the test would need rebuilt, but the rebuild of the test would fail, because you would actually be debugging the first build action, and you would never see the debug results of the test action. Debugging tests is more typical, but you can also debug build actions by using --remote_default_exec_properties='debug-tests-only=false'. It is more convenient to debug things by passing the global exec properties, but you could also tag targets specifically in the BUILD files with these debug options. Finding Operations . All buildfarm operations can be found using the following query: . bazel run //src/main/java/build/buildfarm:bf-find-operations localhost:8980 shard SHA256 . When you run a build invocation with bazel, bazel will you give you an invocation id. You can use that to query your specific operations: . bazel run //src/main/java/build/buildfarm:bf-find-operations localhost:8980 shard SHA256 \"[?(@.operation.metadata.requestMetadata.toolInvocationId == '1877f43a-9b33-4eca-9d6b-aef71b47bf47')]\" . You can find the operation of a specific test name like this: . bazel run //src/main/java/build/buildfarm:bf-find-operations localhost:8980 shard SHA256 \"$.command.environmentVariables[?(@.value == '//code/tools/example_tests/bash_hello_world2:main')]\" . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/debugging-executions/#debugging-executions",
    "relUrl": "/docs/execution/debugging-executions/#debugging-executions"
  },"45": {
    "doc": "Debugging",
    "title": "Debugging",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/debugging-executions/",
    "relUrl": "/docs/execution/debugging-executions/"
  },"46": {
    "doc": "Design Documents",
    "title": "Design Documents",
    "content": "Infinite Cache (Storage Workers) Local and Global Resources . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/contribute/design-documents/",
    "relUrl": "/docs/contribute/design-documents/"
  },"47": {
    "doc": "Environment",
    "title": "Environment",
    "content": "Since workers are expected to execute programs in a way that makes using remote transparent to build users, there is a great deal of nuance to their definition. Operating System, Distribution, runtimes, compilers, their versions, and standard libraries make this a tricky proposition. While hermeticizing a build by declaring the full set of tools a compilation requires is the ‘right’ solution for a build, it may be too big of a hill to climb for everyone, particularly those trying to ease into remote execution. Under Linux at least, this is made somewhat easier through docker, even if the steps to get to a conformant environment are a bit complicated. Here we will provide an example of creating a target execution environment container capable of running buildfarm, and this should be similar for all major distributions, particularly those with released docker hub bases and standard package managers including java runtimes and toolchain software. You should choose the versions of relevant software at first based on the client environment you want to support. For this example, we’re going to assume a target of ubuntu-20 (focal), with a gcc9 compiler supporting C++, and a java runtime from openjdk-14 supplied by its package manager. We will need: . | a bazel 3.3.1 install (the older version is required by rules_docker at the buildfarm version, this will be updated eventually). | docker daemon running | . First we will pull our intended base image from dockerhub: . docker pull ubuntu:focal . Next we will create our Dockerfile for our customized environment. Here is the content of that Dockerfile . # choose our ubuntu distribution with version 20 (focal) from ubuntu:focal # get our current software suite. apt-get is preferred to apt due to warnings # about a non-stable CLI run apt-get update # install the basic apt-utils to limit warnings due to its absence run DEBIAN_FRONTEND=noninteractive apt-get install --no-install-suggests apt-utils # upgrade to the current software suite versions run DEBIAN_FRONTEND=noninteractive apt-get dist-upgrade -y # install the required packages for our execution environment and buildfarm runtime run DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-suggests \\ g++-9 g++ openjdk-14-jdk-headless . And build it with docker build -t ubuntu20-java14:latest . Sending build context to Docker daemon 2.56kB Step 1/5 : from ubuntu:focal ---&gt; 9140108b62dc ... Removing intermediate container 505236be00e4 ---&gt; 77a8c2ae4d16 Successfully built 77a8c2ae4d16 Successfully tagged ubuntu20-java14:latest . In our example’s case, we want to use this image as a base to build a buildfarm worker container image. Unfortunately, rules_docker does not know how to interact with the docker images hosted by a local docker daemon, only docker registries like dockerhub.io. We will get past this by running the local registry utility container. If you really want to host this base elsewhere, I’ll assume that you know how to translate the host:port references below, and you can skip running your own registry. First we need to tag our image so that it gets pushed to our local registry: . docker tag ubuntu20-java14:latest localhost.localdomain:5000/ubuntu20-java14:latest . Then we can start our registry with a recognizable name for later shutdown: . docker run -d --rm --name local-registry -p 5000:5000 registry . And push our newly tagged image: . docker push localhost.localdomain:5000/ubuntu20-java14:latest . Take note of the sha256:&lt;sha256sum&gt; output that was produced with this command, as we will need to use it to specify our base. Now we have a referent docker image that can be identified as a base to apply buildfarm’s worker installation onto. In a suitable location, create a WORKSPACE containing: . load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") BUILDFARM_EXTERNAL_COMMIT = \"04fc2635f5546a4f5dc19dea35bb1fca5569ce24\" BUILDFARM_EXTERNAL_SHA256 = \"f45215ef075c8aff230b737ca3bc5ba183c1137787fcbbb10dd407463f76edb6\" http_archive( name = \"build_buildfarm\", strip_prefix = \"bazel-buildfarm-%s\" % BUILDFARM_EXTERNAL_COMMIT, sha256 = BUILDFARM_EXTERNAL_SHA256, url = \"https://github.com/buildfarm/buildfarm/archive/%s.zip\" % BUILDFARM_EXTERNAL_COMMIT, ) load(\"@build_buildfarm//:deps.bzl\", \"buildfarm_dependencies\") buildfarm_dependencies() load(\"@build_buildfarm//:defs.bzl\", \"buildfarm_init\") buildfarm_init() load(\"@io_bazel_rules_docker//repositories:deps.bzl\", container_deps = \"deps\") load(\"@io_bazel_rules_docker//container:container.bzl\", \"container_pull\") container_deps() container_pull( name = \"ubuntu20_java14_image_base\", digest = \"sha256:&lt;sha256sum&gt;\", registry = \"localhost:5000\", repository = \"ubuntu20-java14\", ) . Be sure to substitute the sha256:&lt;sha256sum&gt; with your content from above. Next we will create a BUILD file to create our target image. We will use the shard variant here, but the memory worker (with supporting server execution) will work as well. The content of the BUILD file should be: . load(\"@io_bazel_rules_docker//container:container.bzl\", \"container_image\") container_image( name = \"buildfarm-shard-worker-ubuntu20-java14\", base = \"@ubuntu20_java14_image_base//image\", files = [ \"@build_buildfarm//src/main/java/build/buildfarm:buildfarm-shard-worker_deploy.jar\", ], ) . And now that this is in place, we can use the following to build the container and make it available to our local docker daemon: . bazel run :buildfarm-shard-worker-ubuntu20-java14 . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/environment/",
    "relUrl": "/docs/execution/environment/"
  },"48": {
    "doc": "Owner",
    "title": "Owner",
    "content": "exec_owner is a global worker configuration option for Shard workers which indicates a username on the worker that all output directories for executions will own. The InputFetchStage uses this value to specify the filesystem owner user id for Action-&gt;Command OutputFiles and OutputDirectories (soon to be OutputPaths in REAPI 2.1) paths created during exec root preparation. Specification of this option requires that the filesystem ownership change is available to the worker process principal. For example, a worker which executes as root under Unix would be capable of changing directory ownership. This option only affects file ownership, not effective user id of the action process. This option can be coupled with a default ExecutionPolicy wrapper that performs an execution persona change (i.e. setuid(2) under Unix), to execute a command with privileges that limit it in writing. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/exec_owner/",
    "relUrl": "/docs/execution/exec_owner/"
  },"49": {
    "doc": "Execution",
    "title": "Execution",
    "content": "Information about the execution of an action on buildfarm. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution/",
    "relUrl": "/docs/execution/execution/"
  },"50": {
    "doc": "Limiting",
    "title": "Limiting",
    "content": "The ExecuteActionStage on Shard Workers can limit the resources available during Command execution per the Platform. This feature currently requires OS-specific cgroups support, available in Linux. The boolean configuration field limit_execution controls the creation of a worker-wide cgroup. This cgroup currently limits cpu resources to the the ExecuteStageWidth (one core per slot), and other worker-wide cgroup controls may be available in the future. Other limiting fields also require this option to be enabled. This does not mandate that all executions take place in this created cgroup. If true, the boolean configuration field limit_global_execution specifies that all executions take place in the above cgroup by default. This currently shrinks the effective cpu area available to be shared by all executions to the width of the execute stage. With limit_execution enabled, a Command may request that it be executed with min-cores available to it (lower bound) and up to max-cores. These are field names in the Platform which are expected to be integers in the string value. After selective consumption of the superscalar execution slots for min-cores, the executing processes will be controlled by cgroups under the above cgroup to give it as many shares of the total (preferred scheduling) for min-cores, and have its scheduling quota limited to a fair number consistent with its max-cores. The effective Platform of an execution can also be affected by the Worker’s default_platform, where it can inherit either of min/max-cores if unspecified. The Worker configuration field only_multicore_tests can further affect the values of min/max-cores by clamping them to 1 if enabled, based on the detection of ‘bazel-like’ test environments for executions: The detection of an environment variable XML_OUTPUT_FILE under the command is the current qualifier . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_limiting/",
    "relUrl": "/docs/execution/execution_limiting/"
  },"51": {
    "doc": "Policies",
    "title": "Wrapper execution policy modifier type",
    "content": "This policy type specifies that a worker should prepend a single path, and a number of arguments, to the execution of a subprocess to generate an action result. These arguments have a limited substitution mechanism that discovers any appearance of &lt;property-name&gt; and substitutes it with a string representation of a value currently available in the platform properties for the action. If a specified property-name platform property is not present for the action, the wrapper is discarded entirely. Note that this substitution does not apply to the path of the wrapper, which may point to any file with appropriate permissions - executable, and readable if necessary as a shell script, on linux, for example. Example: . This example will use the buildfarm-provided executable as-nobody, which will upon execution demote itself to a nobody effective process owner uid, and perform an execvp(2) with the remaining provided program arguments, which will subsequently execute as a user that no longer matches the worker process. # default wrapper policy application worker: executionPolicies: - name: test executionWrapper: path: \"/app/buildfarm/as-nobody\" . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_policies/#wrapper-execution-policy-modifier-type",
    "relUrl": "/docs/execution/execution_policies/#wrapper-execution-policy-modifier-type"
  },"52": {
    "doc": "Policies",
    "title": "Action Specification",
    "content": "An execution may be requested for an Action definition which includes the platform property execution-policy, which will select from the available execution policies present on a worker. A worker will not execute any action for which it does not have definitions matching its requested policies, similar to other platform requirements. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_policies/#action-specification",
    "relUrl": "/docs/execution/execution_policies/#action-specification"
  },"53": {
    "doc": "Policies",
    "title": "Built-in Execution Policies",
    "content": "Buildfarm images are packaged with custom execution wrappers to be used as policies. Some of these wrappers are chosen dynamically based on the action. For example, the bazel-sandbox is included with buildfarm and can be chosen with exec_property{\"linux-sandbox\": \"True\"}. Below is a description of the execution wrappers provided: . process-wrapper . The process wrapper also used by bazel to allow Java to reliably kill processes. It is recommended this is used on all actions. linux-sandbox . The sandbox bazel uses when running actions client-side. This provides many isolations for actions such as tmpfs and block-network. It may also have performance issues. We include it with buildfarm to ensure additional consistency between local/remote actions. tini . tini is a a tiny but valid init for containers. Depending on how buildfarm is containerized you may want to use. as-nobody . This is used to set the action’s user to “nobody”. Otherwise buildfarm will run the action as root which may be undesirable. skip_sleep (skip_sleep.preload + delay) . These wrappers are used for detecting actions that rely on time. Below is a demonstration of how they can be used. This addresses two problems in regards to an action’s dependence on time. The 1st problem is when an action takes longer than it should because it’s sleeping unnecessarily. The 2nd problem is when an action relies on time which causes it to eventually be broken on master despite the code not changing. Both problems are expressed below as unit tests. We demonstrate a time-spoofing mechanism (the re-writing of syscalls) which allows us to detect these problems generically over any action. The objective is to analyze builds for performance inefficiency and discover future instabilities before they occur. Issue 1 (slow test) . #!/usr/bin/env bash set -euo pipefail echo -n \"testing... \" sleep 10; echo \"done\" . The test takes 10 seconds to run on average. $ bazel test --runs_per_test=10 --config=remote //cloud/buildfarm:sleep_test //cloud/buildfarm:sleep_test PASSED in 10.2s Stats over 10 runs: max = 10.2s, min = 10.1s, avg = 10.2s, dev = 0.0s . We can check for performance improvements by using the skip-sleep option. $ bazel test --runs_per_test=10 --config=remote --remote_default_exec_properties='skip-sleep=true' //cloud/buildfarm:sleep_test //cloud/buildfarm:sleep_test PASSED in 1.0s Stats over 10 runs: max = 1.0s, min = 0.9s, avg = 1.0s, dev = 0.0s . Now the test is 10x faster. If skipping sleep makes an action perform significantly faster without affecting its success rate, that would warrant further investigation into the action’s implementation. Issue 2 (future failing test) . #!/usr/bin/env bash set -euo pipefail CURRENT_YEAR=$(date +\"%Y\") if [[ \"$CURRENT_YEAR\" -eq \"2021\" ]]; then echo \"The year matches.\" date exit 0; fi; echo \"Times change.\" date exit -1; . The test passes today, but will it pass tomorrow? Will it pass a year from now? We can find out by using the time-shift option. $ bazel test --test_output=streamed --remote_default_exec_properties='time-shift=31556952' --config=remote //cloud/buildfarm:future_fail INFO: Found 1 test target... Times change. Mon Sep 25 18:31:09 UTC 2023 //cloud/buildfarm:future_fail FAILED in 18.0s . Time is shifted to the year 2023 and the test now fails. We can fix the problem before others see it. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_policies/#built-in-execution-policies",
    "relUrl": "/docs/execution/execution_policies/#built-in-execution-policies"
  },"54": {
    "doc": "Policies",
    "title": "Policies",
    "content": "Execution Policies can be defined to modify or control Action execution on workers. A given policy can be assigned a name, with a policy with a default name (the empty string) affecting all executions on a worker, as well as a single type of modifier. Policies are applied in order of definition for a single matching name, with the default policies applied first, followed by the order specified effective by the action. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_policies/",
    "relUrl": "/docs/execution/execution_policies/"
  },"55": {
    "doc": "Properties",
    "title": "Core Selection:",
    "content": "min-cores . description: the minimum number of cores needed by an action. Should be set to &gt;= 1 Workers and queues can be configured to behave differently based on this property. max-cores . description: the maximum number of cores needed by an action. Buildfarm will enforce a max. Workers and queues can be configured to behave differently based on this property. cores . description: the minimum &amp; maximum number of cores needed by an action. This sets both min-cores and max-cores accordingly. use case: very often you want unit tests (or all actions in general) to be constrained to a core limit via cgroups. This is relevant for performance and stability of the worker as multiple tests share the same hardware as the worker. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_properties/#core-selection",
    "relUrl": "/docs/execution/execution_properties/#core-selection"
  },"56": {
    "doc": "Properties",
    "title": "Memory Selection:",
    "content": "min-mem . description: the minimum amount of bytes the action may use. max-mem . description: the maximum amount of bytes the action may use. use case: very often you want unit tests (or all actions in general) to be constrained to a memory limit via cgroups. This is relevant for performance and stability of the worker as multiple tests share the same hardware as the worker. Tests that exceed their memory requirements will be killed. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_properties/#memory-selection",
    "relUrl": "/docs/execution/execution_properties/#memory-selection"
  },"57": {
    "doc": "Properties",
    "title": "Execution Settings:",
    "content": "linux-sandbox . description: Use bazel’s linux sandbox as an execution wrapper. fake-hostname . description: Uses localhost as the hostname during execution. Assumes the usage of the linux sandbox. block-network . description: Creates a new network namespace. Assumes the usage of the linux sandbox. tmpfs . description: Mounts an empty tmpfs under /tmp for the action. Assumes the usage of the linux sandbox. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_properties/#execution-settings",
    "relUrl": "/docs/execution/execution_properties/#execution-settings"
  },"58": {
    "doc": "Properties",
    "title": "Queue / Pool Selection:",
    "content": "choose-queue . description: place the action directly on the chosen queue (queue name must be known based on buildfarm configuration). use case: Other remote execution solutions have slightly different paradigms on deciding where actions go. They leverage execution properties for selecting a “pool” of machines to send the action. We sort of have a pool of workers waiting on particular queues. For parity with this concept, we support this execution property which will take precedence in deciding queue eligibility. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_properties/#queue--pool-selection",
    "relUrl": "/docs/execution/execution_properties/#queue--pool-selection"
  },"59": {
    "doc": "Properties",
    "title": "Extending Execution:",
    "content": "env-var / env-vars . description: ensure the action is executed with additional environment variables. These variables are applied last in the order given. env-var expects a single key/value like --remote_default_exec_properties=env-var:FOO=VALUE env-vars expects a key/json like --remote_default_exec_properties=env-vars='{\"FOO\": \"VALUE\",\"FOO2\": \"VALUE2\"}' . use case: Users may need to set additional environment variables through exec_properties. Changing code or using --action_env may be less feasible than specifying them through these exec_properties. Additionally, the values of their environment variables may need to be influenced by buildfarm decisions. example: pytorch tests can still see the underlying hardware through /proc/cpuinfo. Despite being given 1 core, they see all of the cpus and decide to spawn that many threads. This essentially starves them and gives poor test performance (we may spoof cpuinfo in the future). Another solution is to use env vars OMP_NUM_THREADS and MKL_NUM_THREADS. This could be done in code, but we can’t trust that developers will do it consistently or keep it in sync with min-cores / max-cores. Allowing these environment variables to be passed the same way as the core settings would be ideal. Standard Example: This test will succeed when env var TESTVAR is foobar, and fail otherwise. #!/usr/bin/env bash [ \"$TESTVAR\" = \"foobar\" ] . $ ./bazel test \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main FAIL . $ ./bazel test --remote_default_exec_properties='env-vars={\"TESTVAR\": \"foobar\"}' \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main PASS . Template Example: If you give a range of cores, buildfarm has the authority to decide how many your operation actually claims. You can let buildfarm resolve this value for you (via mustache). #!/usr/bin/env bash [ \"$MKL_NUM_THREADS\" = \"1\" ] . $ ./bazel test \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main FAIL . $ ./bazel test \\ --remote_default_exec_properties='env-vars=\"MKL_NUM_THREADS\": \"\"' \\ --remote_executor=grpc://127.0.0.1:8980 --noremote_accept_cached --nocache_test_results \\ //env_test:main PASS . Available Templates: : what buildfarm has decided is a valid min core count for the action. : what buildfarm has decided is a valid max core count for the action. ``: buildfarm’s decision on how many cores your action should claim. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_properties/#extending-execution",
    "relUrl": "/docs/execution/execution_properties/#extending-execution"
  },"60": {
    "doc": "Properties",
    "title": "Debugging Execution:",
    "content": "debug-before-execution . description: Fails the execution with important debug information on how the execution will be performed. use case: Sometimes you want to know the exact execution context and cli that the action is going to be run with. This can help any situation where local action behavior seems different than remote action behavior. debug-after-execution . description: Runs the execution, but fails it afterward with important debug information on how the execution was performed. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_properties/#debugging-execution",
    "relUrl": "/docs/execution/execution_properties/#debugging-execution"
  },"61": {
    "doc": "Properties",
    "title": "Execution Flow:",
    "content": "This is a special set of action mnemonics (not Platform properties) that can be used to test the flow of actions through the execution segments (prequeue/queue). In all of these cases, the buildfarm agent which observes the operation at the halt state will deliver a done operation with no further processing. buildfarm:halt-on-execute . Server halts execution after receiving the execute request. This is the earliest possible halt after creating the operation. buildfarm:halt-on-deprequeue . Server which removes the prequeue entry halts execution. This guarantees flow through the prequeue infrastructure. buildfarm:halt-on-dequeue . Worker which removes the queue entry halts execution. This guarantees flow through the queue and match. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_properties/#execution-flow",
    "relUrl": "/docs/execution/execution_properties/#execution-flow"
  },"62": {
    "doc": "Properties",
    "title": "Additional Information",
    "content": "Custom properties can also be added to buildfarm’s configuration in order to facilitate queue matching (see Queues). Please note that not all execution properties may be relevant to you or the best option depending on your build client. For example, some execution properties were created to facilitate behavior before bazel had a better solution in place. Buildfarm’s configuration for accepting execution properties can be strict or flexible. Buildfarm has been used alongside other remote execution tools and allowing increased flexibility on these properties is necessary so the solutions can coexist for the same targets. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_properties/#additional-information",
    "relUrl": "/docs/execution/execution_properties/#additional-information"
  },"63": {
    "doc": "Properties",
    "title": "Properties",
    "content": "This page contains all of the execution properties supported by Buildfarm. Users can also customize buildfarm to understand additional properties that are not listed here (This is often done when configuring the Operation Queue). ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/execution_properties/",
    "relUrl": "/docs/execution/execution_properties/"
  },"64": {
    "doc": "Features",
    "title": "General Features",
    "content": "Buildfarm has endeavored to support a wide variety of features implied or mandated by the Remote Execution API, including those currently not in use or worked around by bazel or other clients. Most notably, buildfarm has universal support for: . | configurable instances with specific instance types | progressive and flow controlled CAS reads and writes | pluggable external CAS endpoints | RequestMetadata behavior attribution | . Bazel Client Feature Usage: . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/features/#general-features",
    "relUrl": "/docs/features/#general-features"
  },"65": {
    "doc": "Features",
    "title": "Fetch API",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/features/#fetch-api",
    "relUrl": "/docs/features/#fetch-api"
  },"66": {
    "doc": "Features",
    "title": "Wire Compression",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/features/#wire-compression",
    "relUrl": "/docs/features/#wire-compression"
  },"67": {
    "doc": "Features",
    "title": "Independent Digest Functions",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/features/#independent-digest-functions",
    "relUrl": "/docs/features/#independent-digest-functions"
  },"68": {
    "doc": "Features",
    "title": "Builds Without The Bytes - Read This",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/features/#builds-without-the-bytes---read-this",
    "relUrl": "/docs/features/#builds-without-the-bytes---read-this"
  },"69": {
    "doc": "Features",
    "title": "Features",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/features/",
    "relUrl": "/docs/features/"
  },"70": {
    "doc": "Home",
    "title": "What is buildfarm?",
    "content": "Buildfarm is a service software stack which presents an implementation of the Remote Execution API. This means it can be used by any client of that API to retain content in a ContentAddressableStorage, cache ActionResults by a key ActionCache, and execute actions asynchronously with Execution. Buildfarm supports many platforms and has been heavily tested with bazel as a client. This documentation is a comprehensive description of the architecture, features, functionality, and operation, as well as a guide to smoothly installing and running the software. Familiarity with the Remote Execution API is helpful, and references to it will be provided as needed. ",
    "url": "https://buildfarm.github.io/buildfarm/#what-is-buildfarm",
    "relUrl": "/#what-is-buildfarm"
  },"71": {
    "doc": "Home",
    "title": "Home",
    "content": ". Remote Caching and Execution Service . Get started now View it on GitHub . ",
    "url": "https://buildfarm.github.io/buildfarm/",
    "relUrl": "/"
  },"72": {
    "doc": "Instance Types",
    "title": "Instance Types",
    "content": "These are the supported instance types selectable for concrete definition of an instance declared for servers in config. The types here are implementations of the Instance interface. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/instance_types/",
    "relUrl": "/docs/architecture/instance_types/"
  },"73": {
    "doc": "Instance Types",
    "title": "Memory",
    "content": "This is a reference implementation of the Remote Execution API which provides an in-memory CAS, AC, and OperationQueue. It has no persistent retention of its state internally, but can be configured to use an external gRPC endpoint for CAS operations, allowing it to act as a proxy. Instances of this type cannot share Operation information across multiple servers/hosts. It presents a matching interface for workers via an OperationQueue service definition, which provides blocking queue take functionality, as well as a put for results, and it maintains watchdogs for all outstanding operations, with expiration resulting in reentrance to the queue, assuming that all input preconditions are still met at that time. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/instance_types/#memory",
    "relUrl": "/docs/architecture/instance_types/#memory"
  },"74": {
    "doc": "Instance Types",
    "title": "Shard",
    "content": "The shard instance type is a frontend for a set of common backplane operations, allowing for wide distribution of retention and execution. The backplane serves as a registry of shard workers, the storage for the AC, the various queues and event monitors used in Operation processing, and the CAS index. The only current backplane implementation uses redis, but the backplane interface is strongly decoupled from its usage, and any single or composite communication layer may be used to satisfy its requirements. Sharded instances select arbitrary members of the Workers set for CAS writes, and reference the backplane CAS index for CAS reads, selecting any of a set of shards that advertise content to retrieve it. These shards can be any gRPC CAS compatible service endpoint. Executions on shard are transformed into a worker queue through a processor built into the instance. A server which runs the instance (configurably) participates in this pool, reducing load on a directly connected service to the client. This allows an entire cluster of servers to participate evenly in populating the worker queue with heavyweight operation definitions, even if a client is only communicating with a single host. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/instance_types/#shard",
    "relUrl": "/docs/architecture/instance_types/#shard"
  },"75": {
    "doc": "Local Development",
    "title": "Developer Information",
    "content": "The recommended solution for deploying a complete dev environment is to use tilt. Follow the installation instructions here. Make sure you can use a local kubernetes cluster by following these steps. If everything is installed correctly, you can go to the root of the repo and run tilt up. Tilt will prompt you to open a web UI and see all the running services. Below is information for running services directly. Setting up Redis for local testing . This is done using examples/development-redis-cluster.sh. Tested with Redis 6.0.10, other versions probably work fine as well. First of all you need Redis installed: . | macOS: brew install redis | Debian / Ubuntu: sudo apt-get update &amp;&amp; sudo apt-get install redis-server redis-tools | . Then you need eight terminal panes for this. Six for a minimal Redis cluster, one for the Buildfarm server and one for a Buildfarm worker. | ./examples/development-redis-cluster.sh 0 | ./examples/development-redis-cluster.sh 1 | ./examples/development-redis-cluster.sh 2 | ./examples/development-redis-cluster.sh 3 | ./examples/development-redis-cluster.sh 4 | ./examples/development-redis-cluster.sh 5 | redis-cli --cluster create 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384 --cluster-replicas 1 . | . Your Redis cluster is now up, and you can now start your Buildfarm server talking to it: . bazel run //src/main/java/build/buildfarm:buildfarm-server $PWD/examples/config.yml . And your Buildfarm worker: . mkdir /tmp/worker bazel run //src/main/java/build/buildfarm:buildfarm-shard-worker $PWD/examples/config.yml . Setting up intelliJ . | Check which IntelliJ versions are supported by the Bazel plugin | Make sure you have a supported IntelliJ version, otherwise download one here | Follow the Bazel plugin instructions and import ij.bazelproject | Once IntelliJ is done loading your project, open BuildFarmServer.java and find the main() method at the bottom | Press the green play button symbol in the gutter next to main() to create a Bazel build configuration for starting a server. Launching this configuration should get you a help text from Buildfarm Server indicating missing a config file. This indicates a successful launch! . | To add a config file, edit your new run configuration and enter the absolute path to examples/config.minimal.yml in the “Executable flags” text box. | . Now, you should have something like this, and you can now run / debug Buildfarm Server from inside of IntelliJ, just like any other program: . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/contribute/local_development/#developer-information",
    "relUrl": "/docs/contribute/local_development/#developer-information"
  },"76": {
    "doc": "Local Development",
    "title": "Committing",
    "content": "Please sign your commits and please use pre-commit before committing. This will help you avoid style drift in Java and Yaml files. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/contribute/local_development/#committing",
    "relUrl": "/docs/contribute/local_development/#committing"
  },"77": {
    "doc": "Local Development",
    "title": "Local Development",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/contribute/local_development/",
    "relUrl": "/docs/contribute/local_development/"
  },"78": {
    "doc": "Metrics",
    "title": "Prometheus Configuration",
    "content": "To enable emitting of Prometheus metrics, add the following configuration to your configuration file: . server: prometheusPort: 9090 . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/metrics/metrics/#prometheus-configuration",
    "relUrl": "/docs/metrics/metrics/#prometheus-configuration"
  },"79": {
    "doc": "Metrics",
    "title": "Available Prometheus Metrics",
    "content": "remote_invocations . Counter for the number of invocations of the capabilities service . expired_key . Counter for key expirations . execution_success . Counter for the number of executions requests received . merged_executions . Counter for the number of executions merged by action . pre_queue_size . Gauge of a number of items in prequeue . cas_miss . Counter for number of CAS misses from worker-worker . queue_failure . Counter for number of operations that failed to queue . requeue_failure . Counter for number of operations that failed to requeue . dispatched_operations_size . Gauge of the number of dispatched operations . worker_pool_size . Gauge of the number of workers available . storage_worker_pool_size . Gauge of the number of storage workers available . execute_worker_pool_size . Gauge of the number of execute workers available. queue_size . Gauge of the size of the queue (using a queue_name label for each individual queue) . actions . Counter for the number of actions processed . operations_stage_load . Counter for the number of operations in each stage (using a stage_name for each individual stage) . operation_status . Counter for the completed operations status (using a code label for each individual GRPC status code) . operation_exit_code . Counter for the completed operations exit code (using an exit_code label for each individual execution exit code) . operation_worker . Counter for the number of operations executed on each worker (using a worker_name label for each individual worker) . action_results . Counter for the number of action results . missing_blobs . Histogram for the number of missing blobs . execution_slot_usage . Gauge for the number of execution slots used on each worker . execution_time_ms . Histogram for the execution time on a worker (in milliseconds) . execution_stall_time_ms . Histogram for the execution stall time on a worker (in milliseconds) . input_fetch_slot_usage . Gauge for the number of input fetch slots used on each worker . input_fetch_time_ms . Histogram for the input fetch time on a worker (in milliseconds) . input_fetch_stall_time_ms . Histogram for the input fetch stall time on a worker (in milliseconds) . queued_time_ms . Histogram for the operation queued time (in milliseconds) . output_upload_time_ms . Histogram for the output upload time (in milliseconds) . completed_operations . Counter for the number of completed operations . operation_poller . Counter for the number of operations being polled . io_bytes . Histogram for the bytes read/written to get system I/O . health_check . Counter showing service restarts . cas_size . Gauge of total size of the worker’s CAS in bytes . cas_ttl_s . Histogram for amount of time CAS entries live on L1 storage before expiration (seconds) . cas_entry_count . Gauge of the total number of entries in the worker’s CAS . cas_copy_fallback Counter for the number of times the CAS performed a file copy because hardlinking failed . Java interceptors can be used to monitor Grpc services using Prometheus. To enable these metrics, add the following configuration to your server: . server: grpcMetrics: enabled: true provideLatencyHistograms: false . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/metrics/metrics/#available-prometheus-metrics",
    "relUrl": "/docs/metrics/metrics/#available-prometheus-metrics"
  },"80": {
    "doc": "Metrics",
    "title": "Metrics",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/metrics/metrics/",
    "relUrl": "/docs/metrics/metrics/"
  },"81": {
    "doc": "Observability",
    "title": "Observability",
    "content": "Executions are organized by a hierarchy of ownership, and indexed with client-specified fields. The longrunning.operations API can be used to find a configuration-limited number of executions organized in this hierarchy by name and filter. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/observability/",
    "relUrl": "/docs/execution/observability/"
  },"82": {
    "doc": "Observability",
    "title": "Hierarchy",
    "content": ". | Correlated Invocations - specified by build tool invoker . | Tool Invocation - generated internally by build tool . | Execution - generated internally by buildfarm | . | . | . An Execution is an individual Execute request made from a client to buildfarm. The names for all Executions on buildfarm will be the string &lt;instance name&gt;/executions/&lt;uuid&gt;, with uuid being generated after the Execute call is made. A Tool Invocation is an organization of all requests made by a client during one logical user invocation. For instance, bazel, as a tool, will generate a UUID per invocation, usually by a user or CI script - e.g: each time you run bazel, it generates internally and uses a different toolInvocationId, making this a collection of Executions (and other requests) per run. A set of Correlated Invocations is uniquely identified by another UUID, the correlatedInvocationsId, which is expected to encompass one or more Tool Invocations. A build tool will provide the mechanism to specify this by a user - e.g: each time you run bazel, you can specify the same --build_request_id to indicate that a run is a member of a collection of Tool Invocations. The correlatedInvocationsId must be a string that ends with an 8-4-4-4-12 format UUID string (important because this value is completely specified by a client user), otherwise it is ignored and buildfarm will emit a warning. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/observability/#hierarchy",
    "relUrl": "/docs/execution/observability/#hierarchy"
  },"83": {
    "doc": "Observability",
    "title": "Indexing",
    "content": "Note that the correlatedInvocationsId only needs to end in a UUID. This means that before the end of the id, this can contain whatever identifying information we want. Buildfarm has taken steps that make it beneficial to provide this information as a URI. The fragment of the URI is an excellent place to specify the required trailing UUID. For example, our archetypal builder “Alice” provided this: . https://alice@dev-workstation/projects/winthorp/modules/interpreter?org=database&amp;milestone=epsilon#fda159f2-bafe-4270-91ba-c7a946c0f2ba . She’s specified a userinfo containing her username (the password field of userinfo will be ignored by buildfarm), the host the build ran on, a path that identified her project and its module per her organization, which is listed here as database, and she’s clearly working on the ‘epsilon’ milestone. Trailing it after a fragment separator is the required uuid. A single layer of optional indexing is provided in buildfarm to collect Correlated Invocations with useful identifying information. The indexing starts with the configuration of buildfarm’s server: . server: ... correlatedInvocationsIndexScopes: !!set ? host ? username . This means that the host and username fields should be indexed for correlatedInvocationsIds observed. This configuration is the default, and you may remove or add any indexing you wish. These values are extracted from the correlatedInvocationsId on every request, and must not differ in content contained within the URI without also changing the UUID, in order to guarantee proper indexing. Any values other than the above will be extracted from the query of the URI - host will come from the component in authority, and username from userinfo. These will create indices which identify correlatedInvocations via the UUID portion of the correlatedInvocationsId, making this a series of collections of correlatedInvocations . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/observability/#indexing",
    "relUrl": "/docs/execution/observability/#indexing"
  },"84": {
    "doc": "Observability",
    "title": "listOperations",
    "content": "The Longrunning Operations method listOperations will serve as the entry point to discover elements of this hierarchy. In the interest of a speedy response, the Operations it returns will not have any details other than name populated. More detailed information about a name returned can be queried with getOperation. The listOperations response can then be considered a list of names for these examples. The ListOperationsRequest name and filter fields have several special meanings intended for discovery. The instance name listed here will be solomon for identification purposes. ListOperationsRequest({ name: \"solomon\", }) . Will yield the names of the indices and special bindings. In the case of the above, after running a build with the above correlatedInvocationsId and configuration: . executions toolInvocations correlatedInvocations host username . To discover the available correlatedInvocations index entries for username: . ListOperationsRequest({ name: \"solomon/username\", }) . Which will yield: . username=alice username=bob username=mallory . For a system where usernames alice, bob, and mallory have been presented in correlatedInvocationsId URIs. To see the correlatedInvocationsIds for mallory, we can switch to a filter and select the correlatedInvocations parent resource: . ListOperationsRequest({ name: \"solomon/correlatedInvocations\", filter: \"username=mallory\", }) . Note that the format is the same as the one in the output of the index entries result. This yields: . 0c1ce9e6-471d-4265-b15b-885e42670fdb 7bd7f8d2-6628-4f3f-a7e3-9eff917eb697 6ca27615-4fb5-4ffc-9349-d484926bd05e . So username=mallory is associated with these three correlatedInvocations. These are correlatedInvocationsIds which can be passed to the toolInvocations parent resource: . ListOperationsRequest({ name: \"solomon/toolInvocations\", filter: \"correlatedInvocationId=0c1ce9e6-471d-4265-b15b-885e42670fdb\", }) . To get a list of toolInvocationIds: . fad33d5e-fbba-483d-8df4-4502621caff2 44e516cd-d8fd-4c61-90d2-9389402242a4 . This correlatedInvocationId was used in two client invocations. Our last stop on the hierarchy is to pass this to the executions binding associated with its toolInvocationId: . ListOperationsRequest({ name: \"solomon/executions\", filter: \"toolInvocationId=fad33d5e-fbba-483d-8df4-4502621caff2\", }) . This yields a huge list of the names of executions that the toolInvocation fad33d5e-fbba-483d-8df4-4502621caff2 requested via the Execute method. The Operation-encapsulated information from these execution UUID results can be retrieved via getOperation with a name of solomon/executions/&lt;uuid&gt;. Those executions will have a wealth of information about where and how they executed, and will be up to date with the current reporting if the execution is still in flight. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/observability/#listoperations",
    "relUrl": "/docs/execution/observability/#listoperations"
  },"85": {
    "doc": "Observability",
    "title": "Tools",
    "content": "bf-cat can issue all of the example requests listed here to both listOperations and getOperation: . bf-cat ... Operations &lt;filter&gt; &lt;name-without-index&gt; . for listOperations, and . bf-cat ... Operation &lt;operation-name&gt; [&lt;more-operation-names&gt;...] . for getOperation, which supports requesting multiple uuids in a single invocation. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/execution/observability/#tools",
    "relUrl": "/docs/execution/observability/#tools"
  },"86": {
    "doc": "Queues",
    "title": "Operation Queue",
    "content": "This section discusses the purpose and design of the Operation queue. It also discusses how it can be customized depending on the type of operations you wish to support, and how you wish to distribute them among workers. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/queues/#operation-queue",
    "relUrl": "/docs/architecture/queues/#operation-queue"
  },"87": {
    "doc": "Queues",
    "title": "Quick Summary",
    "content": "Some time after an Action execute request occurs, the longrunning operation it corresponds to will enter the QUEUED state, and will receive an update to that effect on the operation response stream. An operation in the QUEUED state is present in an Operation Queue, which holds the operations in sequence until a worker is available to execute it. Schedulers put operations on the queue. Workers take them off. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/queues/#quick-summary",
    "relUrl": "/docs/architecture/queues/#quick-summary"
  },"88": {
    "doc": "Queues",
    "title": "Working with different platform requirements",
    "content": "Some operations’ Actions may have specific platform requirements in order to execute. Likewise, specific workers may only want to take on work that they deem eligible. To solve this, the operation queue can be customized to divide work into separate provisioned queues so that specific workers can choose which queue to read from. Provision queues are intended to represent particular operations that should only be processed by particular workers. An example use case for this would be to have two dedicated provision queues for CPU and GPU operations. CPU/GPU requirements would be determined through the remote api’s command platform properties. We designate provision queues to have a set of “required provisions” (which match the platform properties). This allows the scheduler to distribute operations by their properties and allows workers to dequeue from particular queues. If your configuration file does not specify any provisioned queues, buildfarm will automatically provide a default queue with full eligibility on all operations. This will ensure the expected behavior for the paradigm in which all work is put on the same queue. Matching Algorithm . The matching algorithm is performed by the operation queue when the server or worker is requesting to push or pop elements, respectively. The matching algorithm is designed to find the appropriate queue to perform these actions on. On the scheduler side, the action’s platform properties are used for matching. On the worker side, the dequeue_match_settings are used. The matching algorithm works as follows: Each provision queue is checked in the order that it is configured. The first provision queue that is deemed eligible is chosen and used. When deciding if an action is eligible for the provision queue, each platform property is checked individually. By default, there must be a perfect match on each key/value. Wildcards (“*”) can be used to avoid the need of a perfect match. Additionally, if the action contains any platform properties is not mentioned by the provision queue, it will be deemed ineligible. setting allowUnmatched: true can be used to allow a superset of action properties as long as a subset matches the provision queue. If no provision queues can be matched, the operation queue will provide an analysis on why none of the queues were eligible. A worker will dequeue operations from matching queues and determine whether to keep and execute it according to the following procedure: For each property key-value in the operation’s platform, an operation is REJECTED if: The key is min-cores and the integer value is greater than the number of cores on the worker. Or The key is min-mem and the integer value is greater than the number of bytes of RAM on the worker. Or if the key exists in the DequeueMatchSettings platform with neither the value nor a * in the corresponding DMS platform key’s values, Or if the allowUnmatched setting is false. For each resource requested in the operation’s platform with the resource: prefix, the action is rejected if: The resource amount cannot currently be satisfied with the associated resource capacity count . There are special predefined execution property names which resolve to dynamic configuration for the worker to match against: Worker: The worker’s publicName min-cores: Less than or equal to the executeStageWidth process-wrapper: The set of named process-wrappers present in configuration . Server Example . In this example the scheduler declares a GPU queue and CPU queue. All queues must be declared for the server deployment: . backplane: queues: - name: \"cpu\" allowUnmatched: true properties: - name: \"min-cores\" value: \"*\" - name: \"max-cores\" value: \"*\" - name: \"gpu\" allowUnmatched: true properties: - name: \"gpu\" value: \"1\" . Worker Example . Queues are defined similarly on Workers. Only the specific worker type queue must be declared for that specific worker deployment. For example, for a CPU worker pool use: . backplane: queues: - name: \"cpu\" allowUnmatched: true properties: - name: \"min-cores\" value: \"*\" - name: \"max-cores\" value: \"*\" . For example, for a GPU worker pool use: . backplane: queues: - name: \"gpu\" allowUnmatched: true properties: - name: \"gpu\" value: \"1\" . Note: make sure that all workers can communicate with each other before trying these examples . Bazel Perspective . Bazel targets can pass these platform properties to buildfarm via exec_properties. Here is for example how to run a remote build for the GPU queue example above: . bazel build --remote_executor=grpc://server:port --remote_default_exec_properties=gpu=1 //... ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/queues/#working-with-different-platform-requirements",
    "relUrl": "/docs/architecture/queues/#working-with-different-platform-requirements"
  },"89": {
    "doc": "Queues",
    "title": "Queues",
    "content": " ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/queues/",
    "relUrl": "/docs/architecture/queues/"
  },"90": {
    "doc": "Quick Start",
    "title": "Quick Start",
    "content": "Here we describe how to use bazel remote caching or remote execution with buildfarm. We will create a single client workspace that can be used for both. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/quick_start/",
    "relUrl": "/docs/quick_start/"
  },"91": {
    "doc": "Quick Start",
    "title": "Setup",
    "content": "You can run this quick start on a single computer running any flavor of linux that bazel supports. A C++ compiler is used here to demonstrate action execution. This computer is the localhost for the rest of the description. Backplane . Buildfarm requires a backplane to store information that is shared between cluster members. A redis server can be used to meet this requirement. Download/Install a redis-server instance and run it on your localhost. The default redis port of 6379 will be used by the default buildfarm configs. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/quick_start/#setup",
    "relUrl": "/docs/quick_start/#setup"
  },"92": {
    "doc": "Quick Start",
    "title": "Workspace",
    "content": "Let’s start with a bazel workspace with a single file to compile into an executable: . Create a new directory for our workspace and add the following files: . main.cc: . #include &lt;iostream&gt; int main( int argc, char *argv[] ) { std::cout &lt;&lt; \"Hello, World!\" &lt;&lt; std::endl; } . BUILD: . cc_binary( name = \"main\", srcs = [\"main.cc\"], ) . And an empty WORKSPACE file. As a test, verify that bazel run :main builds your main program and runs it, and prints Hello, World!. This will ensure that you have properly installed bazel and a C++ compiler, and have a working target before moving on to remote caching or remote execution. Download and extract the buildfarm repository. Each command sequence below will have the intended working directory indicated, between the client (workspace running bazel), and buildfarm. This tutorial assumes that you have a bazel binary in your path and you are in the root of your buildfarm clone/release, and has been tested to work with bash on linux. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/quick_start/#workspace",
    "relUrl": "/docs/quick_start/#workspace"
  },"93": {
    "doc": "Quick Start",
    "title": "Remote Caching",
    "content": "A Buildfarm cluster can be used strictly as an ActionCache (AC) and ContentAddressableStorage (CAS) to improve build performance. This is an example of running a bazel client that will retrieve results if available, otherwise store them on a cache miss after executing locally. Download the buildfarm repository and change into its directory, then: . | run bazel run src/main/java/build/buildfarm:buildfarm-server $PWD/examples/config.minimal.yml | . This will wait while the server runs, indicating that it is ready for requests. A server alone does not itself store the content of action results. It acts as an endpoint for any number of workers that present storage, so we must also start a single worker. From another prompt (i.e. a separate terminal) in the buildfarm repository directory: . | run bazel run src/main/java/build/buildfarm:buildfarm-shard-worker -- --prometheus_port=9091 $PWD/examples/config.minimal.yml | . The -- option is bazel convention to treat all subsequent arguments as parameters to the running app, like our --prometheus_port, instead of interpreting them with run The --prometheus_port=9091 option allows this worker to run alongside our server, who will have started and logged that it has started a service on port 9090. You can also turn this option off (with -- separator), with --prometheus_option=0 for either server or worker. This will also wait while the worker runs, indicating it will be available to store cache content. From another prompt in your newly created workspace directory from above: . | run bazel clean | run bazel run --remote_cache=grpc://localhost:8980 :main | . Why do we clean here? Since we’re verifying re-execution and caching, this ensures that we will execute any actions in the run step and interact with the remote cache. We should be attempting to retrieve cached results, and then when we miss - since we just started this memory resident server - bazel will upload the results of the execution for later use. There will be no change in the output of this bazel run if everything worked, since bazel does not provide output each time it uploads results. To prove that we have placed something in the action cache, we need to do the following: . | run bazel clean | run bazel run --remote_cache=grpc://localhost:8980 :main | . This should now print statistics on the processes line that indicate that you’ve retrieved results from the cache for your actions: . INFO: 2 processes: 2 remote cache hit. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/quick_start/#remote-caching",
    "relUrl": "/docs/quick_start/#remote-caching"
  },"94": {
    "doc": "Quick Start",
    "title": "Remote Execution (and caching)",
    "content": "Now we will use buildfarm for remote execution with a minimal configuration with a worker on the localhost that can execute a single process at a time, via a bazel invocation on our workspace. First, to clean out the results from the previous cached actions, flush your local redis database: . | run redis-cli flushdb | . Next, we should restart the buildfarm server, and delete the worker’s cas storage to ensure that we get remote execution (this can also be forced from the client by using --noremote_accept_cached). From the buildfarm server prompt and directory: . | interrupt the running buildfarm-server (i.e. Ctrl-C) | run bazel run src/main/java/build/buildfarm:buildfarm-server $PWD/examples/config.minimal.yml | . You can leave the worker running from the Remote Caching step, it will not require a restart . From another prompt, in your client workspace: . | run bazel run --remote_executor=grpc://localhost:8980 :main | . Your build should now print out the following on its processes summary line: . INFO: 2 processes: 2 remote. That 2 remote indicates that your compile and link ran remotely. Congratulations, you just build something through remote execution! . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/quick_start/#remote-execution-and-caching",
    "relUrl": "/docs/quick_start/#remote-execution-and-caching"
  },"95": {
    "doc": "Quick Start",
    "title": "Container Quick Start",
    "content": "To bring up a minimal buildfarm cluster, you can run: . $ ./examples/bf-run start . This will start all of the necessary containers at the latest version. Once the containers are up, you can build with bazel run --remote_executor=grpc://localhost:8980 :main. To stop the containers, run: . $ ./examples/bf-run stop . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/quick_start/#container-quick-start",
    "relUrl": "/docs/quick_start/#container-quick-start"
  },"96": {
    "doc": "Quick Start",
    "title": "Next Steps",
    "content": "We’ve started our worker on the same host as our server, and also the same host on which we built with bazel, but these services can be spread across many machines, per ‘remote’. A large number of workers, with a relatively small number of servers (10:1 and 100:1 ratios have been used in practice), consolidating large disks and beefy multicore cpus/gpus on workers, with specialization of what work they perform for bazel builds (or other client work), and specializing servers to have hefty network connections to funnel content traffic. A buildfarm deployment can service hundreds or thousands of developers or CI processes, enabling them to benefit from each others’ shared context in the AC/CAS, and the pooled execution of a fleet of worker hosts eager to consume operations and deliver results. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/quick_start/#next-steps",
    "relUrl": "/docs/quick_start/#next-steps"
  },"97": {
    "doc": "Quick Start",
    "title": "Buildfarm Manager",
    "content": "You can now easily launch a new Buildfarm cluster locally or in AWS using an open sourced Buildfarm Manager. $ wget https://github.com/80degreeswest/bfmgr/releases/download/1.0.7/bfmgr-1.0.7.jar $ java -jar bfmgr-1.0.7.jar $ open http://localhost . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/quick_start/#buildfarm-manager",
    "relUrl": "/docs/quick_start/#buildfarm-manager"
  },"98": {
    "doc": "Redis",
    "title": "Redis",
    "content": "Redis is used as the in-memory database between sharded actors. Buildfarm’s backplane uses a Jedis Cluster for various abstractions. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/redis/",
    "relUrl": "/docs/architecture/redis/"
  },"99": {
    "doc": "Redis",
    "title": "Balanced Queues",
    "content": "To balance CPU utilization across multiple nodes in a redis cluster, we distribute operations through redis hashtags. We have a conceptual queue that uses multiple redis lists in its implementation. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/redis/#balanced-queues",
    "relUrl": "/docs/architecture/redis/#balanced-queues"
  },"100": {
    "doc": "Schedulers",
    "title": "Schedulers",
    "content": "Information about schedulers. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/schedulers/",
    "relUrl": "/docs/architecture/schedulers/"
  },"101": {
    "doc": "Tools",
    "title": "Tools",
    "content": "Information about tools that can be used alongside buildfarm. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/tools/tools/",
    "relUrl": "/docs/tools/tools/"
  },"102": {
    "doc": "Troubleshooting",
    "title": "bazel logging",
    "content": "Use bazel [build|run|test] --remote_grpc_log=&lt;filename&gt; (--experimental_remote_grpc_log=&lt;filename&gt; if you are using bazel older than 6.0 release) to produce a binary log of all of the grpc activity bazel performs during an invocation. This log is written to at the completion of each request, and may not contain a complete picture if a build is interrupted, or a request is currently ongoing. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/tools/troubleshooting-bazel-remote-execution/#bazel-logging",
    "relUrl": "/docs/tools/troubleshooting-bazel-remote-execution/#bazel-logging"
  },"103": {
    "doc": "Troubleshooting",
    "title": "Dumping the log",
    "content": "Use tools_remote. Build the remote_client in it with bazel build :remote_client. Since we’re going to point to a local file, it might be easiest to use the bazel-bin/remote_client executable to perform a text dump of the previously generated bazel log file: . $ bazel-bin/remote_client --grpc_log=&lt;filename&gt; printlog . Warning: the original log file may be big, and the corresponding text output may be similarly big. This text output is invaluable, however, when trying to associate or distinguish the activities of a single build, and we strongly recommend that users report bugs with these logs attached, in either form. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/tools/troubleshooting-bazel-remote-execution/#dumping-the-log",
    "relUrl": "/docs/tools/troubleshooting-bazel-remote-execution/#dumping-the-log"
  },"104": {
    "doc": "Troubleshooting",
    "title": "The log file",
    "content": "The log file retains all of the Calls (i.e. Requests and Responses) for (at least) content related to remote execution. These calls will have either no status code response (0, OK, with the default proto value for ints omitted), or a numeric status code. Use https://github.com/grpc/grpc/blob/main/doc/statuscodes.md to reference these until tools_remote can print a nicer representation. Also included in each call is the timestamp of the call, its final response, and for streaming calls, some information about how the call proceeded through retries and progressive activity. There is also RequestMetadata information printed for each request to indicate which correlated build id, build id, and action a call is associated with. The action is consistent across all of the requests for a single action, and is vital to group the disparate calls within a log to the same action context. It also helps to establish an expected flow of the sequence of calls. You will see some (or none) of the following Calls in this log file: . Capabilities::GetCapabilities - asking the remote service what is supported. ActionCache::GetActionResult - a call to the action cache service to retrieve an action result. This takes a ‘digest key’, a hex number whose size depends on your hash policy, likely the default SHA256, packaged with the size of the original content, looking something like a948904f2f0f479b8f8197694b30184b0d2ed1c1cd2a1ec0fb85d299a192a447/42. The responses, if successful, will contain the results of an action execution, and constitute a cache HIT. A NOT_FOUND status response corresponds to a cache MISS. ByteStream::Read - Each one of these is a download for a digest that, very likely, was reported in an action result. These likewise use digests in their resource_names, and if you correspond the digest in a Read to a digest in an Action Result, you will learn the name of a file being downloaded. ContentAddressableStorage::FindMissingBlob - The client is likely asking the remote system which of a set of digests does NOT exist in the CAS. An empty response means that everything is present remotely already. Any blobs returned will need to be uploaded by the client. You can find the action key in this set, at a minimum. This call occurs before any uploads of content, because it drives those calls, within an action’s context. ByteStream::Write - Each of these is an upload for a digest that, very likely, was returned in a FindMissingBlobs response. Digests are in their resource_names again, with an additional uuid to indicate a particular client’s upload of a file. Execution Execute - this is the call that directs the remote execution system to execute an action. Each response will be listed as the execution moves through one of several states on the remote side, likely culminating in COMPLETED. Note that this does not mean a successful execution, only that an operation is done executing, and now has its exit status, and outputs, if any, available for download in the CAS. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/tools/troubleshooting-bazel-remote-execution/#the-log-file",
    "relUrl": "/docs/tools/troubleshooting-bazel-remote-execution/#the-log-file"
  },"105": {
    "doc": "Troubleshooting",
    "title": "Troubleshooting",
    "content": "Remote execution is sometimes an exercise fraught with problems, whether they be client definition, specification of remote endpoints, interactions with layered systems, and the complex operation of running a configured remote installation. This guide serves to give you some of the tools to debug your problems, from a bazel client point of view. A typical use case: Something works locally, but breaks when remote execution is introduced. Beyond stdout, and execution instrumentation, you will probably want to find out what step along the path to remote operation responses. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/tools/troubleshooting-bazel-remote-execution/",
    "relUrl": "/docs/tools/troubleshooting-bazel-remote-execution/"
  },"106": {
    "doc": "Worker Execution Environment",
    "title": "Worker Execution Environment",
    "content": "Since workers are expected to execute programs in a way that makes using remote transparent to build users, there is a great deal of nuance to their definition. Operating System, Distribution, runtimes, compilers, their versions, and standard libraries make this a tricky proposition. While hermeticizing a build by declaring the full set of tools a compilation requires is the ‘right’ solution for a build, it may be too big of a hill to climb for everyone, particularly those trying to ease into remote execution. Under Linux at least, this is made somewhat easier through docker, even if the steps to get to a conformant environment are a bit complicated. Here we will provide an example of creating a target execution environment container capable of running buildfarm, and this should be similar for all major distributions, particularly those with released docker hub bases and standard package managers including java runtimes and toolchain software. You should choose the versions of relevant software at first based on the client environment you want to support. For this example, we’re going to assume a target of ubuntu-20 (focal), with a gcc9 compiler supporting C++, and a java runtime from openjdk-14 supplied by its package manager. We will need: . | a bazel 3.3.1 install (the older version is required by rules_docker at the buildfarm version, this will be updated eventually). | docker daemon running | . First we will pull our intended base image from dockerhub: . docker pull ubuntu:focal . Next we will create our Dockerfile for our customized environment. Here is the content of that Dockerfile . # choose our ubuntu distribution with version 20 (focal) from ubuntu:focal # get our current software suite. apt-get is preferred to apt due to warnings # about a non-stable CLI run apt-get update # install the basic apt-utils to limit warnings due to its absence run DEBIAN_FRONTEND=noninteractive apt-get install --no-install-suggests apt-utils # upgrade to the current software suite versions run DEBIAN_FRONTEND=noninteractive apt-get dist-upgrade -y # install the required packages for our execution environment and buildfarm runtime run DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-suggests \\ g++-9 g++ openjdk-14-jdk-headless . And build it with docker build -t ubuntu20-java14:latest . Sending build context to Docker daemon 2.56kB Step 1/5 : from ubuntu:focal ---&gt; 9140108b62dc ... Removing intermediate container 505236be00e4 ---&gt; 77a8c2ae4d16 Successfully built 77a8c2ae4d16 Successfully tagged ubuntu20-java14:latest . In our example’s case, we want to use this image as a base to build a buildfarm worker container image. Unfortunately, rules_docker does not know how to interact with the docker images hosted by a local docker daemon, only docker registries like dockerhub.io. We will get past this by running the local registry utility container. If you really want to host this base elsewhere, I’ll assume that you know how to translate the host:port references below, and you can skip running your own registry. First we need to tag our image so that it gets pushed to our local registry: . docker tag ubuntu20-java14:latest localhost.localdomain:5000/ubuntu20-java14:latest . Then we can start our registry with a recognizable name for later shutdown: . docker run -d --rm --name local-registry -p 5000:5000 registry . And push our newly tagged image: . docker push localhost.localdomain:5000/ubuntu20-java14:latest . Take note of the sha256:&lt;sha256sum&gt; output that was produced with this command, as we will need to use it to specify our base. Now we have a referent docker image that can be identified as a base to apply buildfarm’s worker installation onto. In a suitable location, create a WORKSPACE containing: . load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") BUILDFARM_EXTERNAL_COMMIT = \"04fc2635f5546a4f5dc19dea35bb1fca5569ce24\" BUILDFARM_EXTERNAL_SHA256 = \"f45215ef075c8aff230b737ca3bc5ba183c1137787fcbbb10dd407463f76edb6\" http_archive( name = \"build_buildfarm\", strip_prefix = \"bazel-buildfarm-%s\" % BUILDFARM_EXTERNAL_COMMIT, sha256 = BUILDFARM_EXTERNAL_SHA256, url = \"https://github.com/buildfarm/buildfarm/archive/%s.zip\" % BUILDFARM_EXTERNAL_COMMIT, ) load(\"@build_buildfarm//:deps.bzl\", \"buildfarm_dependencies\") buildfarm_dependencies() load(\"@build_buildfarm//:defs.bzl\", \"buildfarm_init\") buildfarm_init() load(\"@io_bazel_rules_docker//repositories:deps.bzl\", container_deps = \"deps\") load(\"@io_bazel_rules_docker//container:container.bzl\", \"container_pull\") container_deps() container_pull( name = \"ubuntu20_java14_image_base\", digest = \"sha256:&lt;sha256sum&gt;\", registry = \"localhost:5000\", repository = \"ubuntu20-java14\", ) . Be sure to substitute the sha256:&lt;sha256sum&gt; with your content from above. Next we will create a BUILD file to create our target image. We will use the shard variant here, but the memory worker (with supporting server execution) will work as well. The content of the BUILD file should be: . load(\"@io_bazel_rules_docker//java:image.bzl\", \"java_image\") java_image( name = \"buildfarm-shard-worker-ubuntu20-java14\", base = \"@ubuntu20_java14_image_base//image\", deps = [\"//src/main/java/build/buildfarm:buildfarm-shard-worker\"], main_class = \"build.buildfarm.worker.shard.Worker\", ) . And now that this is in place, we can use the following to build the container and make it available to our local docker daemon: . bazel run :buildfarm-shard-worker-ubuntu20-java14 . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/worker-execution-environment/",
    "relUrl": "/docs/architecture/worker-execution-environment/"
  },"107": {
    "doc": "Workers",
    "title": "Workers",
    "content": "Workers have two major roles in Buildfarm: Execution and CAS Shard. Either of these options can be disabled, though a worker with both disabled provides no value. Regardless of role, a worker must have a local FILESYSTEM type storage to retain content. This storage serves both as a resident LRU cache for Execution I/O, and the local storage for a CAS Shard. Workers can delegate to successive storage declarations (FILESYSTEM or GRPC), with read-through or expiration waterfall if configured, but only the first storage entry will be used for Executions. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/workers/",
    "relUrl": "/docs/architecture/workers/"
  },"108": {
    "doc": "Workers",
    "title": "Execution",
    "content": "Execution Workers are responsible for matching their environments against operations, presenting execution roots to those operations, fetching content from a CAS, executing processes required to complete the operations, and reporting the outputs and results of executions. Control and delivery of these behaviors is accomplished with several mechanisms: . | A CAS FileCache, which is capable of reading through content for Digests of files or directories, and efficiently presenting those contents based on usage and reference counting, as well as support for cascading into delegate CASs. | ExecutionPolicies, which allow for explicit and implicit behaviors to control execution. | Execution Resources to limit concurrent execution in installation-defined resource traunches. | Concurrent pipelined execution of operations, with support for superscalar stages at input fetch and execution. | Operation exclusivity, preventing the same operation from running through the worker pipeline concurrently. | . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/workers/#execution",
    "relUrl": "/docs/architecture/workers/#execution"
  },"109": {
    "doc": "Workers",
    "title": "CAS Shard",
    "content": "Sharded workers interact with the shard backplane for both execution and CAS presentation. Their CAS FileCache serves a CAS gRPC interface as well as the execution root factory. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/workers/#cas-shard",
    "relUrl": "/docs/architecture/workers/#cas-shard"
  },"110": {
    "doc": "Workers",
    "title": "Pipelines",
    "content": "A pipeline handles operations as they arrive and are processed on a worker. Each stage of the pipeline performs its task on an operation, and holds that task until the subsequent stage can take over. This creates backpressure to mitigate risk and limit resource consumption. Since a worker only takes on enough work to exhaust the most limited resource at a time (CPU/IO/Bandwidth), losing that worker due to unforeseen failures is not disruptive to the rest of the cluster. Keeping work in each stage holds resources for an operation as well, so preventing operations from piling up in an earlier stage due to a longer running later stage reduces the overall resource footprint required for the worker. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/workers/#pipelines",
    "relUrl": "/docs/architecture/workers/#pipelines"
  },"111": {
    "doc": "Workers",
    "title": "Stages",
    "content": ". Stages have access to a WorkerContext provided to them by their Worker implementation (OperationQueue or Shard) which is used for all activity common across Worker types. Each stage must claim the subsequent stage before submitting it for processing. This allows measurement of processing time and latency for each operation per stage without any interleaving. Superscalar stages have a configurable number of slots for their activity. Claims on these stages block until they are full, and have a number of slots to claim exclusively for activity. Match . The Match stage is responsible for dequeuing an operation from the Ready-To-Run queue. This operation is dequeued as a QueueEntry which contains the ExecuteEntry and a Digest for the transformed QueuedOperation. The ExecuteEntry contains a Platform definition which must match the worker’s provided platform manifest in order to proceed. A rejected QueueEntry for this reason will be reinserted into the Ready-To-Run queue. The Match stage is unique in that it claims a slot in the Input Fetch stage prior to its iteration. This removes a polling requirement for the active operation present in other stages while waiting to feed the interstage, reducing the stage’s complexity. Input Fetch . Input Fetch is a superscalar stage responsible for downloading the QueuedOperation from the CAS, and creating the execution directory for the Operation. This is the worker ingress bandwidth, and likely the disk IO write, consuming stage. Its configured concurrency is available in the worker config as input_fetch_stage_width. The ownership of output directories is configurable with [[exec_owner]]. Execution . Execution is a superscalar stage which initiates operation executions, applying any ExecutionPolicies. The operation transitions to the EXECUTING state when it reaches this stage. After spawning the process, it intercepts writes to stdout and stderr, and will terminate the process if it runs longer than its Action specified timeout. Its configured concurrency is available in the worker config as execute_stage_width. [[Execution Limiting]] is available as a configuration option under cgroups, if supported. Report Result . The Report Result stage injects any outputs from the operation into the CAS, and populates the ActionResult from from the results of the execution. It can inject into the ActionCache for cacheable actions, and can record an action in the blacklist if it violates output policy. The operation transitions to COMPLETED state after the outputs are recorded. After this stage is complete, the execution directory is destroyed, and the Operation exits the worker. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/workers/#stages",
    "relUrl": "/docs/architecture/workers/#stages"
  },"112": {
    "doc": "Workers",
    "title": "Exec Filesystem",
    "content": "Workers use ExecFileSystems to present content to actions, and manage their existence for the lifetime of an operation’s presence within the pipeline. The realization of an operation’s execution root with the execution filesystem constitutes a transaction that the operating directory for an action will appear, be writable for outputs, and released and be made unavailable as it proceeds and exits the pipeline. This means that an action’s entire input directory must be available on a filesystem from a unique location per operation - the Operation Action Input Root, or just Root. Each input file within the Root must contain the content of the inputs, its requested executability via FileNode, and each directory must contain at the outset, child input files and directories. The filesystem is free to handle unspecified outputs as it sees fit, but the directory hierarchy of output files from the Root must be created before execution, and writable during it. When execution and observation of the outputs is completed, the exec filesystem will be asked to destroy the Root and release any associated resources from its retention. Choosing a filesystem storage type in the worker config as the first storage entry will select the CASFileCache CFCExecFileSystem. Choosing any other storage type will create a FuseCAS FuseExecFilesystem. We strongly recommend the use of filesystem storage as the ExecFileSystem-selecting storage entry, the _FuseCAS_ is experimental and may not function reliably over long hauls/with substantial load . ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/workers/#exec-filesystem",
    "relUrl": "/docs/architecture/workers/#exec-filesystem"
  },"113": {
    "doc": "Workers",
    "title": "CASFileCache/CFCExecFilesystem",
    "content": "The CASFileCache provides an Exec Filesystem via CFCExecFilesystem. The (CASFileCache)’s retention of paths is used to reflect individual files, with these paths hard-linked in CFCExecFilesystem under representative directories of the input root to signify usage. The CASFileCache directory retention system is also used to provide a configurable utilization of entire directory trees as a symlink, which was a heuristic optimization applied when substantial cost was observed setting up static trees of input links for operations compared to their execution time. link_input_directories in the common Worker configuration will enable this heuristic. Outputs of actions are physically streamed into CAS writes when they are observed after an action execution. The CASFileCache’s persistence in the filesystem and the availability of common POSIX features like symlinks and inode-based reference counts on almost any filesystem implementation have made it a solid choice for extremely large CAS installations - it scales to multi-TB host attached storages containing millions of entries with relative ease. There are plans to improve CASFileCache that will be reflected in improved performance and memory footprint for the features used by CFCExecFilesystem. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/workers/#casfilecachecfcexecfilesystem",
    "relUrl": "/docs/architecture/workers/#casfilecachecfcexecfilesystem"
  },"114": {
    "doc": "Workers",
    "title": "Fuse",
    "content": "A fuse implementation to provide Roots exists and is specifiable as well. This was an experiment to discover the capacity of a fuse to represent Roots transparently with a ContentAddressableStorage backing, and has not been fully vetted to provide the same reliability as the CFCExecFilesystem. This system is capable of blinking entire trees into existence with ease, as well as supporting write-throughs for outputs suitable for general purpose execution. Some problems with this type were initially observed and never completely resolved, including guaranteed resource release on Root destruction. This implementation is also only built to be backed by its own Memory CAS, with no general purpose CAS support added due to the difficulty of supporting a transaction model for an input tree to enforce the contract of availability. It remains unoptimized yet functional, but difficulties with integrating libfuse 3 into the bazel build, as well as time constraints, have kept it from being scaled and expanded as the rest of Buildfarm has grown. There are plans to revisit this implementation and bring it back into viability with a CASFileCache-like backing. ",
    "url": "https://buildfarm.github.io/buildfarm/docs/architecture/workers/#fuse",
    "relUrl": "/docs/architecture/workers/#fuse"
  }
}
